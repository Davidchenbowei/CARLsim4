<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.9.1"/>
<title>CARLsim: snn_gpu_module.cu Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="CARLsimStyles.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">CARLsim
   &#160;<span id="projectnumber">4.0.0-beta</span>
   </div>
   <div id="projectbrief">CARLsim: a GPU-accelerated SNN simulator</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.9.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="usergroup0.html"><span>User&#160;Guide</span></a></li>
      <li><a href="usergroup1.html"><span>Tutorial</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>File&#160;Members</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('snn__gpu__module_8cu_source.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">snn_gpu_module.cu</div>  </div>
</div><!--header-->
<div class="contents">
<a href="snn__gpu__module_8cu.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;/* * Copyright (c) 2016 Regents of the University of California. All rights reserved.</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;*</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;* Redistribution and use in source and binary forms, with or without</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;* modification, are permitted provided that the following conditions</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;* are met:</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;*</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;* 1. Redistributions of source code must retain the above copyright</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;*    notice, this list of conditions and the following disclaimer.</div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;*</div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;* 2. Redistributions in binary form must reproduce the above copyright</div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;*    notice, this list of conditions and the following disclaimer in the</div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;*    documentation and/or other materials provided with the distribution.</div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;*</div>
<div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;* 3. The names of its contributors may not be used to endorse or promote</div>
<div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;*    products derived from this software without specific prior written</div>
<div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;*    permission.</div>
<div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;*</div>
<div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS</div>
<div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;* &quot;AS IS&quot; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT</div>
<div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;* LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR</div>
<div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;* A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR</div>
<div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;* CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,</div>
<div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;* EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,</div>
<div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;* PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR</div>
<div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;* PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF</div>
<div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;* LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING</div>
<div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;* NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS</div>
<div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;* SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</div>
<div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;*</div>
<div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;* *********************************************************************************************** *</div>
<div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;* CARLsim</div>
<div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;* created by: (MDR) Micah Richert, (JN) Jayram M. Nageswaran</div>
<div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;* maintained by:</div>
<div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;* (MA) Mike Avery &lt;averym@uci.edu&gt;</div>
<div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;* (MB) Michael Beyeler &lt;mbeyeler@uci.edu&gt;,</div>
<div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;* (KDC) Kristofor Carlson &lt;kdcarlso@uci.edu&gt;</div>
<div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;* (TSC) Ting-Shuo Chou &lt;tingshuc@uci.edu&gt;</div>
<div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;* (HK) Hirak J Kashyap &lt;kashyaph@uci.edu&gt;</div>
<div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;*</div>
<div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;* CARLsim v1.0: JM, MDR</div>
<div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;* CARLsim v2.0/v2.1/v2.2: JM, MDR, MA, MB, KDC</div>
<div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;* CARLsim3: MB, KDC, TSC</div>
<div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;* CARLsim4: TSC, HK</div>
<div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;*</div>
<div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;* CARLsim available from http://socsci.uci.edu/~jkrichma/CARLsim/</div>
<div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;* Ver 12/31/2016</div>
<div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;*/</div>
<div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div>
<div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;#include &lt;snn.h&gt;</div>
<div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;#include &lt;spike_buffer.h&gt;</div>
<div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;#include &lt;error_code.h&gt;</div>
<div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;#include &lt;cuda_runtime.h&gt;</div>
<div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;</div>
<div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;#define NUM_THREADS 128</div>
<div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;#define NUM_BLOCKS 64</div>
<div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;#define WARP_SIZE 32</div>
<div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;</div>
<div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;///////////////////////////////////////////////////////////////////</div>
<div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;// Some important ideas that explains the GPU execution are as follows:</div>
<div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;//  1. Each GPU block has a local firing table (called fireTable). The block of threads</div>
<div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;//     reads a bunch of neurons parameters and determines if it needs to fire or not</div>
<div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;//     Whenever a neuron need to fire, it keeps track of the fired neuron in the local</div>
<div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;//     table. When the table is full, we go and write back the fireTable to the global</div>
<div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;//     firing table. </div>
<div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;//  2. Firing information is maintained in two tables globally (timingTable and the globalFiringTable)</div>
<div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;//     for excitatory neuron population and inhibitory neurons.</div>
<div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;//     The globalFiringTable only stores a sequence of id corresponding to fired neurons.</div>
<div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;//     The timingTable store the total number of fired neurons till the current time step t.</div>
<div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;//     These two tables are flushed and adjusted every second.</div>
<div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;//     This approach requires about half of the memory compared to the traditional AER scheme which</div>
<div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;//     stores the firing time and firing id together.</div>
<div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;//  For more details kindly read the enclosed report (report.pdf) in the source directory</div>
<div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;//</div>
<div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;//</div>
<div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;//  timeTableD2GPU[0] always is 0 -- index into firingTableD2</div>
<div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;//  timeTableD2GPU[maxDelay_] -- should be the number of spikes &quot;leftover&quot; from the previous second</div>
<div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;// timeTableD2GPU[maxDelay_+1]-timeTableD2GPU[maxDelay_] -- should be the number of spikes in the first ms of the current second</div>
<div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;//  timeTableD2GPU[1000+maxDelay_] -- should be the number of spikes in the current second + the leftover spikes.</div>
<div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;//</div>
<div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;///////////////////////////////////////////////////////////////////</div>
<div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;</div>
<div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;__device__ unsigned int  timeTableD2GPU[TIMING_COUNT];</div>
<div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;__device__ unsigned int  timeTableD1GPU[TIMING_COUNT];</div>
<div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;</div>
<div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;__device__ unsigned int    spikeCountD2SecGPU;</div>
<div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;__device__ unsigned int    spikeCountD1SecGPU;</div>
<div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;__device__ unsigned int spikeCountD2GPU;</div>
<div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;__device__ unsigned int spikeCountD1GPU;</div>
<div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;</div>
<div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;__device__ unsigned int    secD2fireCntTest;</div>
<div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;__device__ unsigned int    secD1fireCntTest;</div>
<div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;</div>
<div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;__device__ unsigned int spikeCountLastSecLeftD2GPU;</div>
<div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;</div>
<div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;__device__ unsigned int spikeCountExtRxD1SecGPU;</div>
<div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;__device__ unsigned int spikeCountExtRxD2SecGPU;</div>
<div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;__device__ unsigned int spikeCountExtRxD2GPU;</div>
<div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;__device__ unsigned int spikeCountExtRxD1GPU;</div>
<div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;</div>
<div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;__device__ __constant__ RuntimeData     runtimeDataGPU;</div>
<div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;__device__ __constant__ NetworkConfigRT    networkConfigGPU;</div>
<div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;__device__ __constant__ GroupConfigRT   groupConfigsGPU[MAX_GRP_PER_SNN];</div>
<div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;</div>
<div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;__device__ __constant__ float               d_mulSynFast[MAX_CONN_PER_SNN];</div>
<div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;__device__ __constant__ float               d_mulSynSlow[MAX_CONN_PER_SNN];</div>
<div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;</div>
<div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;__device__  int      loadBufferCount; </div>
<div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;__device__  int   loadBufferSize;</div>
<div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;</div>
<div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;texture &lt;int,    1, cudaReadModeElementType&gt;  timeTableD2GPU_tex;</div>
<div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;texture &lt;int,    1, cudaReadModeElementType&gt;  timeTableD1GPU_tex;</div>
<div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;texture &lt;int,    1, cudaReadModeElementType&gt;  groupIdInfo_tex; // groupIDInfo is allocated using cudaMalloc thus doesn&#39;t require an offset when using textures</div>
<div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;__device__  int timeTableD1GPU_tex_offset;</div>
<div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;__device__  int timeTableD2GPU_tex_offset;</div>
<div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div>
<div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;// example of the quick synaptic table</div>
<div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;// index     cnt</div>
<div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;// 0000000 - 0</div>
<div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;// 0000001 - 0</div>
<div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;// 0000010 - 1</div>
<div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;// 0100000 - 5</div>
<div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;// 0110000 - 4</div>
<div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;int quickSynIdTable[256];</div>
<div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;__device__ int  quickSynIdTableGPU[256];</div>
<div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;void initQuickSynIdTable(int netId) {</div>
<div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;   void* devPtr;</div>
<div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;      </div>
<div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;   for(int i = 1; i &lt; 256; i++) {</div>
<div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;       int cnt = 0;</div>
<div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;       while(i) {</div>
<div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;           if(((i &gt;&gt; cnt) &amp; 1) == 1) break;</div>
<div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;           cnt++;</div>
<div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;           assert(cnt &lt;= 7);</div>
<div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;       }</div>
<div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;       quickSynIdTable[i] = cnt;        </div>
<div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;   }</div>
<div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;</div>
<div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;   cudaSetDevice(netId);</div>
<div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;   cudaGetSymbolAddress(&amp;devPtr, quickSynIdTableGPU);</div>
<div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy( devPtr, quickSynIdTable, sizeof(quickSynIdTable), cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;}</div>
<div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;</div>
<div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;__device__ inline bool isPoissonGroup(short int lGrpId) {</div>
<div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;   return (groupConfigsGPU[lGrpId].Type &amp; POISSON_NEURON);</div>
<div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;}</div>
<div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;</div>
<div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;__device__ inline void setFiringBitSynapses(int lNId, int synId) {</div>
<div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;   unsigned int* tmp_I_set_p = ((unsigned int*)((char*)runtimeDataGPU.I_set + ((synId &gt;&gt; 5) * networkConfigGPU.I_setPitch)) + lNId);</div>
<div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;   atomicOr(tmp_I_set_p, 1 &lt;&lt; (synId % 32));</div>
<div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;}</div>
<div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;</div>
<div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;__device__ inline unsigned int* getFiringBitGroupPtr(int lNId, int synId) {</div>
<div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;   return (((unsigned int*)((char*)runtimeDataGPU.I_set + synId * networkConfigGPU.I_setPitch)) + lNId);</div>
<div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;}</div>
<div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;</div>
<div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;__device__ inline int getSTPBufPos(int lNId, int simTime) {</div>
<div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;   return (((simTime + 1) % (networkConfigGPU.maxDelay + 1)) * networkConfigGPU.STP_Pitch + lNId);</div>
<div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;}</div>
<div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;</div>
<div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;__device__ inline int2 getStaticThreadLoad(int bufPos) {</div>
<div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;   return (runtimeDataGPU.neuronAllocation[bufPos]);</div>
<div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;}</div>
<div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;</div>
<div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;__device__ inline bool getPoissonSpike(int lNId) {</div>
<div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;   // Random number value is less than the poisson firing probability</div>
<div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;   // if poisson firing probability is say 1.0 then the random poisson ptr</div>
<div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;   // will always be less than 1.0 and hence it will continiously fire</div>
<div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;   return runtimeDataGPU.randNum[lNId - networkConfigGPU.numNReg] * 1000.0f</div>
<div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;           &lt; runtimeDataGPU.poissonFireRate[lNId - networkConfigGPU.numNReg];</div>
<div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;}</div>
<div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;</div>
<div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;__device__ inline bool getSpikeGenBit(unsigned int nidPos) {</div>
<div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;   const int nidBitPos = nidPos % 32;</div>
<div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;   const int nidIndex  = nidPos / 32;</div>
<div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;   return ((runtimeDataGPU.spikeGenBits[nidIndex] &gt;&gt; nidBitPos) &amp; 0x1);</div>
<div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;}</div>
<div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;</div>
<div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;/*!</div>
<div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160; * \brief This device function updates the average firing rate of each neuron, which is required for homeostasis</div>
<div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160; *</div>
<div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160; * \param[in] lNId The neuron id to be updated</div>
<div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160; * \param[in] lGrpId The group id of the neuron</div>
<div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160; */</div>
<div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;__device__ inline void updateHomeoStaticState(int lNId, int lGrpId) {</div>
<div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;   // here the homeostasis adjustment</div>
<div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;   runtimeDataGPU.avgFiring[lNId] *= (groupConfigsGPU[lGrpId].avgTimeScale_decay);</div>
<div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;}</div>
<div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;</div>
<div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;/*!</div>
<div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160; * \brief After every time step we update the time table</div>
<div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160; *</div>
<div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160; * Only one cuda thread is required for updating the time table</div>
<div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160; *</div>
<div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160; * \param[in] simTime The current time step</div>
<div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160; */</div>
<div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;__global__ void kernel_updateTimeTable(int simTime) {</div>
<div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;   if (threadIdx.x == 0 &amp;&amp; blockIdx.x == 0) {</div>
<div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;       timeTableD2GPU[simTime + networkConfigGPU.maxDelay + 1] = spikeCountD2SecGPU + spikeCountLastSecLeftD2GPU;</div>
<div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;       timeTableD1GPU[simTime + networkConfigGPU.maxDelay + 1] = spikeCountD1SecGPU;</div>
<div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;   }</div>
<div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;   __syncthreads();                                         </div>
<div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;}</div>
<div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;</div>
<div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;/////////////////////////////////////////////////////////////////////////////////</div>
<div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;// Device Kernel Function:  Intialization of the GPU side of the simulator    ///</div>
<div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;// KERNEL: This kernel is called after initialization of various parameters   ///</div>
<div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;// so that we can reset all required parameters.                              ///</div>
<div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;/////////////////////////////////////////////////////////////////////////////////</div>
<div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;__global__ void kernel_initGPUMemory() {</div>
<div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;   // FIXME: use parallel access</div>
<div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;   int timeTableIdx = blockIdx.x * blockDim.x + threadIdx.x;</div>
<div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;</div>
<div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;   if (timeTableIdx &lt; TIMING_COUNT) {</div>
<div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;       timeTableD2GPU[timeTableIdx] = 0;</div>
<div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;       timeTableD1GPU[timeTableIdx] = 0;</div>
<div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;   }</div>
<div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;</div>
<div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;   if (threadIdx.x == 0 &amp;&amp; blockIdx.x == 0) {</div>
<div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;       spikeCountD2SecGPU = 0;</div>
<div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;       spikeCountD1SecGPU = 0;</div>
<div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;       spikeCountD2GPU = 0;</div>
<div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;       spikeCountD1GPU = 0;</div>
<div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;</div>
<div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;       secD2fireCntTest = 0;</div>
<div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;       secD1fireCntTest = 0;</div>
<div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;</div>
<div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;       spikeCountLastSecLeftD2GPU = 0;</div>
<div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;</div>
<div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;       spikeCountExtRxD2GPU = 0;</div>
<div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;       spikeCountExtRxD1GPU = 0;</div>
<div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;       spikeCountExtRxD2SecGPU = 0;</div>
<div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;       spikeCountExtRxD1SecGPU = 0;</div>
<div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;   }</div>
<div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;}</div>
<div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;</div>
<div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;// Allocation of the group and its id..</div>
<div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;void SNN::allocateGroupId(int netId) {</div>
<div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;</div>
<div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;   assert (runtimeData[netId].groupIdInfo == NULL);</div>
<div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;   int3* tempNeuronAllocation = (int3*)malloc(sizeof(int3) * networkConfigs[netId].numGroups);</div>
<div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;   for (int lGrpId = 0; lGrpId &lt; networkConfigs[netId].numGroups; lGrpId++) {</div>
<div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;       int3  threadLoad;</div>
<div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;       threadLoad.x = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;       threadLoad.y = groupConfigs[netId][lGrpId].lEndN;</div>
<div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;       threadLoad.z = lGrpId;</div>
<div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;       tempNeuronAllocation[lGrpId] = threadLoad;</div>
<div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;   }</div>
<div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;</div>
<div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;   CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;runtimeData[netId].groupIdInfo, sizeof(int3) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(runtimeData[netId].groupIdInfo, tempNeuronAllocation, sizeof(int3) * networkConfigs[netId].numGroups, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;   CUDA_CHECK_ERRORS(cudaBindTexture(NULL, groupIdInfo_tex, runtimeData[netId].groupIdInfo, sizeof(int3) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;</div>
<div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;   free(tempNeuronAllocation);</div>
<div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;}</div>
<div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;</div>
<div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;/************************ VARIOUS KERNELS FOR FIRING CALCULATION AND FIRING UPDATE ****************************/</div>
<div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;// Static Thread Load Allocation...</div>
<div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;// This function is necessary for static allocation of load that each CUDA-SM needs for its computation.</div>
<div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;// We store the static load allocation using the following format</div>
<div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;// Neuron starting position (32 bit): Group identification (16) : Buffer size (16 bit)</div>
<div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;// if we have 3 groups. grp(1) = 400 neurons, grp(2) = 100, grp(3) = 600</div>
<div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;// The allocated static table will look as follows..</div>
<div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;//-------------------------</div>
<div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;// start |  grp   |   size</div>
<div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;//-------------------------</div>
<div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;//    0  :   0    :   256</div>
<div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;//  256  :   0    :   144</div>
<div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;//  400  :   1    :   100</div>
<div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;//  500  :   2    :   256</div>
<div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;//  756  :   2    :   256</div>
<div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;// 1012  :   2    :    88</div>
<div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;//-----------------------</div>
<div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;int SNN::allocateStaticLoad(int netId, int bufSize) {</div>
<div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;</div>
<div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;   // only one thread does the static load table</div>
<div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;   int bufferCnt = 0;</div>
<div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;   for (int lGrpId = 0; lGrpId &lt; networkConfigs[netId].numGroups; lGrpId++) {</div>
<div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;       int grpBufCnt = (int) ceil(1.0f * groupConfigs[netId][lGrpId].numN / bufSize);</div>
<div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;       assert(grpBufCnt &gt;= 0);</div>
<div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;       bufferCnt += grpBufCnt;</div>
<div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;       KERNEL_DEBUG(&quot;Grp Size = %d, Total Buffer Cnt = %d, Buffer Cnt = %d&quot;, groupConfigs[netId][lGrpId].numN, bufferCnt, grpBufCnt);</div>
<div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;   }</div>
<div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;   assert(bufferCnt &gt; 0);</div>
<div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;</div>
<div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;   int2*  tempNeuronAllocation = (int2*)malloc(sizeof(int2) * bufferCnt);</div>
<div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;   KERNEL_DEBUG(&quot;STATIC THREAD ALLOCATION&quot;);</div>
<div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;   KERNEL_DEBUG(&quot;------------------------&quot;);</div>
<div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;   KERNEL_DEBUG(&quot;Buffer Size = %d, Buffer Count = %d&quot;, bufSize, bufferCnt);</div>
<div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;</div>
<div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;   bufferCnt = 0;</div>
<div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;   for (int lGrpId = 0; lGrpId &lt; networkConfigs[netId].numGroups; lGrpId++) {</div>
<div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;       for (int lNId = groupConfigs[netId][lGrpId].lStartN; lNId &lt;= groupConfigs[netId][lGrpId].lEndN; lNId += bufSize) {</div>
<div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;           int2  threadLoad;</div>
<div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;           // starting neuron id is saved...</div>
<div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;           threadLoad.x = lNId;</div>
<div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;           if ((lNId + bufSize - 1) &lt;= groupConfigs[netId][lGrpId].lEndN)</div>
<div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;               // grpID + full size</div>
<div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;               threadLoad.y = (lGrpId + (bufSize &lt;&lt; 16)); // can&#39;t support group id &gt; 2^16</div>
<div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;           else</div>
<div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;               // grpID + left-over size</div>
<div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;               threadLoad.y = (lGrpId + ((groupConfigs[netId][lGrpId].lEndN - lNId + 1) &lt;&lt; 16)); // can&#39;t support group id &gt; 2^16</div>
<div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;</div>
<div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;           // fill the static load distribution here...</div>
<div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;           int testGrpId = STATIC_LOAD_GROUP(threadLoad);</div>
<div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;           tempNeuronAllocation[bufferCnt] = threadLoad;</div>
<div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;           KERNEL_DEBUG(&quot;%d. Start=%d, size=%d grpId=%d:%s (SpikeMonId=%d) (GroupMonId=%d)&quot;,</div>
<div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;                   bufferCnt, STATIC_LOAD_START(threadLoad),</div>
<div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;                   STATIC_LOAD_SIZE(threadLoad),</div>
<div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;                   STATIC_LOAD_GROUP(threadLoad),</div>
<div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;                   groupConfigMap[groupConfigs[netId][testGrpId].gGrpId].grpName.c_str(),</div>
<div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;                   groupConfigMDMap[groupConfigs[netId][testGrpId].gGrpId].spikeMonitorId,</div>
<div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;                   groupConfigMDMap[groupConfigs[netId][testGrpId].gGrpId].groupMonitorId);</div>
<div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;           bufferCnt++;</div>
<div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;       }</div>
<div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;   }</div>
<div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;</div>
<div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;   assert(runtimeData[netId].allocated == false);</div>
<div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;   // Finally writeback the total bufferCnt</div>
<div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;   // Note down the buffer size for reference</div>
<div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;   KERNEL_DEBUG(&quot;GPU loadBufferSize = %d, GPU loadBufferCount = %d&quot;, bufSize, bufferCnt);</div>
<div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(loadBufferCount, &amp;bufferCnt, sizeof(int), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(loadBufferSize, &amp;bufSize, sizeof(int), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;   CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;runtimeData[netId].neuronAllocation, sizeof(int2) * bufferCnt));</div>
<div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(runtimeData[netId].neuronAllocation, tempNeuronAllocation, sizeof(int2) * bufferCnt, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;   free(tempNeuronAllocation);</div>
<div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;   return bufferCnt;</div>
<div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;}</div>
<div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;</div>
<div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;//////////////////////////////////////////////////</div>
<div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;// 1. KERNELS used when a specific neuron fires //</div>
<div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;//////////////////////////////////////////////////</div>
<div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;</div>
<div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;/////////////////////////////////////////////////////////////////////////////////</div>
<div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;// Device local function:          Update the STP Variables                      ///</div>
<div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;// update the STPU and STPX variable after firing                             ///</div>
<div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;/////////////////////////////////////////////////////////////////////////////////</div>
<div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;</div>
<div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;// update the spike-dependent part of du/dt and dx/dt</div>
<div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;__device__ void firingUpdateSTP (int nid, int simTime, short int grpId) {</div>
<div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;   // we need to retrieve the STP values from the right buffer position (right before vs. right after the spike)</div>
<div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;   int ind_plus  = getSTPBufPos(nid, simTime);</div>
<div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;   int ind_minus = getSTPBufPos(nid, (simTime - 1));</div>
<div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;</div>
<div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;   // at this point, stpu[ind_plus] has already been assigned, and the decay applied</div>
<div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;   // so add the spike-dependent part to that</div>
<div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;   // du/dt = -u/tau_F + U * (1-u^-) * \delta(t-t_{spk})</div>
<div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;   runtimeDataGPU.stpu[ind_plus] += groupConfigsGPU[grpId].STP_U * (1.0f - runtimeDataGPU.stpu[ind_minus]);</div>
<div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;</div>
<div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;   // dx/dt = (1-x)/tau_D - u^+ * x^- * \delta(t-t_{spk})</div>
<div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;   runtimeDataGPU.stpx[ind_plus] -= runtimeDataGPU.stpu[ind_plus] * runtimeDataGPU.stpx[ind_minus];</div>
<div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;}</div>
<div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;</div>
<div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;__device__ void resetFiredNeuron(int lNId, short int lGrpId, int simTime) {</div>
<div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;   // \FIXME \TODO: convert this to use coalesced access by grouping into a</div>
<div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;   // single 16 byte access. This might improve bandwidth performance</div>
<div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;   // This is fully uncoalsced access...need to convert to coalsced access..</div>
<div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;   runtimeDataGPU.voltage[lNId] = runtimeDataGPU.Izh_c[lNId];</div>
<div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;   runtimeDataGPU.recovery[lNId] += runtimeDataGPU.Izh_d[lNId];</div>
<div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;   if (groupConfigsGPU[lGrpId].WithSTDP)</div>
<div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;       runtimeDataGPU.lastSpikeTime[lNId] = simTime;</div>
<div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;   </div>
<div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;   if (networkConfigGPU.sim_with_homeostasis) {</div>
<div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;       // with homeostasis flag can be used here.</div>
<div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;       runtimeDataGPU.avgFiring[lNId] += 1000/(groupConfigsGPU[lGrpId].avgTimeScale*1000);</div>
<div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;   }</div>
<div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;}</div>
<div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;</div>
<div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;/*!</div>
<div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160; * \brief 1. Copy neuron id from local table to global firing table. 2. Reset all neuron properties of neuron id in local table</div>
<div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160; *</div>
<div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160; *</div>
<div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160; * \param[in] fireTablePtr the local shared memory firing table with neuron ids of fired neuron</div>
<div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160; * \param[in] fireCntD2 the number of neurons in local table that has fired with group&#39;s max delay == 1</div>
<div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160; * \param[in] fireCntD1 the number of neurons in local table that has fired with group&#39;s max delay &gt; 1</div>
<div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160; * \param[in] simTime the current time step, stored as neuron firing time  entry</div>
<div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160; */</div>
<div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;__device__ void updateSpikeCount(volatile unsigned int&amp; fireCnt, volatile unsigned int&amp; fireCntD1, volatile unsigned int&amp; cntD2, volatile unsigned int&amp; cntD1, volatile int&amp;  blkErrCode) {</div>
<div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;   int fireCntD2 = fireCnt - fireCntD1;</div>
<div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;</div>
<div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;   cntD2 = atomicAdd(&amp;secD2fireCntTest, fireCntD2);</div>
<div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;   cntD1 = atomicAdd(&amp;secD1fireCntTest, fireCntD1);</div>
<div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;</div>
<div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;   //check for overflow in the firing table size....</div>
<div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;   if(secD2fireCntTest&gt;networkConfigGPU.maxSpikesD2) {</div>
<div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;       blkErrCode = NEW_FIRE_UPDATE_OVERFLOW_ERROR2;</div>
<div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;       return;</div>
<div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;   }</div>
<div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;   else if(secD1fireCntTest&gt;networkConfigGPU.maxSpikesD1) {</div>
<div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;       blkErrCode = NEW_FIRE_UPDATE_OVERFLOW_ERROR1;</div>
<div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;       return;</div>
<div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;   }</div>
<div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;   blkErrCode = 0;</div>
<div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;</div>
<div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;   // get a distinct counter to store firing info</div>
<div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;   // into the firing table</div>
<div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;   cntD2 = atomicAdd(&amp;spikeCountD2SecGPU, fireCntD2) + spikeCountLastSecLeftD2GPU;</div>
<div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;   cntD1 = atomicAdd(&amp;spikeCountD1SecGPU, fireCntD1);</div>
<div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;}</div>
<div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;</div>
<div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;// update the firing table...</div>
<div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;__device__ void updateFiringTable(int lNId, short int lGrpId, volatile unsigned int&amp; cntD2, volatile unsigned int&amp; cntD1) {</div>
<div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;   int pos;</div>
<div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;   if (groupConfigsGPU[lGrpId].MaxDelay == 1) {</div>
<div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;       // this group has a delay of only 1</div>
<div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;       pos = atomicAdd((int*)&amp;cntD1, 1);</div>
<div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;       //runtimeDataGPU.firingTableD1[pos]  = SET_FIRING_TABLE(nid, grpId);</div>
<div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;       runtimeDataGPU.firingTableD1[pos] = lNId;</div>
<div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;   } else {</div>
<div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;       // all other groups is dumped here </div>
<div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;       pos = atomicAdd((int*)&amp;cntD2, 1);</div>
<div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;       //runtimeDataGPU.firingTableD2[pos]  = SET_FIRING_TABLE(nid, grpId);</div>
<div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;       runtimeDataGPU.firingTableD2[pos] = lNId;</div>
<div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;   }</div>
<div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;}</div>
<div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;</div>
<div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;// update the firing table...</div>
<div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;__device__ void updateExtFiringTable(int lNId, short int lGrpId) {</div>
<div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;   int pos;</div>
<div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;   if (groupConfigsGPU[lGrpId].MaxDelay == 1) {</div>
<div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;       // this group has a delay of only 1</div>
<div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;       pos = atomicAdd((int*)&amp;runtimeDataGPU.extFiringTableEndIdxD1[lGrpId] , 1);</div>
<div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;       //runtimeDataGPU.firingTableD1[pos]  = SET_FIRING_TABLE(nid, grpId);</div>
<div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;       runtimeDataGPU.extFiringTableD1[lGrpId][pos] = lNId + groupConfigsGPU[lGrpId].LtoGOffset; // convert to global neuron id</div>
<div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;   } else {</div>
<div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;       // all other groups is dumped here </div>
<div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;       pos = atomicAdd((int*)&amp;runtimeDataGPU.extFiringTableEndIdxD2[lGrpId], 1);</div>
<div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;       //runtimeDataGPU.firingTableD2[pos]  = SET_FIRING_TABLE(nid, grpId);</div>
<div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;       runtimeDataGPU.extFiringTableD2[lGrpId][pos] = lNId + groupConfigsGPU[lGrpId].LtoGOffset; // convert to global neuron id</div>
<div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;   }</div>
<div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;}</div>
<div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;</div>
<div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;__device__ int updateNewFirings(int* fireTablePtr, short int* fireGrpId,</div>
<div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;                                volatile unsigned int&amp; fireCnt, volatile unsigned int&amp; fireCntD1, int simTime) {</div>
<div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;   __shared__ volatile unsigned int cntD2;</div>
<div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;   __shared__ volatile unsigned int cntD1;</div>
<div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;   __shared__ volatile int blkErrCode;</div>
<div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;</div>
<div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;   blkErrCode = 0;</div>
<div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;   if (threadIdx.x == 0) {</div>
<div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;       updateSpikeCount(fireCnt, fireCntD1, cntD2, cntD1, blkErrCode);</div>
<div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;   }</div>
<div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;</div>
<div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;</div>
<div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;   // if we overflow the spike buffer space that is available,</div>
<div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;   // then we return with an error here...</div>
<div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;   if (blkErrCode)</div>
<div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;       return blkErrCode;</div>
<div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;</div>
<div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;   for (int i = threadIdx.x; i &lt; fireCnt; i += blockDim.x) {</div>
<div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;       // Read the firing id from the local table.....</div>
<div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;       int lNId = fireTablePtr[i];</div>
<div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;</div>
<div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;       updateFiringTable(lNId, fireGrpId[i], cntD2, cntD1);</div>
<div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;</div>
<div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;       if (groupConfigsGPU[fireGrpId[i]].hasExternalConnect)</div>
<div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;           updateExtFiringTable(lNId, fireGrpId[i]);</div>
<div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;</div>
<div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;       if (groupConfigsGPU[fireGrpId[i]].WithSTP)</div>
<div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;           firingUpdateSTP(lNId, simTime, fireGrpId[i]);</div>
<div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;</div>
<div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;       // keep track of number spikes per neuron</div>
<div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;       runtimeDataGPU.nSpikeCnt[lNId]++;</div>
<div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;</div>
<div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;       // only neurons would do the remaining settings...</div>
<div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;       // pure poisson generators will return without changing anything else..</div>
<div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;       if (IS_REGULAR_NEURON(lNId, networkConfigGPU.numNReg, networkConfigGPU.numNPois))</div>
<div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;           resetFiredNeuron(lNId, fireGrpId[i], simTime);</div>
<div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;   }</div>
<div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;</div>
<div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;</div>
<div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;    return 0;</div>
<div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;}</div>
<div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;</div>
<div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;// zero GPU spike counts</div>
<div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;__global__ void kernel_resetNSpikeCnt(int lGrpId) {</div>
<div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;   const int totBuffers = loadBufferCount;</div>
<div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;</div>
<div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;   for (int bufPos = blockIdx.x; bufPos &lt; totBuffers; bufPos += gridDim.x) {</div>
<div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;       // KILLME !!! This can be further optimized ....</div>
<div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;       // instead of reading each neuron group separately .....</div>
<div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;       // read a whole buffer and use the result ......</div>
<div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;       int2 threadLoad  = getStaticThreadLoad(bufPos);</div>
<div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;       int nid = (STATIC_LOAD_START(threadLoad) + threadIdx.x);</div>
<div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;       int  lastId = STATIC_LOAD_SIZE(threadLoad);</div>
<div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;       int  grpId = STATIC_LOAD_GROUP(threadLoad);</div>
<div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;</div>
<div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;       if ((lGrpId == ALL || lGrpId == grpId) &amp;&amp; (nid &lt;= lastId)) {</div>
<div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;           runtimeDataGPU.nSpikeCnt[nid] = 0;</div>
<div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;       }</div>
<div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;   }</div>
<div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;}</div>
<div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;</div>
<div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;// wrapper to call resetSpikeCnt</div>
<div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;void SNN::resetSpikeCnt_GPU(int netId, int lGrpId) {</div>
<div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;</div>
<div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;   if (lGrpId == ALL) {</div>
<div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;       checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;       CUDA_CHECK_ERRORS(cudaMemset((void*)runtimeData[netId].nSpikeCnt, 0, sizeof(int) * networkConfigs[netId].numN));</div>
<div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;   } else {</div>
<div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;       checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;       kernel_resetNSpikeCnt&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(lGrpId);</div>
<div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;   }</div>
<div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;}</div>
<div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;</div>
<div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;#define LTP_GROUPING_SZ 16 //!&lt; synaptic grouping for LTP Calculation</div>
<div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;/*!</div>
<div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160; * \brief Computes the STDP update values for each of fired neurons stored in the local firing table.</div>
<div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160; *</div>
<div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160; * \param[in] fireTablePtr the local firing table with neuron ids of fired neuron</div>
<div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160; * \param[in] fireCnt the number of fired neurons in local firing table</div>
<div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160; * \param[in] simTime the current time step, stored as neuron firing time entry</div>
<div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160; */</div>
<div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;__device__ void updateLTP(int* fireTablePtr, short int* fireGrpId, volatile unsigned int&amp; fireCnt, int simTime) {</div>
<div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;   for(int pos=threadIdx.x/LTP_GROUPING_SZ; pos &lt; fireCnt; pos += (blockDim.x/LTP_GROUPING_SZ))  {</div>
<div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;       // each neuron has two variable pre and pre_exc</div>
<div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;       // pre: number of pre-neuron</div>
<div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;       // pre_exc: number of neuron had has plastic connections</div>
<div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;       short int grpId = fireGrpId[pos];</div>
<div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;</div>
<div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;       // STDP calculation: the post-synaptic neron fires after the arrival of pre-synaptic neuron&#39;s spike</div>
<div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;       if (groupConfigsGPU[grpId].WithSTDP) { // MDR, FIXME this probably will cause more thread divergence than need be...</div>
<div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;           int  nid   = fireTablePtr[pos];</div>
<div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;           unsigned int  end_p = runtimeDataGPU.cumulativePre[nid] + runtimeDataGPU.Npre_plastic[nid];</div>
<div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;           for(unsigned int p  = runtimeDataGPU.cumulativePre[nid] + threadIdx.x % LTP_GROUPING_SZ;</div>
<div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;                   p &lt; end_p;</div>
<div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;                   p+=LTP_GROUPING_SZ) {</div>
<div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;               int stdp_tDiff = (simTime - runtimeDataGPU.synSpikeTime[p]);</div>
<div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;               if (stdp_tDiff &gt; 0) {</div>
<div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;                   if (groupConfigsGPU[grpId].WithESTDP) {</div>
<div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;                       // Handle E-STDP curves</div>
<div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;                       switch (groupConfigsGPU[grpId].WithESTDPcurve) {</div>
<div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;                       case EXP_CURVE: // exponential curve</div>
<div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;                           if (stdp_tDiff * groupConfigsGPU[grpId].TAU_PLUS_INV_EXC &lt; 25)</div>
<div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;                               runtimeDataGPU.wtChange[p] += STDP(stdp_tDiff, groupConfigsGPU[grpId].ALPHA_PLUS_EXC, groupConfigsGPU[grpId].TAU_PLUS_INV_EXC);</div>
<div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;                           break;</div>
<div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;                       case TIMING_BASED_CURVE: // sc curve</div>
<div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;                           if (stdp_tDiff * groupConfigsGPU[grpId].TAU_PLUS_INV_EXC &lt; 25) {</div>
<div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;                               if (stdp_tDiff &lt;= groupConfigsGPU[grpId].GAMMA)</div>
<div class="line"><a name="l00544"></a><span class="lineno">  544</span>&#160;                                   runtimeDataGPU.wtChange[p] += groupConfigsGPU[grpId].OMEGA + groupConfigsGPU[grpId].KAPPA * STDP(stdp_tDiff, groupConfigsGPU[grpId].ALPHA_PLUS_EXC, groupConfigsGPU[grpId].TAU_PLUS_INV_EXC);</div>
<div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;                               else // stdp_tDiff &gt; GAMMA</div>
<div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;                                   runtimeDataGPU.wtChange[p] -= STDP(stdp_tDiff, groupConfigsGPU[grpId].ALPHA_PLUS_EXC, groupConfigsGPU[grpId].TAU_PLUS_INV_EXC);</div>
<div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;                           }</div>
<div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;                           break;</div>
<div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;                       default:</div>
<div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;                           break;</div>
<div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;                       }</div>
<div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;                   }</div>
<div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;                   if (groupConfigsGPU[grpId].WithISTDP) {</div>
<div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;                       // Handle I-STDP curves</div>
<div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;                       switch (groupConfigsGPU[grpId].WithISTDPcurve) {</div>
<div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;                       case EXP_CURVE: // exponential curve</div>
<div class="line"><a name="l00557"></a><span class="lineno">  557</span>&#160;                           if (stdp_tDiff * groupConfigsGPU[grpId].TAU_PLUS_INV_INB &lt; 25) { // LTP of inhibitory synapse, which decreases synapse weight</div>
<div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;                               runtimeDataGPU.wtChange[p] -= STDP(stdp_tDiff, groupConfigsGPU[grpId].ALPHA_PLUS_INB, groupConfigsGPU[grpId].TAU_PLUS_INV_INB);</div>
<div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;                           }</div>
<div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;                           break;</div>
<div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;                       case PULSE_CURVE: // pulse curve</div>
<div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;                           if (stdp_tDiff &lt;= groupConfigsGPU[grpId].LAMBDA) { // LTP of inhibitory synapse, which decreases synapse weight</div>
<div class="line"><a name="l00563"></a><span class="lineno">  563</span>&#160;                               runtimeDataGPU.wtChange[p] -= groupConfigsGPU[grpId].BETA_LTP;</div>
<div class="line"><a name="l00564"></a><span class="lineno">  564</span>&#160;                           } else if (stdp_tDiff &lt;= groupConfigsGPU[grpId].DELTA) { // LTD of inhibitory syanpse, which increase sysnapse weight</div>
<div class="line"><a name="l00565"></a><span class="lineno">  565</span>&#160;                               runtimeDataGPU.wtChange[p] -= groupConfigsGPU[grpId].BETA_LTD;</div>
<div class="line"><a name="l00566"></a><span class="lineno">  566</span>&#160;                           }</div>
<div class="line"><a name="l00567"></a><span class="lineno">  567</span>&#160;                           break;</div>
<div class="line"><a name="l00568"></a><span class="lineno">  568</span>&#160;                       default:</div>
<div class="line"><a name="l00569"></a><span class="lineno">  569</span>&#160;                           break;</div>
<div class="line"><a name="l00570"></a><span class="lineno">  570</span>&#160;                       }</div>
<div class="line"><a name="l00571"></a><span class="lineno">  571</span>&#160;                   }</div>
<div class="line"><a name="l00572"></a><span class="lineno">  572</span>&#160;               }</div>
<div class="line"><a name="l00573"></a><span class="lineno">  573</span>&#160;           }</div>
<div class="line"><a name="l00574"></a><span class="lineno">  574</span>&#160;       }</div>
<div class="line"><a name="l00575"></a><span class="lineno">  575</span>&#160;   }</div>
<div class="line"><a name="l00576"></a><span class="lineno">  576</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l00577"></a><span class="lineno">  577</span>&#160;}</div>
<div class="line"><a name="l00578"></a><span class="lineno">  578</span>&#160;</div>
<div class="line"><a name="l00579"></a><span class="lineno">  579</span>&#160;#define FIRE_CHUNK_CNT 512</div>
<div class="line"><a name="l00580"></a><span class="lineno">  580</span>&#160;/*!</div>
<div class="line"><a name="l00581"></a><span class="lineno">  581</span>&#160; * \brief This kernel is responsible for finding the neurons that need to be fired.</div>
<div class="line"><a name="l00582"></a><span class="lineno">  582</span>&#160; *</div>
<div class="line"><a name="l00583"></a><span class="lineno">  583</span>&#160; * We use a buffered firing table that allows neuron to gradually load</div>
<div class="line"><a name="l00584"></a><span class="lineno">  584</span>&#160; * the buffer and make it easy to carry out the calculations in a single group.</div>
<div class="line"><a name="l00585"></a><span class="lineno">  585</span>&#160; * A single function is used for simple neurons and also for poisson neurons.</div>
<div class="line"><a name="l00586"></a><span class="lineno">  586</span>&#160; * The function also update LTP</div>
<div class="line"><a name="l00587"></a><span class="lineno">  587</span>&#160; *</div>
<div class="line"><a name="l00588"></a><span class="lineno">  588</span>&#160; * device access: spikeCountD2SecGPU, spikeCountD1SecGPU</div>
<div class="line"><a name="l00589"></a><span class="lineno">  589</span>&#160; * net access: numNReg numNPois, numN, sim_with_stdp, sim_in_testing, sim_with_homeostasis, maxSpikesD1, maxSpikesD2</div>
<div class="line"><a name="l00590"></a><span class="lineno">  590</span>&#160; * grp access: Type, spikeGenFunc, Noffset, withSpikeCounter, spkCntBufPos, StartN, WithSTP, avgTimeScale</div>
<div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;               WithSTDP, WithESTDP, WithISTDP, WithESTDPCurve, With ISTDPCurve, all STDP parameters</div>
<div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160; * rtd access: randNum, poissonFireRate, spkCntBuf, nSpikeCnt, voltage, recovery, Izh_c, Izh_d</div>
<div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160; *             cumulativePre, Npre_plastic, (R)synSpikeTime, (W)lastSpikeTime, (W)wtChange,</div>
<div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160; *             avgFiring</div>
<div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160; */</div>
<div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;__global__     void kernel_findFiring (int simTime) {</div>
<div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;   __shared__ volatile unsigned int fireCnt;</div>
<div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;   __shared__ volatile unsigned int fireCntTest;</div>
<div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;   __shared__ volatile unsigned int fireCntD1;</div>
<div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;   __shared__ int      fireTable[FIRE_CHUNK_CNT];</div>
<div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;   __shared__ short int    fireGrpId[FIRE_CHUNK_CNT];</div>
<div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;   __shared__ volatile int errCode;</div>
<div class="line"><a name="l00603"></a><span class="lineno">  603</span>&#160;</div>
<div class="line"><a name="l00604"></a><span class="lineno">  604</span>&#160;   if (threadIdx.x == 0) {</div>
<div class="line"><a name="l00605"></a><span class="lineno">  605</span>&#160;       fireCnt   = 0; // initialize total cnt to 0</div>
<div class="line"><a name="l00606"></a><span class="lineno">  606</span>&#160;       fireCntD1  = 0; // initialize d1 cnt to 0</div>
<div class="line"><a name="l00607"></a><span class="lineno">  607</span>&#160;       fireCntTest = 0; // initialize test cnt to 0</div>
<div class="line"><a name="l00608"></a><span class="lineno">  608</span>&#160;   }</div>
<div class="line"><a name="l00609"></a><span class="lineno">  609</span>&#160;</div>
<div class="line"><a name="l00610"></a><span class="lineno">  610</span>&#160;   const int totBuffers=loadBufferCount;</div>
<div class="line"><a name="l00611"></a><span class="lineno">  611</span>&#160;</div>
<div class="line"><a name="l00612"></a><span class="lineno">  612</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l00613"></a><span class="lineno">  613</span>&#160;</div>
<div class="line"><a name="l00614"></a><span class="lineno">  614</span>&#160;   for (int bufPos = blockIdx.x; bufPos &lt; totBuffers; bufPos += gridDim.x) {</div>
<div class="line"><a name="l00615"></a><span class="lineno">  615</span>&#160;       // KILLME !!! This can be further optimized ....</div>
<div class="line"><a name="l00616"></a><span class="lineno">  616</span>&#160;       // instead of reading each neuron group separately .....</div>
<div class="line"><a name="l00617"></a><span class="lineno">  617</span>&#160;       // read a whole buffer and use the result ......</div>
<div class="line"><a name="l00618"></a><span class="lineno">  618</span>&#160;       int2 threadLoad = getStaticThreadLoad(bufPos);</div>
<div class="line"><a name="l00619"></a><span class="lineno">  619</span>&#160;       int  lNId          = (STATIC_LOAD_START(threadLoad) + threadIdx.x);</div>
<div class="line"><a name="l00620"></a><span class="lineno">  620</span>&#160;       int  lastLNId       = STATIC_LOAD_SIZE(threadLoad);</div>
<div class="line"><a name="l00621"></a><span class="lineno">  621</span>&#160;       short int lGrpId  = STATIC_LOAD_GROUP(threadLoad);</div>
<div class="line"><a name="l00622"></a><span class="lineno">  622</span>&#160;       bool needToWrite  = false;  // used by all neuron to indicate firing condition</div>
<div class="line"><a name="l00623"></a><span class="lineno">  623</span>&#160;       int  fireId       = 0;</div>
<div class="line"><a name="l00624"></a><span class="lineno">  624</span>&#160;</div>
<div class="line"><a name="l00625"></a><span class="lineno">  625</span>&#160;       // threadId is valid and lies within the lastId.....</div>
<div class="line"><a name="l00626"></a><span class="lineno">  626</span>&#160;       if ((threadIdx.x &lt; lastLNId) &amp;&amp; (lNId &lt; networkConfigGPU.numN)) {</div>
<div class="line"><a name="l00627"></a><span class="lineno">  627</span>&#160;           // Simple poisson spiker uses the poisson firing probability</div>
<div class="line"><a name="l00628"></a><span class="lineno">  628</span>&#160;           // to detect whether it has fired or not....</div>
<div class="line"><a name="l00629"></a><span class="lineno">  629</span>&#160;           if(isPoissonGroup(lGrpId)) { // spikes generated by spikeGenFunc</div>
<div class="line"><a name="l00630"></a><span class="lineno">  630</span>&#160;               if(groupConfigsGPU[lGrpId].isSpikeGenFunc) {</div>
<div class="line"><a name="l00631"></a><span class="lineno">  631</span>&#160;                   unsigned int offset = lNId - groupConfigsGPU[lGrpId].lStartN + groupConfigsGPU[lGrpId].Noffset;</div>
<div class="line"><a name="l00632"></a><span class="lineno">  632</span>&#160;                   needToWrite = getSpikeGenBit(offset);</div>
<div class="line"><a name="l00633"></a><span class="lineno">  633</span>&#160;               } else { // spikes generated by poission rate</div>
<div class="line"><a name="l00634"></a><span class="lineno">  634</span>&#160;                   needToWrite = getPoissonSpike(lNId);</div>
<div class="line"><a name="l00635"></a><span class="lineno">  635</span>&#160;               }</div>
<div class="line"><a name="l00636"></a><span class="lineno">  636</span>&#160;               // Note: valid lastSpikeTime of spike gen neurons is required by userDefinedSpikeGenerator()</div>
<div class="line"><a name="l00637"></a><span class="lineno">  637</span>&#160;               if (needToWrite)</div>
<div class="line"><a name="l00638"></a><span class="lineno">  638</span>&#160;                   runtimeDataGPU.lastSpikeTime[lNId] = simTime;</div>
<div class="line"><a name="l00639"></a><span class="lineno">  639</span>&#160;           } else {</div>
<div class="line"><a name="l00640"></a><span class="lineno">  640</span>&#160;               if (runtimeDataGPU.voltage[lNId] &gt;= 30.0f) {</div>
<div class="line"><a name="l00641"></a><span class="lineno">  641</span>&#160;                   needToWrite = true;</div>
<div class="line"><a name="l00642"></a><span class="lineno">  642</span>&#160;               }</div>
<div class="line"><a name="l00643"></a><span class="lineno">  643</span>&#160;           }</div>
<div class="line"><a name="l00644"></a><span class="lineno">  644</span>&#160;       }</div>
<div class="line"><a name="l00645"></a><span class="lineno">  645</span>&#160;</div>
<div class="line"><a name="l00646"></a><span class="lineno">  646</span>&#160;       // loop through a few times to ensure that we have added/processed all spikes that need to be written</div>
<div class="line"><a name="l00647"></a><span class="lineno">  647</span>&#160;       // if the buffer is small relative to the number of spikes needing to be written, we may have to empty the buffer a few times...</div>
<div class="line"><a name="l00648"></a><span class="lineno">  648</span>&#160;       for (int c = 0; c &lt; 2; c++) {</div>
<div class="line"><a name="l00649"></a><span class="lineno">  649</span>&#160;           // we first increment fireCntTest to make sure we haven&#39;t filled the buffer</div>
<div class="line"><a name="l00650"></a><span class="lineno">  650</span>&#160;           if (needToWrite)</div>
<div class="line"><a name="l00651"></a><span class="lineno">  651</span>&#160;               fireId = atomicAdd((int*)&amp;fireCntTest, 1);</div>
<div class="line"><a name="l00652"></a><span class="lineno">  652</span>&#160;</div>
<div class="line"><a name="l00653"></a><span class="lineno">  653</span>&#160;           // if there is a spike and the buffer still has space...</div>
<div class="line"><a name="l00654"></a><span class="lineno">  654</span>&#160;           if (needToWrite &amp;&amp; (fireId &lt;(FIRE_CHUNK_CNT))) {</div>
<div class="line"><a name="l00655"></a><span class="lineno">  655</span>&#160;               // get our position in the buffer</div>
<div class="line"><a name="l00656"></a><span class="lineno">  656</span>&#160;               fireId = atomicAdd((int*)&amp;fireCnt, 1);</div>
<div class="line"><a name="l00657"></a><span class="lineno">  657</span>&#160;</div>
<div class="line"><a name="l00658"></a><span class="lineno">  658</span>&#160;               if (groupConfigsGPU[lGrpId].MaxDelay == 1)</div>
<div class="line"><a name="l00659"></a><span class="lineno">  659</span>&#160;                   atomicAdd((int*)&amp;fireCntD1, 1);</div>
<div class="line"><a name="l00660"></a><span class="lineno">  660</span>&#160;</div>
<div class="line"><a name="l00661"></a><span class="lineno">  661</span>&#160;               // store ID of the fired neuron</div>
<div class="line"><a name="l00662"></a><span class="lineno">  662</span>&#160;               needToWrite       = false;</div>
<div class="line"><a name="l00663"></a><span class="lineno">  663</span>&#160;               fireTable[fireId] = lNId;</div>
<div class="line"><a name="l00664"></a><span class="lineno">  664</span>&#160;               fireGrpId[fireId] = lGrpId;//setFireProperties(grpId, isInhib);</div>
<div class="line"><a name="l00665"></a><span class="lineno">  665</span>&#160;           }</div>
<div class="line"><a name="l00666"></a><span class="lineno">  666</span>&#160;</div>
<div class="line"><a name="l00667"></a><span class="lineno">  667</span>&#160;           __syncthreads();</div>
<div class="line"><a name="l00668"></a><span class="lineno">  668</span>&#160;</div>
<div class="line"><a name="l00669"></a><span class="lineno">  669</span>&#160;           // the local firing table is full. dump the local firing table to the global firing table before proceeding</div>
<div class="line"><a name="l00670"></a><span class="lineno">  670</span>&#160;           if (fireCntTest &gt;= (FIRE_CHUNK_CNT)) {</div>
<div class="line"><a name="l00671"></a><span class="lineno">  671</span>&#160;</div>
<div class="line"><a name="l00672"></a><span class="lineno">  672</span>&#160;               // clear the table and update...</div>
<div class="line"><a name="l00673"></a><span class="lineno">  673</span>&#160;               int retCode = updateNewFirings(fireTable, fireGrpId, fireCnt, fireCntD1, simTime);</div>
<div class="line"><a name="l00674"></a><span class="lineno">  674</span>&#160;               if (retCode != 0) return;</div>
<div class="line"><a name="l00675"></a><span class="lineno">  675</span>&#160;               // update based on stdp rule</div>
<div class="line"><a name="l00676"></a><span class="lineno">  676</span>&#160;               // KILLME !!! if (simTime &gt; 0))</div>
<div class="line"><a name="l00677"></a><span class="lineno">  677</span>&#160;               if (networkConfigGPU.sim_with_stdp &amp;&amp; !networkConfigGPU.sim_in_testing)</div>
<div class="line"><a name="l00678"></a><span class="lineno">  678</span>&#160;                   updateLTP (fireTable, fireGrpId, fireCnt, simTime);</div>
<div class="line"><a name="l00679"></a><span class="lineno">  679</span>&#160;</div>
<div class="line"><a name="l00680"></a><span class="lineno">  680</span>&#160;               // reset counters</div>
<div class="line"><a name="l00681"></a><span class="lineno">  681</span>&#160;               if (threadIdx.x == 0) {</div>
<div class="line"><a name="l00682"></a><span class="lineno">  682</span>&#160;                   fireCntD1  = 0;</div>
<div class="line"><a name="l00683"></a><span class="lineno">  683</span>&#160;                   fireCnt   = 0;</div>
<div class="line"><a name="l00684"></a><span class="lineno">  684</span>&#160;                   fireCntTest = 0;</div>
<div class="line"><a name="l00685"></a><span class="lineno">  685</span>&#160;               }</div>
<div class="line"><a name="l00686"></a><span class="lineno">  686</span>&#160;           }</div>
<div class="line"><a name="l00687"></a><span class="lineno">  687</span>&#160;       }</div>
<div class="line"><a name="l00688"></a><span class="lineno">  688</span>&#160;   }</div>
<div class="line"><a name="l00689"></a><span class="lineno">  689</span>&#160;</div>
<div class="line"><a name="l00690"></a><span class="lineno">  690</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l00691"></a><span class="lineno">  691</span>&#160;</div>
<div class="line"><a name="l00692"></a><span class="lineno">  692</span>&#160;   // few more fired neurons are left. we update their firing state here..</div>
<div class="line"><a name="l00693"></a><span class="lineno">  693</span>&#160;   if (fireCnt) {</div>
<div class="line"><a name="l00694"></a><span class="lineno">  694</span>&#160;       int retCode = updateNewFirings(fireTable, fireGrpId, fireCnt, fireCntD1, simTime);</div>
<div class="line"><a name="l00695"></a><span class="lineno">  695</span>&#160;       if (retCode != 0) return;</div>
<div class="line"><a name="l00696"></a><span class="lineno">  696</span>&#160;</div>
<div class="line"><a name="l00697"></a><span class="lineno">  697</span>&#160;       if (networkConfigGPU.sim_with_stdp &amp;&amp; !networkConfigGPU.sim_in_testing)</div>
<div class="line"><a name="l00698"></a><span class="lineno">  698</span>&#160;           updateLTP(fireTable, fireGrpId, fireCnt, simTime);</div>
<div class="line"><a name="l00699"></a><span class="lineno">  699</span>&#160;   }</div>
<div class="line"><a name="l00700"></a><span class="lineno">  700</span>&#160;}</div>
<div class="line"><a name="l00701"></a><span class="lineno">  701</span>&#160;</div>
<div class="line"><a name="l00702"></a><span class="lineno">  702</span>&#160;//******************************** UPDATE CONDUCTANCES AND TOTAL SYNAPTIC CURRENT EVERY TIME STEP *****************************</div>
<div class="line"><a name="l00703"></a><span class="lineno">  703</span>&#160;</div>
<div class="line"><a name="l00704"></a><span class="lineno">  704</span>&#160;#define LOG_CURRENT_GROUP 5</div>
<div class="line"><a name="l00705"></a><span class="lineno">  705</span>&#160;/*!</div>
<div class="line"><a name="l00706"></a><span class="lineno">  706</span>&#160; * \brief Based on the bitvector used for indicating the presence of spike, the global conductance values are updated.</div>
<div class="line"><a name="l00707"></a><span class="lineno">  707</span>&#160; *</div>
<div class="line"><a name="l00708"></a><span class="lineno">  708</span>&#160; * net access: numNReg, numNPois, I_setPitch, maxDelay, STP_Pitch, sim_with_conductances,</div>
<div class="line"><a name="l00709"></a><span class="lineno">  709</span>&#160;               sim_with_NMDA_rise, sim_withGABAb_Rise, sNMDA, sGABAb</div>
<div class="line"><a name="l00710"></a><span class="lineno">  710</span>&#160; * grp access: WithSTP, STP_A</div>
<div class="line"><a name="l00711"></a><span class="lineno">  711</span>&#160; * rtd access: Npre, cumulativePre, I_set, preSynapticIds, grpIds, wt, stpx, stpu, connIdsPreIdx,</div>
<div class="line"><a name="l00712"></a><span class="lineno">  712</span>&#160;               gAMPA, gGABAa, gNMDA_r, gNMDA_d, gNMDA, gGABAb_r, gGABAb_d, gGABAb</div>
<div class="line"><a name="l00713"></a><span class="lineno">  713</span>&#160; * glb access: d_mulSynFast, d_mulSynSlow</div>
<div class="line"><a name="l00714"></a><span class="lineno">  714</span>&#160; */</div>
<div class="line"><a name="l00715"></a><span class="lineno">  715</span>&#160;__global__ void kernel_conductanceUpdate (int simTimeMs, int simTimeSec, int simTime) {</div>
<div class="line"><a name="l00716"></a><span class="lineno">  716</span>&#160;   __shared__ int sh_quickSynIdTable[256];</div>
<div class="line"><a name="l00717"></a><span class="lineno">  717</span>&#160;</div>
<div class="line"><a name="l00718"></a><span class="lineno">  718</span>&#160;   // Table for quick access</div>
<div class="line"><a name="l00719"></a><span class="lineno">  719</span>&#160;   for (int i = 0; i &lt; 256; i += blockDim.x) {</div>
<div class="line"><a name="l00720"></a><span class="lineno">  720</span>&#160;       if ((i + threadIdx.x) &lt; 256) {</div>
<div class="line"><a name="l00721"></a><span class="lineno">  721</span>&#160;           sh_quickSynIdTable[i + threadIdx.x] = quickSynIdTableGPU[i + threadIdx.x];</div>
<div class="line"><a name="l00722"></a><span class="lineno">  722</span>&#160;       }</div>
<div class="line"><a name="l00723"></a><span class="lineno">  723</span>&#160;   }</div>
<div class="line"><a name="l00724"></a><span class="lineno">  724</span>&#160;</div>
<div class="line"><a name="l00725"></a><span class="lineno">  725</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l00726"></a><span class="lineno">  726</span>&#160;</div>
<div class="line"><a name="l00727"></a><span class="lineno">  727</span>&#160;   const int totBuffers = loadBufferCount;</div>
<div class="line"><a name="l00728"></a><span class="lineno">  728</span>&#160;   for (int bufPos = blockIdx.x; bufPos &lt; totBuffers; bufPos += gridDim.x) {</div>
<div class="line"><a name="l00729"></a><span class="lineno">  729</span>&#160;       // KILLME !!! This can be further optimized ....</div>
<div class="line"><a name="l00730"></a><span class="lineno">  730</span>&#160;       // instead of reading each neuron group separately .....</div>
<div class="line"><a name="l00731"></a><span class="lineno">  731</span>&#160;       // read a whole buffer and use the result ......</div>
<div class="line"><a name="l00732"></a><span class="lineno">  732</span>&#160;       int2 threadLoad = getStaticThreadLoad(bufPos);</div>
<div class="line"><a name="l00733"></a><span class="lineno">  733</span>&#160;       int  postNId    = STATIC_LOAD_START(threadLoad) + threadIdx.x;</div>
<div class="line"><a name="l00734"></a><span class="lineno">  734</span>&#160;       int  lastNId    = STATIC_LOAD_SIZE(threadLoad);</div>
<div class="line"><a name="l00735"></a><span class="lineno">  735</span>&#160;</div>
<div class="line"><a name="l00736"></a><span class="lineno">  736</span>&#160;       if ((threadIdx.x &lt; lastNId) &amp;&amp; (IS_REGULAR_NEURON(postNId, networkConfigGPU.numNReg, networkConfigGPU.numNPois))) {</div>
<div class="line"><a name="l00737"></a><span class="lineno">  737</span>&#160;           // P6-1</div>
<div class="line"><a name="l00738"></a><span class="lineno">  738</span>&#160;           // load the initial current due to noise inputs for neuron &#39;post_nid&#39;</div>
<div class="line"><a name="l00739"></a><span class="lineno">  739</span>&#160;           // initial values of the conductances for neuron &#39;post_nid&#39;</div>
<div class="line"><a name="l00740"></a><span class="lineno">  740</span>&#160;           float AMPA_sum       = 0.0f;</div>
<div class="line"><a name="l00741"></a><span class="lineno">  741</span>&#160;           float NMDA_sum       = 0.0f;</div>
<div class="line"><a name="l00742"></a><span class="lineno">  742</span>&#160;           float NMDA_r_sum     = 0.0f;</div>
<div class="line"><a name="l00743"></a><span class="lineno">  743</span>&#160;           float NMDA_d_sum     = 0.0f;</div>
<div class="line"><a name="l00744"></a><span class="lineno">  744</span>&#160;           float GABAa_sum      = 0.0f;</div>
<div class="line"><a name="l00745"></a><span class="lineno">  745</span>&#160;           float GABAb_sum      = 0.0f;</div>
<div class="line"><a name="l00746"></a><span class="lineno">  746</span>&#160;           float GABAb_r_sum    = 0.0f;</div>
<div class="line"><a name="l00747"></a><span class="lineno">  747</span>&#160;           float GABAb_d_sum    = 0.0f;</div>
<div class="line"><a name="l00748"></a><span class="lineno">  748</span>&#160;           int   lmt            = runtimeDataGPU.Npre[postNId];</div>
<div class="line"><a name="l00749"></a><span class="lineno">  749</span>&#160;           unsigned int cum_pos = runtimeDataGPU.cumulativePre[postNId];</div>
<div class="line"><a name="l00750"></a><span class="lineno">  750</span>&#160;</div>
<div class="line"><a name="l00751"></a><span class="lineno">  751</span>&#160;           // find the total current to this neuron...</div>
<div class="line"><a name="l00752"></a><span class="lineno">  752</span>&#160;           for (int j = 0; (lmt) &amp;&amp; (j &lt;= ((lmt - 1) &gt;&gt; LOG_CURRENT_GROUP)); j++) {</div>
<div class="line"><a name="l00753"></a><span class="lineno">  753</span>&#160;               // because of malloc2D operation we are using pitch, post_nid, j to get</div>
<div class="line"><a name="l00754"></a><span class="lineno">  754</span>&#160;               // actual position of the input current....</div>
<div class="line"><a name="l00755"></a><span class="lineno">  755</span>&#160;               // int* tmp_I_set_p = ((int*)((char*)runtimeDataGPU.I_set + j * networkConfigGPU.I_setPitch) + post_nid);</div>
<div class="line"><a name="l00756"></a><span class="lineno">  756</span>&#160;               uint32_t* tmp_I_set_p = getFiringBitGroupPtr(postNId, j);</div>
<div class="line"><a name="l00757"></a><span class="lineno">  757</span>&#160;               uint32_t  tmp_I_set = *tmp_I_set_p;</div>
<div class="line"><a name="l00758"></a><span class="lineno">  758</span>&#160;</div>
<div class="line"><a name="l00759"></a><span class="lineno">  759</span>&#160;               // table lookup based find bits that are set</div>
<div class="line"><a name="l00760"></a><span class="lineno">  760</span>&#160;               int cnt = 0;</div>
<div class="line"><a name="l00761"></a><span class="lineno">  761</span>&#160;               int tmp_I_cnt = 0;</div>
<div class="line"><a name="l00762"></a><span class="lineno">  762</span>&#160;               while (tmp_I_set) {</div>
<div class="line"><a name="l00763"></a><span class="lineno">  763</span>&#160;                   int k = (tmp_I_set &gt;&gt; (8 * cnt)) &amp; 0xff;</div>
<div class="line"><a name="l00764"></a><span class="lineno">  764</span>&#160;                   if (k == 0) {</div>
<div class="line"><a name="l00765"></a><span class="lineno">  765</span>&#160;                       cnt = cnt + 1;</div>
<div class="line"><a name="l00766"></a><span class="lineno">  766</span>&#160;                       continue;</div>
<div class="line"><a name="l00767"></a><span class="lineno">  767</span>&#160;                   }</div>
<div class="line"><a name="l00768"></a><span class="lineno">  768</span>&#160;                   int wt_i = sh_quickSynIdTable[k];</div>
<div class="line"><a name="l00769"></a><span class="lineno">  769</span>&#160;                   int wtId = (j * 32 + cnt * 8 + wt_i);</div>
<div class="line"><a name="l00770"></a><span class="lineno">  770</span>&#160;</div>
<div class="line"><a name="l00771"></a><span class="lineno">  771</span>&#160;                   SynInfo synInfo = runtimeDataGPU.preSynapticIds[cum_pos + wtId];</div>
<div class="line"><a name="l00772"></a><span class="lineno">  772</span>&#160;                   //uint8_t  pre_grpId  = GET_CONN_GRP_ID(pre_Id);</div>
<div class="line"><a name="l00773"></a><span class="lineno">  773</span>&#160;                   uint32_t  preNId  = GET_CONN_NEURON_ID(synInfo);</div>
<div class="line"><a name="l00774"></a><span class="lineno">  774</span>&#160;                   short int preGrpId = runtimeDataGPU.grpIds[preNId];</div>
<div class="line"><a name="l00775"></a><span class="lineno">  775</span>&#160;                   char type = groupConfigsGPU[preGrpId].Type;</div>
<div class="line"><a name="l00776"></a><span class="lineno">  776</span>&#160;</div>
<div class="line"><a name="l00777"></a><span class="lineno">  777</span>&#160;                   // load the synaptic weight for the wtId&#39;th input</div>
<div class="line"><a name="l00778"></a><span class="lineno">  778</span>&#160;                   float change = runtimeDataGPU.wt[cum_pos + wtId];</div>
<div class="line"><a name="l00779"></a><span class="lineno">  779</span>&#160;</div>
<div class="line"><a name="l00780"></a><span class="lineno">  780</span>&#160;                   // Adjust the weight according to STP scaling</div>
<div class="line"><a name="l00781"></a><span class="lineno">  781</span>&#160;                   if (groupConfigsGPU[preGrpId].WithSTP) {</div>
<div class="line"><a name="l00782"></a><span class="lineno">  782</span>&#160;                       int tD = 0; // \FIXME find delay</div>
<div class="line"><a name="l00783"></a><span class="lineno">  783</span>&#160;                       // \FIXME I think pre_nid needs to be adjusted for the delay</div>
<div class="line"><a name="l00784"></a><span class="lineno">  784</span>&#160;                       int ind_minus = getSTPBufPos(preNId, (simTime - tD - 1)); // \FIXME should be adjusted for delay</div>
<div class="line"><a name="l00785"></a><span class="lineno">  785</span>&#160;                       int ind_plus = getSTPBufPos(preNId, (simTime - tD));</div>
<div class="line"><a name="l00786"></a><span class="lineno">  786</span>&#160;                       // dI/dt = -I/tau_S + A * u^+ * x^- * \delta(t-t_{spk})</div>
<div class="line"><a name="l00787"></a><span class="lineno">  787</span>&#160;                       change *= groupConfigsGPU[preGrpId].STP_A * runtimeDataGPU.stpx[ind_minus] * runtimeDataGPU.stpu[ind_plus];</div>
<div class="line"><a name="l00788"></a><span class="lineno">  788</span>&#160;                   }</div>
<div class="line"><a name="l00789"></a><span class="lineno">  789</span>&#160;</div>
<div class="line"><a name="l00790"></a><span class="lineno">  790</span>&#160;                   if (networkConfigGPU.sim_with_conductances) {</div>
<div class="line"><a name="l00791"></a><span class="lineno">  791</span>&#160;                       short int connId = runtimeDataGPU.connIdsPreIdx[cum_pos+wtId];</div>
<div class="line"><a name="l00792"></a><span class="lineno">  792</span>&#160;                       if (type &amp; TARGET_AMPA)</div>
<div class="line"><a name="l00793"></a><span class="lineno">  793</span>&#160;                           AMPA_sum += change * d_mulSynFast[connId];</div>
<div class="line"><a name="l00794"></a><span class="lineno">  794</span>&#160;                       if (type &amp; TARGET_NMDA) {</div>
<div class="line"><a name="l00795"></a><span class="lineno">  795</span>&#160;                           if (networkConfigGPU.sim_with_NMDA_rise) {</div>
<div class="line"><a name="l00796"></a><span class="lineno">  796</span>&#160;                               NMDA_r_sum += change * d_mulSynSlow[connId] * networkConfigGPU.sNMDA;</div>
<div class="line"><a name="l00797"></a><span class="lineno">  797</span>&#160;                               NMDA_d_sum += change * d_mulSynSlow[connId] * networkConfigGPU.sNMDA;</div>
<div class="line"><a name="l00798"></a><span class="lineno">  798</span>&#160;                           } else {</div>
<div class="line"><a name="l00799"></a><span class="lineno">  799</span>&#160;                               NMDA_sum += change * d_mulSynSlow[connId];</div>
<div class="line"><a name="l00800"></a><span class="lineno">  800</span>&#160;                           }</div>
<div class="line"><a name="l00801"></a><span class="lineno">  801</span>&#160;                       }</div>
<div class="line"><a name="l00802"></a><span class="lineno">  802</span>&#160;                       if (type &amp; TARGET_GABAa)</div>
<div class="line"><a name="l00803"></a><span class="lineno">  803</span>&#160;                           GABAa_sum += change * d_mulSynFast[connId]; // wt should be negative for GABAa and GABAb</div>
<div class="line"><a name="l00804"></a><span class="lineno">  804</span>&#160;                       if (type &amp; TARGET_GABAb) {                      // but that is dealt with below</div>
<div class="line"><a name="l00805"></a><span class="lineno">  805</span>&#160;                           if (networkConfigGPU.sim_with_GABAb_rise) {</div>
<div class="line"><a name="l00806"></a><span class="lineno">  806</span>&#160;                               GABAb_r_sum += change * d_mulSynSlow[connId] * networkConfigGPU.sGABAb;</div>
<div class="line"><a name="l00807"></a><span class="lineno">  807</span>&#160;                               GABAb_d_sum += change * d_mulSynSlow[connId] * networkConfigGPU.sGABAb;</div>
<div class="line"><a name="l00808"></a><span class="lineno">  808</span>&#160;                           } else {</div>
<div class="line"><a name="l00809"></a><span class="lineno">  809</span>&#160;                               GABAb_sum += change * d_mulSynSlow[connId];</div>
<div class="line"><a name="l00810"></a><span class="lineno">  810</span>&#160;                           }</div>
<div class="line"><a name="l00811"></a><span class="lineno">  811</span>&#160;                       }</div>
<div class="line"><a name="l00812"></a><span class="lineno">  812</span>&#160;                   }</div>
<div class="line"><a name="l00813"></a><span class="lineno">  813</span>&#160;                   else {</div>
<div class="line"><a name="l00814"></a><span class="lineno">  814</span>&#160;                       // current based model with STP (CUBA)</div>
<div class="line"><a name="l00815"></a><span class="lineno">  815</span>&#160;                       // updated current for neuron &#39;post_nid&#39;</div>
<div class="line"><a name="l00816"></a><span class="lineno">  816</span>&#160;                       AMPA_sum += change;</div>
<div class="line"><a name="l00817"></a><span class="lineno">  817</span>&#160;                   }</div>
<div class="line"><a name="l00818"></a><span class="lineno">  818</span>&#160;</div>
<div class="line"><a name="l00819"></a><span class="lineno">  819</span>&#160;                   tmp_I_cnt++;</div>
<div class="line"><a name="l00820"></a><span class="lineno">  820</span>&#160;                   tmp_I_set = tmp_I_set &amp; (~(1 &lt;&lt; (8 * cnt + wt_i)));</div>
<div class="line"><a name="l00821"></a><span class="lineno">  821</span>&#160;               }</div>
<div class="line"><a name="l00822"></a><span class="lineno">  822</span>&#160;</div>
<div class="line"><a name="l00823"></a><span class="lineno">  823</span>&#160;               // FIXME: move reset outside kernel for debbuing I_set, resume it later</div>
<div class="line"><a name="l00824"></a><span class="lineno">  824</span>&#160;               // reset the input if there are any bit&#39;wt set</div>
<div class="line"><a name="l00825"></a><span class="lineno">  825</span>&#160;               if(tmp_I_cnt)</div>
<div class="line"><a name="l00826"></a><span class="lineno">  826</span>&#160;                   *tmp_I_set_p = 0;</div>
<div class="line"><a name="l00827"></a><span class="lineno">  827</span>&#160;</div>
<div class="line"><a name="l00828"></a><span class="lineno">  828</span>&#160;               __syncthreads();</div>
<div class="line"><a name="l00829"></a><span class="lineno">  829</span>&#160;           }</div>
<div class="line"><a name="l00830"></a><span class="lineno">  830</span>&#160;</div>
<div class="line"><a name="l00831"></a><span class="lineno">  831</span>&#160;           __syncthreads();</div>
<div class="line"><a name="l00832"></a><span class="lineno">  832</span>&#160;</div>
<div class="line"><a name="l00833"></a><span class="lineno">  833</span>&#160;           // P6-2</div>
<div class="line"><a name="l00834"></a><span class="lineno">  834</span>&#160;           if (networkConfigGPU.sim_with_conductances) {</div>
<div class="line"><a name="l00835"></a><span class="lineno">  835</span>&#160;               // don&#39;t add mulSynFast/mulSynSlow here, because they depend on the exact pre&lt;-&gt;post connection, not</div>
<div class="line"><a name="l00836"></a><span class="lineno">  836</span>&#160;               // just post_nid</div>
<div class="line"><a name="l00837"></a><span class="lineno">  837</span>&#160;               runtimeDataGPU.gAMPA[postNId]        += AMPA_sum;</div>
<div class="line"><a name="l00838"></a><span class="lineno">  838</span>&#160;               runtimeDataGPU.gGABAa[postNId]       -= GABAa_sum; // wt should be negative for GABAa and GABAb</div>
<div class="line"><a name="l00839"></a><span class="lineno">  839</span>&#160;               if (networkConfigGPU.sim_with_NMDA_rise) {</div>
<div class="line"><a name="l00840"></a><span class="lineno">  840</span>&#160;                   runtimeDataGPU.gNMDA_r[postNId]  += NMDA_r_sum;</div>
<div class="line"><a name="l00841"></a><span class="lineno">  841</span>&#160;                   runtimeDataGPU.gNMDA_d[postNId]  += NMDA_d_sum;</div>
<div class="line"><a name="l00842"></a><span class="lineno">  842</span>&#160;               } else {</div>
<div class="line"><a name="l00843"></a><span class="lineno">  843</span>&#160;                   runtimeDataGPU.gNMDA[postNId]    += NMDA_sum;</div>
<div class="line"><a name="l00844"></a><span class="lineno">  844</span>&#160;               }</div>
<div class="line"><a name="l00845"></a><span class="lineno">  845</span>&#160;               if (networkConfigGPU.sim_with_GABAb_rise) {</div>
<div class="line"><a name="l00846"></a><span class="lineno">  846</span>&#160;                   runtimeDataGPU.gGABAb_r[postNId] -= GABAb_r_sum;</div>
<div class="line"><a name="l00847"></a><span class="lineno">  847</span>&#160;                   runtimeDataGPU.gGABAb_d[postNId] -= GABAb_d_sum;</div>
<div class="line"><a name="l00848"></a><span class="lineno">  848</span>&#160;               } else {</div>
<div class="line"><a name="l00849"></a><span class="lineno">  849</span>&#160;                   runtimeDataGPU.gGABAb[postNId]   -= GABAb_sum;</div>
<div class="line"><a name="l00850"></a><span class="lineno">  850</span>&#160;               }</div>
<div class="line"><a name="l00851"></a><span class="lineno">  851</span>&#160;           }</div>
<div class="line"><a name="l00852"></a><span class="lineno">  852</span>&#160;           else {</div>
<div class="line"><a name="l00853"></a><span class="lineno">  853</span>&#160;               runtimeDataGPU.current[postNId] += AMPA_sum;</div>
<div class="line"><a name="l00854"></a><span class="lineno">  854</span>&#160;           }</div>
<div class="line"><a name="l00855"></a><span class="lineno">  855</span>&#160;       }</div>
<div class="line"><a name="l00856"></a><span class="lineno">  856</span>&#160;   }</div>
<div class="line"><a name="l00857"></a><span class="lineno">  857</span>&#160;}</div>
<div class="line"><a name="l00858"></a><span class="lineno">  858</span>&#160;</div>
<div class="line"><a name="l00859"></a><span class="lineno">  859</span>&#160;//************************ UPDATE GLOBAL STATE EVERY TIME STEP *******************************************************//</div>
<div class="line"><a name="l00860"></a><span class="lineno">  860</span>&#160;</div>
<div class="line"><a name="l00861"></a><span class="lineno">  861</span>&#160;/*!</div>
<div class="line"><a name="l00862"></a><span class="lineno">  862</span>&#160; * \brief This device function implements the equations of neuron dynamics</div>
<div class="line"><a name="l00863"></a><span class="lineno">  863</span>&#160; *</div>
<div class="line"><a name="l00864"></a><span class="lineno">  864</span>&#160; * \param[in] nid The neuron id to be updated</div>
<div class="line"><a name="l00865"></a><span class="lineno">  865</span>&#160; * \param[in] grpId The group id of the neuron</div>
<div class="line"><a name="l00866"></a><span class="lineno">  866</span>&#160; */</div>
<div class="line"><a name="l00867"></a><span class="lineno">  867</span>&#160;__device__ void updateNeuronState(int nid, int grpId) {</div>
<div class="line"><a name="l00868"></a><span class="lineno">  868</span>&#160;   float v = runtimeDataGPU.voltage[nid];</div>
<div class="line"><a name="l00869"></a><span class="lineno">  869</span>&#160;   float u = runtimeDataGPU.recovery[nid];</div>
<div class="line"><a name="l00870"></a><span class="lineno">  870</span>&#160;   float I_sum, NMDAtmp;</div>
<div class="line"><a name="l00871"></a><span class="lineno">  871</span>&#160;   float gNMDA, gGABAb;</div>
<div class="line"><a name="l00872"></a><span class="lineno">  872</span>&#160;</div>
<div class="line"><a name="l00873"></a><span class="lineno">  873</span>&#160;   // loop that allows smaller integration time step for v&#39;s and u&#39;s</div>
<div class="line"><a name="l00874"></a><span class="lineno">  874</span>&#160;   for (int c = 0; c &lt; COND_INTEGRATION_SCALE; c++) {</div>
<div class="line"><a name="l00875"></a><span class="lineno">  875</span>&#160;       I_sum = 0.0f;</div>
<div class="line"><a name="l00876"></a><span class="lineno">  876</span>&#160;       if (networkConfigGPU.sim_with_conductances) {</div>
<div class="line"><a name="l00877"></a><span class="lineno">  877</span>&#160;           NMDAtmp = (v + 80.0f) * (v + 80.0f) / 60.0f / 60.0f;</div>
<div class="line"><a name="l00878"></a><span class="lineno">  878</span>&#160;           gNMDA = (networkConfigGPU.sim_with_NMDA_rise) ? (runtimeDataGPU.gNMDA_d[nid] - runtimeDataGPU.gNMDA_r[nid]) : runtimeDataGPU.gNMDA[nid];</div>
<div class="line"><a name="l00879"></a><span class="lineno">  879</span>&#160;           gGABAb = (networkConfigGPU.sim_with_GABAb_rise) ? (runtimeDataGPU.gGABAb_d[nid] - runtimeDataGPU.gGABAb_r[nid]) : runtimeDataGPU.gGABAb[nid];</div>
<div class="line"><a name="l00880"></a><span class="lineno">  880</span>&#160;           I_sum = -(runtimeDataGPU.gAMPA[nid] * (v - 0.0f)</div>
<div class="line"><a name="l00881"></a><span class="lineno">  881</span>&#160;                       + gNMDA * NMDAtmp / (1.0f + NMDAtmp) * (v - 0.0f)</div>
<div class="line"><a name="l00882"></a><span class="lineno">  882</span>&#160;                       + runtimeDataGPU.gGABAa[nid] * (v + 70.0f)</div>
<div class="line"><a name="l00883"></a><span class="lineno">  883</span>&#160;                       + gGABAb * (v + 90.0f));</div>
<div class="line"><a name="l00884"></a><span class="lineno">  884</span>&#160;       } else {</div>
<div class="line"><a name="l00885"></a><span class="lineno">  885</span>&#160;           I_sum = runtimeDataGPU.current[nid];</div>
<div class="line"><a name="l00886"></a><span class="lineno">  886</span>&#160;       }</div>
<div class="line"><a name="l00887"></a><span class="lineno">  887</span>&#160;</div>
<div class="line"><a name="l00888"></a><span class="lineno">  888</span>&#160;       // update vpos and upos for the current neuron</div>
<div class="line"><a name="l00889"></a><span class="lineno">  889</span>&#160;       v += ((0.04f * v + 5.0f) * v + 140.0f - u + I_sum + runtimeDataGPU.extCurrent[nid]) / COND_INTEGRATION_SCALE;</div>
<div class="line"><a name="l00890"></a><span class="lineno">  890</span>&#160;       if (v &gt; 30.0f) { </div>
<div class="line"><a name="l00891"></a><span class="lineno">  891</span>&#160;           v = 30.0f; // break the loop but evaluate u[i]</div>
<div class="line"><a name="l00892"></a><span class="lineno">  892</span>&#160;           c = COND_INTEGRATION_SCALE;</div>
<div class="line"><a name="l00893"></a><span class="lineno">  893</span>&#160;       }</div>
<div class="line"><a name="l00894"></a><span class="lineno">  894</span>&#160;</div>
<div class="line"><a name="l00895"></a><span class="lineno">  895</span>&#160;       if (v &lt; -90.0f) v = -90.0f;</div>
<div class="line"><a name="l00896"></a><span class="lineno">  896</span>&#160;</div>
<div class="line"><a name="l00897"></a><span class="lineno">  897</span>&#160;       u += (runtimeDataGPU.Izh_a[nid] * (runtimeDataGPU.Izh_b[nid] * v - u) / COND_INTEGRATION_SCALE);</div>
<div class="line"><a name="l00898"></a><span class="lineno">  898</span>&#160;   }</div>
<div class="line"><a name="l00899"></a><span class="lineno">  899</span>&#160;   if(networkConfigGPU.sim_with_conductances) {</div>
<div class="line"><a name="l00900"></a><span class="lineno">  900</span>&#160;       runtimeDataGPU.current[nid] = I_sum;</div>
<div class="line"><a name="l00901"></a><span class="lineno">  901</span>&#160;   } else {</div>
<div class="line"><a name="l00902"></a><span class="lineno">  902</span>&#160;       // current must be reset here for CUBA and not kernel_STPUpdateAndDecayConductances</div>
<div class="line"><a name="l00903"></a><span class="lineno">  903</span>&#160;       runtimeDataGPU.current[nid] = 0.0f;</div>
<div class="line"><a name="l00904"></a><span class="lineno">  904</span>&#160;   }</div>
<div class="line"><a name="l00905"></a><span class="lineno">  905</span>&#160;   runtimeDataGPU.voltage[nid] = v;</div>
<div class="line"><a name="l00906"></a><span class="lineno">  906</span>&#160;   runtimeDataGPU.recovery[nid] = u;</div>
<div class="line"><a name="l00907"></a><span class="lineno">  907</span>&#160;}</div>
<div class="line"><a name="l00908"></a><span class="lineno">  908</span>&#160;</div>
<div class="line"><a name="l00909"></a><span class="lineno">  909</span>&#160;/*!</div>
<div class="line"><a name="l00910"></a><span class="lineno">  910</span>&#160; *  \brief update neuron state</div>
<div class="line"><a name="l00911"></a><span class="lineno">  911</span>&#160; *</div>
<div class="line"><a name="l00912"></a><span class="lineno">  912</span>&#160; * This kernel update neurons&#39; membrance potential according to neurons&#39; dynamics model.</div>
<div class="line"><a name="l00913"></a><span class="lineno">  913</span>&#160; * This kernel also update variables required by homeostasis</div>
<div class="line"><a name="l00914"></a><span class="lineno">  914</span>&#160; *</div>
<div class="line"><a name="l00915"></a><span class="lineno">  915</span>&#160; * net access: numN, numNReg, numNPois, sim_with_conductances, sim_with_NMDA_rise, sim_with_GABAb_rise</div>
<div class="line"><a name="l00916"></a><span class="lineno">  916</span>&#160; * grp access: WithHomeostasis, avgTimeScale_decay</div>
<div class="line"><a name="l00917"></a><span class="lineno">  917</span>&#160; * rtd access: avgFiring, voltage, recovery, gNMDA, gNMDA_r, gNMDA_d, gGABAb, gGABAb_r, gGABAb_d, gAMPA, gGABAa,</div>
<div class="line"><a name="l00918"></a><span class="lineno">  918</span>&#160; *             current, extCurrent, Izh_a, Izh_b</div>
<div class="line"><a name="l00919"></a><span class="lineno">  919</span>&#160; * glb access:</div>
<div class="line"><a name="l00920"></a><span class="lineno">  920</span>&#160; */</div>
<div class="line"><a name="l00921"></a><span class="lineno">  921</span>&#160;__global__ void kernel_neuronStateUpdate() {</div>
<div class="line"><a name="l00922"></a><span class="lineno">  922</span>&#160;   const int totBuffers = loadBufferCount;</div>
<div class="line"><a name="l00923"></a><span class="lineno">  923</span>&#160;</div>
<div class="line"><a name="l00924"></a><span class="lineno">  924</span>&#160;   // update neuron state</div>
<div class="line"><a name="l00925"></a><span class="lineno">  925</span>&#160;   for (int bufPos = blockIdx.x; bufPos &lt; totBuffers; bufPos += gridDim.x) {</div>
<div class="line"><a name="l00926"></a><span class="lineno">  926</span>&#160;       // KILLME !!! This can be further optimized ....</div>
<div class="line"><a name="l00927"></a><span class="lineno">  927</span>&#160;       // instead of reading each neuron group separately .....</div>
<div class="line"><a name="l00928"></a><span class="lineno">  928</span>&#160;       // read a whole buffer and use the result ......</div>
<div class="line"><a name="l00929"></a><span class="lineno">  929</span>&#160;       int2 threadLoad  = getStaticThreadLoad(bufPos);</div>
<div class="line"><a name="l00930"></a><span class="lineno">  930</span>&#160;       int nid = (STATIC_LOAD_START(threadLoad) + threadIdx.x);</div>
<div class="line"><a name="l00931"></a><span class="lineno">  931</span>&#160;       int lastId = STATIC_LOAD_SIZE(threadLoad);</div>
<div class="line"><a name="l00932"></a><span class="lineno">  932</span>&#160;       int grpId = STATIC_LOAD_GROUP(threadLoad);</div>
<div class="line"><a name="l00933"></a><span class="lineno">  933</span>&#160;</div>
<div class="line"><a name="l00934"></a><span class="lineno">  934</span>&#160;       if ((threadIdx.x &lt; lastId) &amp;&amp; (nid &lt; networkConfigGPU.numN)) {</div>
<div class="line"><a name="l00935"></a><span class="lineno">  935</span>&#160;</div>
<div class="line"><a name="l00936"></a><span class="lineno">  936</span>&#160;           if (IS_REGULAR_NEURON(nid, networkConfigGPU.numNReg, networkConfigGPU.numNPois)) {</div>
<div class="line"><a name="l00937"></a><span class="lineno">  937</span>&#160;               // P7</div>
<div class="line"><a name="l00938"></a><span class="lineno">  938</span>&#160;               // update neuron state here....</div>
<div class="line"><a name="l00939"></a><span class="lineno">  939</span>&#160;               updateNeuronState(nid, grpId);</div>
<div class="line"><a name="l00940"></a><span class="lineno">  940</span>&#160;</div>
<div class="line"><a name="l00941"></a><span class="lineno">  941</span>&#160;               // P8</div>
<div class="line"><a name="l00942"></a><span class="lineno">  942</span>&#160;               if (groupConfigsGPU[grpId].WithHomeostasis)</div>
<div class="line"><a name="l00943"></a><span class="lineno">  943</span>&#160;                   updateHomeoStaticState(nid, grpId);</div>
<div class="line"><a name="l00944"></a><span class="lineno">  944</span>&#160;           }</div>
<div class="line"><a name="l00945"></a><span class="lineno">  945</span>&#160;       }</div>
<div class="line"><a name="l00946"></a><span class="lineno">  946</span>&#160;   }       </div>
<div class="line"><a name="l00947"></a><span class="lineno">  947</span>&#160;}</div>
<div class="line"><a name="l00948"></a><span class="lineno">  948</span>&#160;</div>
<div class="line"><a name="l00949"></a><span class="lineno">  949</span>&#160;/*!</div>
<div class="line"><a name="l00950"></a><span class="lineno">  950</span>&#160; *  \brief Update the state of groups, which includes concentration of dopamine currently</div>
<div class="line"><a name="l00951"></a><span class="lineno">  951</span>&#160; *</div>
<div class="line"><a name="l00952"></a><span class="lineno">  952</span>&#160; * Update the concentration of neuronmodulator</div>
<div class="line"><a name="l00953"></a><span class="lineno">  953</span>&#160; *</div>
<div class="line"><a name="l00954"></a><span class="lineno">  954</span>&#160; * net access: numGroups</div>
<div class="line"><a name="l00955"></a><span class="lineno">  955</span>&#160; * grp access: WithESTDPtype, WithISTDPtype, baseDP, decayDP</div>
<div class="line"><a name="l00956"></a><span class="lineno">  956</span>&#160; * rtd access: grpDA, grpDABuffer</div>
<div class="line"><a name="l00957"></a><span class="lineno">  957</span>&#160; * glb access:</div>
<div class="line"><a name="l00958"></a><span class="lineno">  958</span>&#160; */</div>
<div class="line"><a name="l00959"></a><span class="lineno">  959</span>&#160;__global__ void kernel_groupStateUpdate(int simTime) {</div>
<div class="line"><a name="l00960"></a><span class="lineno">  960</span>&#160;   // update group state</div>
<div class="line"><a name="l00961"></a><span class="lineno">  961</span>&#160;   int grpIdx = blockIdx.x * blockDim.x + threadIdx.x;</div>
<div class="line"><a name="l00962"></a><span class="lineno">  962</span>&#160;</div>
<div class="line"><a name="l00963"></a><span class="lineno">  963</span>&#160;   // P9</div>
<div class="line"><a name="l00964"></a><span class="lineno">  964</span>&#160;   if (grpIdx &lt; networkConfigGPU.numGroups) {</div>
<div class="line"><a name="l00965"></a><span class="lineno">  965</span>&#160;       // decay dopamine concentration</div>
<div class="line"><a name="l00966"></a><span class="lineno">  966</span>&#160;       if ((groupConfigsGPU[grpIdx].WithESTDPtype == DA_MOD || groupConfigsGPU[grpIdx].WithISTDPtype == DA_MOD) &amp;&amp; runtimeDataGPU.grpDA[grpIdx] &gt; groupConfigsGPU[grpIdx].baseDP) {</div>
<div class="line"><a name="l00967"></a><span class="lineno">  967</span>&#160;           runtimeDataGPU.grpDA[grpIdx] *= groupConfigsGPU[grpIdx].decayDP;</div>
<div class="line"><a name="l00968"></a><span class="lineno">  968</span>&#160;       }</div>
<div class="line"><a name="l00969"></a><span class="lineno">  969</span>&#160;       runtimeDataGPU.grpDABuffer[grpIdx * 1000 + simTime] = runtimeDataGPU.grpDA[grpIdx]; // log dopamine concentration</div>
<div class="line"><a name="l00970"></a><span class="lineno">  970</span>&#160;   }</div>
<div class="line"><a name="l00971"></a><span class="lineno">  971</span>&#160;}</div>
<div class="line"><a name="l00972"></a><span class="lineno">  972</span>&#160;</div>
<div class="line"><a name="l00973"></a><span class="lineno">  973</span>&#160;//******************************** UPDATE STP STATE EVERY TIME STEP **********************************************</div>
<div class="line"><a name="l00974"></a><span class="lineno">  974</span>&#160;/*!</div>
<div class="line"><a name="l00975"></a><span class="lineno">  975</span>&#160; * \brief This function is called for updat STP and decay coductance every time step </div>
<div class="line"><a name="l00976"></a><span class="lineno">  976</span>&#160; *</div>
<div class="line"><a name="l00977"></a><span class="lineno">  977</span>&#160; * net access sim_with_conductance, sim_with_NMDA_rise, sim_with_GABAb_rise, numNReg, numNPois, numN, STP_Pitch, maxDelay</div>
<div class="line"><a name="l00978"></a><span class="lineno">  978</span>&#160; * grp access WithSTP </div>
<div class="line"><a name="l00979"></a><span class="lineno">  979</span>&#160; * rtd access gAMPA, gNMDA_r, gNMDA_d, gNMDA, gBABAa, gGABAb_r, gGABAb_d, gGABAb</div>
<div class="line"><a name="l00980"></a><span class="lineno">  980</span>&#160; * rtd access stpu, stpx</div>
<div class="line"><a name="l00981"></a><span class="lineno">  981</span>&#160; */</div>
<div class="line"><a name="l00982"></a><span class="lineno">  982</span>&#160;__global__ void kernel_STPUpdateAndDecayConductances (int t, int sec, int simTime) {</div>
<div class="line"><a name="l00983"></a><span class="lineno">  983</span>&#160;   const int totBuffers = loadBufferCount;</div>
<div class="line"><a name="l00984"></a><span class="lineno">  984</span>&#160;</div>
<div class="line"><a name="l00985"></a><span class="lineno">  985</span>&#160;   for (int bufPos = blockIdx.x; bufPos &lt; totBuffers; bufPos += gridDim.x) {</div>
<div class="line"><a name="l00986"></a><span class="lineno">  986</span>&#160;       // KILLME !!! This can be further optimized ....</div>
<div class="line"><a name="l00987"></a><span class="lineno">  987</span>&#160;       // instead of reading each neuron group separately .....</div>
<div class="line"><a name="l00988"></a><span class="lineno">  988</span>&#160;       // read a whole buffer and use the result ......</div>
<div class="line"><a name="l00989"></a><span class="lineno">  989</span>&#160;       int2 threadLoad = getStaticThreadLoad(bufPos);</div>
<div class="line"><a name="l00990"></a><span class="lineno">  990</span>&#160;       int nid         = (STATIC_LOAD_START(threadLoad) + threadIdx.x);</div>
<div class="line"><a name="l00991"></a><span class="lineno">  991</span>&#160;       int lastId      = STATIC_LOAD_SIZE(threadLoad);</div>
<div class="line"><a name="l00992"></a><span class="lineno">  992</span>&#160;       int grpId       = STATIC_LOAD_GROUP(threadLoad);</div>
<div class="line"><a name="l00993"></a><span class="lineno">  993</span>&#160;</div>
<div class="line"><a name="l00994"></a><span class="lineno">  994</span>&#160;</div>
<div class="line"><a name="l00995"></a><span class="lineno">  995</span>&#160;    // update the conductane parameter of the current neron</div>
<div class="line"><a name="l00996"></a><span class="lineno">  996</span>&#160;       if (networkConfigGPU.sim_with_conductances &amp;&amp; IS_REGULAR_NEURON(nid, networkConfigGPU.numNReg, networkConfigGPU.numNPois)) {</div>
<div class="line"><a name="l00997"></a><span class="lineno">  997</span>&#160;           runtimeDataGPU.gAMPA[nid]   *=  networkConfigGPU.dAMPA;</div>
<div class="line"><a name="l00998"></a><span class="lineno">  998</span>&#160;           if (networkConfigGPU.sim_with_NMDA_rise) {</div>
<div class="line"><a name="l00999"></a><span class="lineno">  999</span>&#160;               runtimeDataGPU.gNMDA_r[nid]   *=  networkConfigGPU.rNMDA;</div>
<div class="line"><a name="l01000"></a><span class="lineno"> 1000</span>&#160;               runtimeDataGPU.gNMDA_d[nid]   *=  networkConfigGPU.dNMDA;</div>
<div class="line"><a name="l01001"></a><span class="lineno"> 1001</span>&#160;           } else {</div>
<div class="line"><a name="l01002"></a><span class="lineno"> 1002</span>&#160;               runtimeDataGPU.gNMDA[nid]   *=  networkConfigGPU.dNMDA;</div>
<div class="line"><a name="l01003"></a><span class="lineno"> 1003</span>&#160;           }</div>
<div class="line"><a name="l01004"></a><span class="lineno"> 1004</span>&#160;           runtimeDataGPU.gGABAa[nid]  *=  networkConfigGPU.dGABAa;</div>
<div class="line"><a name="l01005"></a><span class="lineno"> 1005</span>&#160;           if (networkConfigGPU.sim_with_GABAb_rise) {</div>
<div class="line"><a name="l01006"></a><span class="lineno"> 1006</span>&#160;               runtimeDataGPU.gGABAb_r[nid]  *=  networkConfigGPU.rGABAb;</div>
<div class="line"><a name="l01007"></a><span class="lineno"> 1007</span>&#160;               runtimeDataGPU.gGABAb_d[nid]  *=  networkConfigGPU.dGABAb;</div>
<div class="line"><a name="l01008"></a><span class="lineno"> 1008</span>&#160;           } else {</div>
<div class="line"><a name="l01009"></a><span class="lineno"> 1009</span>&#160;               runtimeDataGPU.gGABAb[nid]  *=  networkConfigGPU.dGABAb;</div>
<div class="line"><a name="l01010"></a><span class="lineno"> 1010</span>&#160;           }</div>
<div class="line"><a name="l01011"></a><span class="lineno"> 1011</span>&#160;       }</div>
<div class="line"><a name="l01012"></a><span class="lineno"> 1012</span>&#160;</div>
<div class="line"><a name="l01013"></a><span class="lineno"> 1013</span>&#160;       if (groupConfigsGPU[grpId].WithSTP &amp;&amp; (threadIdx.x &lt; lastId) &amp;&amp; (nid &lt; networkConfigGPU.numN)) {</div>
<div class="line"><a name="l01014"></a><span class="lineno"> 1014</span>&#160;           int ind_plus  = getSTPBufPos(nid, simTime);</div>
<div class="line"><a name="l01015"></a><span class="lineno"> 1015</span>&#160;           int ind_minus = getSTPBufPos(nid, (simTime-1)); // \FIXME sure?</div>
<div class="line"><a name="l01016"></a><span class="lineno"> 1016</span>&#160;               runtimeDataGPU.stpu[ind_plus] = runtimeDataGPU.stpu[ind_minus]*(1.0f-groupConfigsGPU[grpId].STP_tau_u_inv);</div>
<div class="line"><a name="l01017"></a><span class="lineno"> 1017</span>&#160;               runtimeDataGPU.stpx[ind_plus] = runtimeDataGPU.stpx[ind_minus] + (1.0f-runtimeDataGPU.stpx[ind_minus])*groupConfigsGPU[grpId].STP_tau_x_inv;</div>
<div class="line"><a name="l01018"></a><span class="lineno"> 1018</span>&#160;       }</div>
<div class="line"><a name="l01019"></a><span class="lineno"> 1019</span>&#160;   }</div>
<div class="line"><a name="l01020"></a><span class="lineno"> 1020</span>&#160;}</div>
<div class="line"><a name="l01021"></a><span class="lineno"> 1021</span>&#160;</div>
<div class="line"><a name="l01022"></a><span class="lineno"> 1022</span>&#160;//********************************UPDATE SYNAPTIC WEIGHTS EVERY SECOND  *************************************************************</div>
<div class="line"><a name="l01023"></a><span class="lineno"> 1023</span>&#160;</div>
<div class="line"><a name="l01024"></a><span class="lineno"> 1024</span>&#160;/*!</div>
<div class="line"><a name="l01025"></a><span class="lineno"> 1025</span>&#160; * \brief This kernel update synaptic weights</div>
<div class="line"><a name="l01026"></a><span class="lineno"> 1026</span>&#160; *</div>
<div class="line"><a name="l01027"></a><span class="lineno"> 1027</span>&#160; * This kernel is called every second to adjust the timingTable and globalFiringTable</div>
<div class="line"><a name="l01028"></a><span class="lineno"> 1028</span>&#160; * We do the following thing:</div>
<div class="line"><a name="l01029"></a><span class="lineno"> 1029</span>&#160; * 1. We discard all firing information that happened more than 1000-maxDelay_ time step.</div>
<div class="line"><a name="l01030"></a><span class="lineno"> 1030</span>&#160; * 2. We move the firing information that happened in the last 1000-maxDelay_ time step to</div>
<div class="line"><a name="l01031"></a><span class="lineno"> 1031</span>&#160; * the begining of the gloalFiringTable.</div>
<div class="line"><a name="l01032"></a><span class="lineno"> 1032</span>&#160; * 3. We read each value of &quot;wtChange&quot; and update the value of &quot;synaptic weights wt&quot;.</div>
<div class="line"><a name="l01033"></a><span class="lineno"> 1033</span>&#160; * We also clip the &quot;synaptic weight wt&quot; to lie within the required range.</div>
<div class="line"><a name="l01034"></a><span class="lineno"> 1034</span>&#160; */</div>
<div class="line"><a name="l01035"></a><span class="lineno"> 1035</span>&#160;__device__ void updateSynapticWeights(int nid, unsigned int synId, int grpId, float diff_firing, float homeostasisScale, float baseFiring, float avgTimeScaleInv) {</div>
<div class="line"><a name="l01036"></a><span class="lineno"> 1036</span>&#160;   // This function does not get called if the neuron group has all fixed weights.</div>
<div class="line"><a name="l01037"></a><span class="lineno"> 1037</span>&#160;   // t_twChange is adjusted by stdpScaleFactor based on frequency of weight updates (e.g., 10ms, 100ms, 1s)   </div>
<div class="line"><a name="l01038"></a><span class="lineno"> 1038</span>&#160;   float t_wt = runtimeDataGPU.wt[synId];</div>
<div class="line"><a name="l01039"></a><span class="lineno"> 1039</span>&#160;   float t_wtChange = runtimeDataGPU.wtChange[synId];</div>
<div class="line"><a name="l01040"></a><span class="lineno"> 1040</span>&#160;   float t_effectiveWtChange = networkConfigGPU.stdpScaleFactor * t_wtChange;</div>
<div class="line"><a name="l01041"></a><span class="lineno"> 1041</span>&#160;   float t_maxWt = runtimeDataGPU.maxSynWt[synId];</div>
<div class="line"><a name="l01042"></a><span class="lineno"> 1042</span>&#160;</div>
<div class="line"><a name="l01043"></a><span class="lineno"> 1043</span>&#160;   switch (groupConfigsGPU[grpId].WithESTDPtype) {</div>
<div class="line"><a name="l01044"></a><span class="lineno"> 1044</span>&#160;   case STANDARD:</div>
<div class="line"><a name="l01045"></a><span class="lineno"> 1045</span>&#160;       if (groupConfigsGPU[grpId].WithHomeostasis) {</div>
<div class="line"><a name="l01046"></a><span class="lineno"> 1046</span>&#160;           // this factor is slow</div>
<div class="line"><a name="l01047"></a><span class="lineno"> 1047</span>&#160;           t_wt += (diff_firing*t_wt*homeostasisScale + t_effectiveWtChange) * baseFiring * avgTimeScaleInv / (1.0f+fabs(diff_firing)*50.0f);</div>
<div class="line"><a name="l01048"></a><span class="lineno"> 1048</span>&#160;       } else {</div>
<div class="line"><a name="l01049"></a><span class="lineno"> 1049</span>&#160;           t_wt += t_effectiveWtChange;</div>
<div class="line"><a name="l01050"></a><span class="lineno"> 1050</span>&#160;       }</div>
<div class="line"><a name="l01051"></a><span class="lineno"> 1051</span>&#160;       break;</div>
<div class="line"><a name="l01052"></a><span class="lineno"> 1052</span>&#160;   case DA_MOD:</div>
<div class="line"><a name="l01053"></a><span class="lineno"> 1053</span>&#160;       if (groupConfigsGPU[grpId].WithHomeostasis) {</div>
<div class="line"><a name="l01054"></a><span class="lineno"> 1054</span>&#160;           t_effectiveWtChange = runtimeDataGPU.grpDA[grpId] * t_effectiveWtChange;</div>
<div class="line"><a name="l01055"></a><span class="lineno"> 1055</span>&#160;           t_wt += (diff_firing*t_wt*homeostasisScale + t_effectiveWtChange) * baseFiring * avgTimeScaleInv / (1.0f+fabs(diff_firing)*50.0f);</div>
<div class="line"><a name="l01056"></a><span class="lineno"> 1056</span>&#160;       } else {</div>
<div class="line"><a name="l01057"></a><span class="lineno"> 1057</span>&#160;           t_wt += runtimeDataGPU.grpDA[grpId] * t_effectiveWtChange;</div>
<div class="line"><a name="l01058"></a><span class="lineno"> 1058</span>&#160;       }</div>
<div class="line"><a name="l01059"></a><span class="lineno"> 1059</span>&#160;       break;</div>
<div class="line"><a name="l01060"></a><span class="lineno"> 1060</span>&#160;   case UNKNOWN_STDP:</div>
<div class="line"><a name="l01061"></a><span class="lineno"> 1061</span>&#160;   default:</div>
<div class="line"><a name="l01062"></a><span class="lineno"> 1062</span>&#160;       // we shouldn&#39;t even be here if !WithSTDP</div>
<div class="line"><a name="l01063"></a><span class="lineno"> 1063</span>&#160;       break;</div>
<div class="line"><a name="l01064"></a><span class="lineno"> 1064</span>&#160;   }</div>
<div class="line"><a name="l01065"></a><span class="lineno"> 1065</span>&#160;</div>
<div class="line"><a name="l01066"></a><span class="lineno"> 1066</span>&#160;   switch (groupConfigsGPU[grpId].WithISTDPtype) {</div>
<div class="line"><a name="l01067"></a><span class="lineno"> 1067</span>&#160;   case STANDARD:</div>
<div class="line"><a name="l01068"></a><span class="lineno"> 1068</span>&#160;       if (groupConfigsGPU[grpId].WithHomeostasis) {</div>
<div class="line"><a name="l01069"></a><span class="lineno"> 1069</span>&#160;           // this factor is slow</div>
<div class="line"><a name="l01070"></a><span class="lineno"> 1070</span>&#160;           t_wt += (diff_firing*t_wt*homeostasisScale + t_effectiveWtChange) * baseFiring * avgTimeScaleInv / (1.0f+fabs(diff_firing)*50.0f);</div>
<div class="line"><a name="l01071"></a><span class="lineno"> 1071</span>&#160;       } else {</div>
<div class="line"><a name="l01072"></a><span class="lineno"> 1072</span>&#160;           t_wt += t_effectiveWtChange;</div>
<div class="line"><a name="l01073"></a><span class="lineno"> 1073</span>&#160;       }</div>
<div class="line"><a name="l01074"></a><span class="lineno"> 1074</span>&#160;       break;</div>
<div class="line"><a name="l01075"></a><span class="lineno"> 1075</span>&#160;   case DA_MOD:</div>
<div class="line"><a name="l01076"></a><span class="lineno"> 1076</span>&#160;       if (groupConfigsGPU[grpId].WithHomeostasis) {</div>
<div class="line"><a name="l01077"></a><span class="lineno"> 1077</span>&#160;           t_effectiveWtChange = runtimeDataGPU.grpDA[grpId] * t_effectiveWtChange;</div>
<div class="line"><a name="l01078"></a><span class="lineno"> 1078</span>&#160;           t_wt += (diff_firing*t_wt*homeostasisScale + t_effectiveWtChange) * baseFiring * avgTimeScaleInv / (1.0f+fabs(diff_firing)*50.0f);</div>
<div class="line"><a name="l01079"></a><span class="lineno"> 1079</span>&#160;       } else {</div>
<div class="line"><a name="l01080"></a><span class="lineno"> 1080</span>&#160;           t_wt += runtimeDataGPU.grpDA[grpId] * t_effectiveWtChange;</div>
<div class="line"><a name="l01081"></a><span class="lineno"> 1081</span>&#160;       }</div>
<div class="line"><a name="l01082"></a><span class="lineno"> 1082</span>&#160;       break;</div>
<div class="line"><a name="l01083"></a><span class="lineno"> 1083</span>&#160;   case UNKNOWN_STDP:</div>
<div class="line"><a name="l01084"></a><span class="lineno"> 1084</span>&#160;   default:</div>
<div class="line"><a name="l01085"></a><span class="lineno"> 1085</span>&#160;       // we shouldn&#39;t even be here if !WithSTDP</div>
<div class="line"><a name="l01086"></a><span class="lineno"> 1086</span>&#160;       break;</div>
<div class="line"><a name="l01087"></a><span class="lineno"> 1087</span>&#160;   }</div>
<div class="line"><a name="l01088"></a><span class="lineno"> 1088</span>&#160;</div>
<div class="line"><a name="l01089"></a><span class="lineno"> 1089</span>&#160;   // It&#39;s user&#39;s choice to decay weight change or not</div>
<div class="line"><a name="l01090"></a><span class="lineno"> 1090</span>&#160;   // see setWeightAndWeightChangeUpdate()</div>
<div class="line"><a name="l01091"></a><span class="lineno"> 1091</span>&#160;   t_wtChange *= networkConfigGPU.wtChangeDecay;</div>
<div class="line"><a name="l01092"></a><span class="lineno"> 1092</span>&#160;</div>
<div class="line"><a name="l01093"></a><span class="lineno"> 1093</span>&#160;   // Check the synapse is excitatory or inhibitory first</div>
<div class="line"><a name="l01094"></a><span class="lineno"> 1094</span>&#160;   if (t_maxWt &gt;= 0.0f) { // excitatory synapse</div>
<div class="line"><a name="l01095"></a><span class="lineno"> 1095</span>&#160;       if (t_wt &gt;= t_maxWt) t_wt = t_maxWt;</div>
<div class="line"><a name="l01096"></a><span class="lineno"> 1096</span>&#160;       if (t_wt &lt; 0.0f) t_wt = 0.0f;</div>
<div class="line"><a name="l01097"></a><span class="lineno"> 1097</span>&#160;   } else { // inhibitory synapse</div>
<div class="line"><a name="l01098"></a><span class="lineno"> 1098</span>&#160;       if (t_wt &lt;= t_maxWt) t_wt = t_maxWt;</div>
<div class="line"><a name="l01099"></a><span class="lineno"> 1099</span>&#160;       if (t_wt &gt; 0.0f) t_wt = 0.0f;</div>
<div class="line"><a name="l01100"></a><span class="lineno"> 1100</span>&#160;   }</div>
<div class="line"><a name="l01101"></a><span class="lineno"> 1101</span>&#160;</div>
<div class="line"><a name="l01102"></a><span class="lineno"> 1102</span>&#160;   runtimeDataGPU.wt[synId] = t_wt;</div>
<div class="line"><a name="l01103"></a><span class="lineno"> 1103</span>&#160;   runtimeDataGPU.wtChange[synId] = t_wtChange;</div>
<div class="line"><a name="l01104"></a><span class="lineno"> 1104</span>&#160;}</div>
<div class="line"><a name="l01105"></a><span class="lineno"> 1105</span>&#160;</div>
<div class="line"><a name="l01106"></a><span class="lineno"> 1106</span>&#160;</div>
<div class="line"><a name="l01107"></a><span class="lineno"> 1107</span>&#160;#define UPWTS_CLUSTERING_SZ    32</div>
<div class="line"><a name="l01108"></a><span class="lineno"> 1108</span>&#160;/*!</div>
<div class="line"><a name="l01109"></a><span class="lineno"> 1109</span>&#160; * \brief this kernel updates all synaptic weights</div>
<div class="line"><a name="l01110"></a><span class="lineno"> 1110</span>&#160; *</div>
<div class="line"><a name="l01111"></a><span class="lineno"> 1111</span>&#160; * net access: stdpScaleFactor, wtChangeDecay</div>
<div class="line"><a name="l01112"></a><span class="lineno"> 1112</span>&#160; * grp access: homeostasisScale, avgTimeScaleInv, FixedInputWts, WithESTDPtype, WithISTDOtype, WithHomeostasis</div>
<div class="line"><a name="l01113"></a><span class="lineno"> 1113</span>&#160; * rtd access: Npre_plastic, cumulativePre, avgFiring, baseFiringInv, baseFiring, wt, wtChange, maxSynWt</div>
<div class="line"><a name="l01114"></a><span class="lineno"> 1114</span>&#160; * glb access:</div>
<div class="line"><a name="l01115"></a><span class="lineno"> 1115</span>&#160; */</div>
<div class="line"><a name="l01116"></a><span class="lineno"> 1116</span>&#160;__global__ void kernel_updateWeights() {</div>
<div class="line"><a name="l01117"></a><span class="lineno"> 1117</span>&#160;   __shared__ volatile int errCode;</div>
<div class="line"><a name="l01118"></a><span class="lineno"> 1118</span>&#160;   __shared__ int          startId, lastId, grpId, totBuffers, grpNCnt;</div>
<div class="line"><a name="l01119"></a><span class="lineno"> 1119</span>&#160;   __shared__ int2         threadLoad;</div>
<div class="line"><a name="l01120"></a><span class="lineno"> 1120</span>&#160;   // added for homeostasis</div>
<div class="line"><a name="l01121"></a><span class="lineno"> 1121</span>&#160;   __shared__ float        homeostasisScale, avgTimeScaleInv;</div>
<div class="line"><a name="l01122"></a><span class="lineno"> 1122</span>&#160;</div>
<div class="line"><a name="l01123"></a><span class="lineno"> 1123</span>&#160;   if(threadIdx.x == 0) {</div>
<div class="line"><a name="l01124"></a><span class="lineno"> 1124</span>&#160;       totBuffers = loadBufferCount;</div>
<div class="line"><a name="l01125"></a><span class="lineno"> 1125</span>&#160;       grpNCnt = (blockDim.x / UPWTS_CLUSTERING_SZ) + ((blockDim.x % UPWTS_CLUSTERING_SZ) != 0);</div>
<div class="line"><a name="l01126"></a><span class="lineno"> 1126</span>&#160;   }</div>
<div class="line"><a name="l01127"></a><span class="lineno"> 1127</span>&#160;</div>
<div class="line"><a name="l01128"></a><span class="lineno"> 1128</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l01129"></a><span class="lineno"> 1129</span>&#160;</div>
<div class="line"><a name="l01130"></a><span class="lineno"> 1130</span>&#160;   for (int bufPos = blockIdx.x; bufPos &lt; totBuffers; bufPos += gridDim.x) {</div>
<div class="line"><a name="l01131"></a><span class="lineno"> 1131</span>&#160;       // KILLME !!! This can be further optimized ....</div>
<div class="line"><a name="l01132"></a><span class="lineno"> 1132</span>&#160;       // instead of reading each neuron group separately .....</div>
<div class="line"><a name="l01133"></a><span class="lineno"> 1133</span>&#160;       // read a whole buffer and use the result ......</div>
<div class="line"><a name="l01134"></a><span class="lineno"> 1134</span>&#160;       // if ( threadIdx.x) { // TSC: this could be a performance bug, 127 threads other than the first thread try to read</div>
<div class="line"><a name="l01135"></a><span class="lineno"> 1135</span>&#160;                              // threadLoad and wirte homeostatsisScale and avgTimeScaleInv at the same time</div>
<div class="line"><a name="l01136"></a><span class="lineno"> 1136</span>&#160;       if (threadIdx.x == 0) {</div>
<div class="line"><a name="l01137"></a><span class="lineno"> 1137</span>&#160;           threadLoad  = getStaticThreadLoad(bufPos);</div>
<div class="line"><a name="l01138"></a><span class="lineno"> 1138</span>&#160;           startId     = STATIC_LOAD_START(threadLoad);</div>
<div class="line"><a name="l01139"></a><span class="lineno"> 1139</span>&#160;           lastId      = STATIC_LOAD_SIZE(threadLoad);</div>
<div class="line"><a name="l01140"></a><span class="lineno"> 1140</span>&#160;           grpId       = STATIC_LOAD_GROUP(threadLoad);</div>
<div class="line"><a name="l01141"></a><span class="lineno"> 1141</span>&#160;</div>
<div class="line"><a name="l01142"></a><span class="lineno"> 1142</span>&#160;           // load homestasis parameters</div>
<div class="line"><a name="l01143"></a><span class="lineno"> 1143</span>&#160;           if (groupConfigsGPU[grpId].WithHomeostasis) {</div>
<div class="line"><a name="l01144"></a><span class="lineno"> 1144</span>&#160;               homeostasisScale = groupConfigsGPU[grpId].homeostasisScale;</div>
<div class="line"><a name="l01145"></a><span class="lineno"> 1145</span>&#160;               avgTimeScaleInv = groupConfigsGPU[grpId].avgTimeScaleInv;</div>
<div class="line"><a name="l01146"></a><span class="lineno"> 1146</span>&#160;           } else {</div>
<div class="line"><a name="l01147"></a><span class="lineno"> 1147</span>&#160;               homeostasisScale = 0.0f;</div>
<div class="line"><a name="l01148"></a><span class="lineno"> 1148</span>&#160;               avgTimeScaleInv = 1.0f;</div>
<div class="line"><a name="l01149"></a><span class="lineno"> 1149</span>&#160;           }</div>
<div class="line"><a name="l01150"></a><span class="lineno"> 1150</span>&#160;       }</div>
<div class="line"><a name="l01151"></a><span class="lineno"> 1151</span>&#160;</div>
<div class="line"><a name="l01152"></a><span class="lineno"> 1152</span>&#160;       __syncthreads();</div>
<div class="line"><a name="l01153"></a><span class="lineno"> 1153</span>&#160;</div>
<div class="line"><a name="l01154"></a><span class="lineno"> 1154</span>&#160;       // the weights are fixed for this group.. so dont make any changes on</div>
<div class="line"><a name="l01155"></a><span class="lineno"> 1155</span>&#160;       // the weight and continue to the next set of neurons...</div>
<div class="line"><a name="l01156"></a><span class="lineno"> 1156</span>&#160;       if (groupConfigsGPU[grpId].FixedInputWts)</div>
<div class="line"><a name="l01157"></a><span class="lineno"> 1157</span>&#160;           continue;</div>
<div class="line"><a name="l01158"></a><span class="lineno"> 1158</span>&#160;</div>
<div class="line"><a name="l01159"></a><span class="lineno"> 1159</span>&#160;       int nid = (threadIdx.x / UPWTS_CLUSTERING_SZ) + startId;</div>
<div class="line"><a name="l01160"></a><span class="lineno"> 1160</span>&#160;       // update the synaptic weights from the synaptic weight derivatives</div>
<div class="line"><a name="l01161"></a><span class="lineno"> 1161</span>&#160;       for(; nid &lt; startId + lastId; nid += grpNCnt) {</div>
<div class="line"><a name="l01162"></a><span class="lineno"> 1162</span>&#160;           int Npre_plastic = runtimeDataGPU.Npre_plastic[nid];</div>
<div class="line"><a name="l01163"></a><span class="lineno"> 1163</span>&#160;           unsigned int cumulativePre = runtimeDataGPU.cumulativePre[nid];</div>
<div class="line"><a name="l01164"></a><span class="lineno"> 1164</span>&#160;           float diff_firing  = 0.0f;</div>
<div class="line"><a name="l01165"></a><span class="lineno"> 1165</span>&#160;           float baseFiring = 0.0f;</div>
<div class="line"><a name="l01166"></a><span class="lineno"> 1166</span>&#160;</div>
<div class="line"><a name="l01167"></a><span class="lineno"> 1167</span>&#160;           if (groupConfigsGPU[grpId].WithHomeostasis) {</div>
<div class="line"><a name="l01168"></a><span class="lineno"> 1168</span>&#160;               diff_firing = (1.0f - runtimeDataGPU.avgFiring[nid] * runtimeDataGPU.baseFiringInv[nid]);</div>
<div class="line"><a name="l01169"></a><span class="lineno"> 1169</span>&#160;               baseFiring = runtimeDataGPU.baseFiring[nid];</div>
<div class="line"><a name="l01170"></a><span class="lineno"> 1170</span>&#160;           }</div>
<div class="line"><a name="l01171"></a><span class="lineno"> 1171</span>&#160;</div>
<div class="line"><a name="l01172"></a><span class="lineno"> 1172</span>&#160;           const int threadIdGrp = (threadIdx.x % UPWTS_CLUSTERING_SZ);</div>
<div class="line"><a name="l01173"></a><span class="lineno"> 1173</span>&#160;           // use 32 threads to update 32 synapses parallely</div>
<div class="line"><a name="l01174"></a><span class="lineno"> 1174</span>&#160;           for(unsigned int synIdOffset = cumulativePre; synIdOffset &lt; cumulativePre + Npre_plastic; synIdOffset += UPWTS_CLUSTERING_SZ) {</div>
<div class="line"><a name="l01175"></a><span class="lineno"> 1175</span>&#160;               //excitatory connection change the synaptic weights</div>
<div class="line"><a name="l01176"></a><span class="lineno"> 1176</span>&#160;               unsigned int synId = synIdOffset + threadIdGrp;</div>
<div class="line"><a name="l01177"></a><span class="lineno"> 1177</span>&#160;               if(synId &lt; cumulativePre + Npre_plastic) {</div>
<div class="line"><a name="l01178"></a><span class="lineno"> 1178</span>&#160;                   updateSynapticWeights(nid, synId, grpId, diff_firing, homeostasisScale, baseFiring, avgTimeScaleInv);</div>
<div class="line"><a name="l01179"></a><span class="lineno"> 1179</span>&#160;               }</div>
<div class="line"><a name="l01180"></a><span class="lineno"> 1180</span>&#160;           }</div>
<div class="line"><a name="l01181"></a><span class="lineno"> 1181</span>&#160;       }</div>
<div class="line"><a name="l01182"></a><span class="lineno"> 1182</span>&#160;   }</div>
<div class="line"><a name="l01183"></a><span class="lineno"> 1183</span>&#160;}</div>
<div class="line"><a name="l01184"></a><span class="lineno"> 1184</span>&#160;</div>
<div class="line"><a name="l01185"></a><span class="lineno"> 1185</span>&#160;//********************************UPDATE TABLES AND COUNTERS EVERY SECOND  *************************************************************</div>
<div class="line"><a name="l01186"></a><span class="lineno"> 1186</span>&#160;</div>
<div class="line"><a name="l01187"></a><span class="lineno"> 1187</span>&#160;/*!</div>
<div class="line"><a name="l01188"></a><span class="lineno"> 1188</span>&#160; * \brief This kernel shift the un-processed firing information in firingTableD2 to the beginning of</div>
<div class="line"><a name="l01189"></a><span class="lineno"> 1189</span>&#160; * firingTableD2 for the next second of simulation.</div>
<div class="line"><a name="l01190"></a><span class="lineno"> 1190</span>&#160; *</div>
<div class="line"><a name="l01191"></a><span class="lineno"> 1191</span>&#160; * net access: maxDelay</div>
<div class="line"><a name="l01192"></a><span class="lineno"> 1192</span>&#160; * grp access: N/A</div>
<div class="line"><a name="l01193"></a><span class="lineno"> 1193</span>&#160; * rtd access: firingTableD2</div>
<div class="line"><a name="l01194"></a><span class="lineno"> 1194</span>&#160; * glb access: timeTableD2GPU</div>
<div class="line"><a name="l01195"></a><span class="lineno"> 1195</span>&#160; */</div>
<div class="line"><a name="l01196"></a><span class="lineno"> 1196</span>&#160;__global__ void kernel_shiftFiringTable() {</div>
<div class="line"><a name="l01197"></a><span class="lineno"> 1197</span>&#160;   int gnthreads= blockDim.x * gridDim.x;</div>
<div class="line"><a name="l01198"></a><span class="lineno"> 1198</span>&#160;</div>
<div class="line"><a name="l01199"></a><span class="lineno"> 1199</span>&#160;   for(int p = timeTableD2GPU[999], k = 0; p &lt; timeTableD2GPU[999 + networkConfigGPU.maxDelay + 1]; p += gnthreads, k += gnthreads) {</div>
<div class="line"><a name="l01200"></a><span class="lineno"> 1200</span>&#160;       if ((p + threadIdx.x) &lt; timeTableD2GPU[999 + networkConfigGPU.maxDelay + 1])</div>
<div class="line"><a name="l01201"></a><span class="lineno"> 1201</span>&#160;           runtimeDataGPU.firingTableD2[k + threadIdx.x] = runtimeDataGPU.firingTableD2[p + threadIdx.x];</div>
<div class="line"><a name="l01202"></a><span class="lineno"> 1202</span>&#160;   }</div>
<div class="line"><a name="l01203"></a><span class="lineno"> 1203</span>&#160;}</div>
<div class="line"><a name="l01204"></a><span class="lineno"> 1204</span>&#160;</div>
<div class="line"><a name="l01205"></a><span class="lineno"> 1205</span>&#160;/*!</div>
<div class="line"><a name="l01206"></a><span class="lineno"> 1206</span>&#160; * \brief This kernel shift the un-processed firing information in timeTableD1(D2)GPU to the beginning of</div>
<div class="line"><a name="l01207"></a><span class="lineno"> 1207</span>&#160; * timeTableD1(D2)GPU for the next second of simulation.</div>
<div class="line"><a name="l01208"></a><span class="lineno"> 1208</span>&#160; *</div>
<div class="line"><a name="l01209"></a><span class="lineno"> 1209</span>&#160; * After all the threads/blocks had adjusted the firingTableD1(D2)GPU, we update the timeTableD1(D2)GPU</div>
<div class="line"><a name="l01210"></a><span class="lineno"> 1210</span>&#160; * so that the firing information that happended in the last maxDelay_ time step would become</div>
<div class="line"><a name="l01211"></a><span class="lineno"> 1211</span>&#160; * the first maxDelay_ time step firing information for the next second of simulation.</div>
<div class="line"><a name="l01212"></a><span class="lineno"> 1212</span>&#160; * We also reset/update all spike counters to appropriate values as indicated in the second part </div>
<div class="line"><a name="l01213"></a><span class="lineno"> 1213</span>&#160; * of this kernel.</div>
<div class="line"><a name="l01214"></a><span class="lineno"> 1214</span>&#160; */</div>
<div class="line"><a name="l01215"></a><span class="lineno"> 1215</span>&#160;__global__ void kernel_shiftTimeTable() {</div>
<div class="line"><a name="l01216"></a><span class="lineno"> 1216</span>&#160;   int maxDelay = networkConfigGPU.maxDelay;</div>
<div class="line"><a name="l01217"></a><span class="lineno"> 1217</span>&#160;</div>
<div class="line"><a name="l01218"></a><span class="lineno"> 1218</span>&#160;   if(blockIdx.x == 0) {</div>
<div class="line"><a name="l01219"></a><span class="lineno"> 1219</span>&#160;       for(int i = threadIdx.x; i &lt; maxDelay; i += blockDim.x) {</div>
<div class="line"><a name="l01220"></a><span class="lineno"> 1220</span>&#160;           // use i+1 instead of just i because timeTableD2GPU[0] should always be 0</div>
<div class="line"><a name="l01221"></a><span class="lineno"> 1221</span>&#160;           timeTableD2GPU[i + 1] = timeTableD2GPU[1000 + i + 1] - timeTableD2GPU[1000];</div>
<div class="line"><a name="l01222"></a><span class="lineno"> 1222</span>&#160;           timeTableD1GPU[i + 1] = timeTableD1GPU[1000 + i + 1] - timeTableD1GPU[1000];</div>
<div class="line"><a name="l01223"></a><span class="lineno"> 1223</span>&#160;       }</div>
<div class="line"><a name="l01224"></a><span class="lineno"> 1224</span>&#160;   }</div>
<div class="line"><a name="l01225"></a><span class="lineno"> 1225</span>&#160;</div>
<div class="line"><a name="l01226"></a><span class="lineno"> 1226</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l01227"></a><span class="lineno"> 1227</span>&#160;</div>
<div class="line"><a name="l01228"></a><span class="lineno"> 1228</span>&#160;   // reset various counters for the firing information</div>
<div class="line"><a name="l01229"></a><span class="lineno"> 1229</span>&#160;   if((blockIdx.x == 0) &amp;&amp; (threadIdx.x == 0)) {</div>
<div class="line"><a name="l01230"></a><span class="lineno"> 1230</span>&#160;       timeTableD1GPU[maxDelay]  = 0;</div>
<div class="line"><a name="l01231"></a><span class="lineno"> 1231</span>&#160;       spikeCountD2GPU += spikeCountD2SecGPU;</div>
<div class="line"><a name="l01232"></a><span class="lineno"> 1232</span>&#160;       spikeCountD1GPU += spikeCountD1SecGPU;</div>
<div class="line"><a name="l01233"></a><span class="lineno"> 1233</span>&#160;</div>
<div class="line"><a name="l01234"></a><span class="lineno"> 1234</span>&#160;       spikeCountD2SecGPU = 0; </div>
<div class="line"><a name="l01235"></a><span class="lineno"> 1235</span>&#160;       spikeCountD1SecGPU = 0;</div>
<div class="line"><a name="l01236"></a><span class="lineno"> 1236</span>&#160;</div>
<div class="line"><a name="l01237"></a><span class="lineno"> 1237</span>&#160;       spikeCountExtRxD2SecGPU = 0;</div>
<div class="line"><a name="l01238"></a><span class="lineno"> 1238</span>&#160;       spikeCountExtRxD1SecGPU = 0;</div>
<div class="line"><a name="l01239"></a><span class="lineno"> 1239</span>&#160;</div>
<div class="line"><a name="l01240"></a><span class="lineno"> 1240</span>&#160;       spikeCountLastSecLeftD2GPU = timeTableD2GPU[maxDelay];</div>
<div class="line"><a name="l01241"></a><span class="lineno"> 1241</span>&#160;       secD2fireCntTest = timeTableD2GPU[maxDelay];</div>
<div class="line"><a name="l01242"></a><span class="lineno"> 1242</span>&#160;       secD1fireCntTest = 0;</div>
<div class="line"><a name="l01243"></a><span class="lineno"> 1243</span>&#160;   }</div>
<div class="line"><a name="l01244"></a><span class="lineno"> 1244</span>&#160;}</div>
<div class="line"><a name="l01245"></a><span class="lineno"> 1245</span>&#160;</div>
<div class="line"><a name="l01246"></a><span class="lineno"> 1246</span>&#160;//****************************** GENERATE POST-SYNAPTIC CURRENT EVERY TIME-STEP  ****************************</div>
<div class="line"><a name="l01247"></a><span class="lineno"> 1247</span>&#160;</div>
<div class="line"><a name="l01248"></a><span class="lineno"> 1248</span>&#160;/*</div>
<div class="line"><a name="l01249"></a><span class="lineno"> 1249</span>&#160;* The sequence of handling an post synaptic spike in GPU mode:</div>
<div class="line"><a name="l01250"></a><span class="lineno"> 1250</span>&#160;* P1. Update synSpikeTime</div>
<div class="line"><a name="l01251"></a><span class="lineno"> 1251</span>&#160;* P2. Update DA,5HT,ACh,NE accordingly</div>
<div class="line"><a name="l01252"></a><span class="lineno"> 1252</span>&#160;* P3. Update STDP wtChange</div>
<div class="line"><a name="l01253"></a><span class="lineno"> 1253</span>&#160;* P4. Load wt into change (temporary variable)</div>
<div class="line"><a name="l01254"></a><span class="lineno"> 1254</span>&#160;* P5. Modulate change by STP (if enabled)</div>
<div class="line"><a name="l01255"></a><span class="lineno"> 1255</span>&#160;* P6-1. Modulate change by d_mulSynSlow and d_mulSynFast</div>
<div class="line"><a name="l01256"></a><span class="lineno"> 1256</span>&#160;* P6-2. Accumulate g(AMPA,NMDA,GABAa,GABAb) or current</div>
<div class="line"><a name="l01257"></a><span class="lineno"> 1257</span>&#160;* P7. Update v(voltage), u(recovery)</div>
<div class="line"><a name="l01258"></a><span class="lineno"> 1258</span>&#160;* P8. Update homeostasis</div>
<div class="line"><a name="l01259"></a><span class="lineno"> 1259</span>&#160;* P9. Decay and log DA,5HT,ACh,NE</div>
<div class="line"><a name="l01260"></a><span class="lineno"> 1260</span>&#160;*/</div>
<div class="line"><a name="l01261"></a><span class="lineno"> 1261</span>&#160;__device__ void generatePostSynapticSpike(int simTime, int preNId, int postNId, int synId) {</div>
<div class="line"><a name="l01262"></a><span class="lineno"> 1262</span>&#160;   // get the actual position of the synapses and other variables...</div>
<div class="line"><a name="l01263"></a><span class="lineno"> 1263</span>&#160;   unsigned int pos = runtimeDataGPU.cumulativePre[postNId] + synId;</div>
<div class="line"><a name="l01264"></a><span class="lineno"> 1264</span>&#160;</div>
<div class="line"><a name="l01265"></a><span class="lineno"> 1265</span>&#160;   short int preGrpId = runtimeDataGPU.grpIds[preNId]; // STP uses preGrpId</div>
<div class="line"><a name="l01266"></a><span class="lineno"> 1266</span>&#160;   short int postGrpId = runtimeDataGPU.grpIds[postNId]; // STDP uses postGrpId</div>
<div class="line"><a name="l01267"></a><span class="lineno"> 1267</span>&#160;</div>
<div class="line"><a name="l01268"></a><span class="lineno"> 1268</span>&#160;   setFiringBitSynapses(postNId, synId);</div>
<div class="line"><a name="l01269"></a><span class="lineno"> 1269</span>&#160;</div>
<div class="line"><a name="l01270"></a><span class="lineno"> 1270</span>&#160;   // P1</div>
<div class="line"><a name="l01271"></a><span class="lineno"> 1271</span>&#160;   runtimeDataGPU.synSpikeTime[pos] = simTime;       //uncoalesced access</div>
<div class="line"><a name="l01272"></a><span class="lineno"> 1272</span>&#160;</div>
<div class="line"><a name="l01273"></a><span class="lineno"> 1273</span>&#160;   // P2</div>
<div class="line"><a name="l01274"></a><span class="lineno"> 1274</span>&#160;   // Got one spike from dopaminergic neuron, increase dopamine concentration in the target area</div>
<div class="line"><a name="l01275"></a><span class="lineno"> 1275</span>&#160;   if (groupConfigsGPU[preGrpId].Type &amp; TARGET_DA) {</div>
<div class="line"><a name="l01276"></a><span class="lineno"> 1276</span>&#160;       atomicAdd(&amp;(runtimeDataGPU.grpDA[postGrpId]), 0.04f);</div>
<div class="line"><a name="l01277"></a><span class="lineno"> 1277</span>&#160;   }</div>
<div class="line"><a name="l01278"></a><span class="lineno"> 1278</span>&#160;</div>
<div class="line"><a name="l01279"></a><span class="lineno"> 1279</span>&#160;   // P3</div>
<div class="line"><a name="l01280"></a><span class="lineno"> 1280</span>&#160;   // STDP calculation: the post-synaptic neuron fires before the arrival of pre-synaptic neuron&#39;s spike</div>
<div class="line"><a name="l01281"></a><span class="lineno"> 1281</span>&#160;   if (groupConfigsGPU[postGrpId].WithSTDP &amp;&amp; !networkConfigGPU.sim_in_testing)  {</div>
<div class="line"><a name="l01282"></a><span class="lineno"> 1282</span>&#160;       int stdp_tDiff = simTime - runtimeDataGPU.lastSpikeTime[postNId];</div>
<div class="line"><a name="l01283"></a><span class="lineno"> 1283</span>&#160;       if (stdp_tDiff &gt;= 0) {</div>
<div class="line"><a name="l01284"></a><span class="lineno"> 1284</span>&#160;           if (groupConfigsGPU[postGrpId].WithESTDP) {</div>
<div class="line"><a name="l01285"></a><span class="lineno"> 1285</span>&#160;               // Handle E-STDP curves</div>
<div class="line"><a name="l01286"></a><span class="lineno"> 1286</span>&#160;               switch (groupConfigsGPU[postGrpId].WithESTDPcurve) {</div>
<div class="line"><a name="l01287"></a><span class="lineno"> 1287</span>&#160;               case EXP_CURVE: // exponential curve</div>
<div class="line"><a name="l01288"></a><span class="lineno"> 1288</span>&#160;               case TIMING_BASED_CURVE: // sc curve</div>
<div class="line"><a name="l01289"></a><span class="lineno"> 1289</span>&#160;                   if (stdp_tDiff * groupConfigsGPU[postGrpId].TAU_MINUS_INV_EXC &lt; 25.0f)</div>
<div class="line"><a name="l01290"></a><span class="lineno"> 1290</span>&#160;                       runtimeDataGPU.wtChange[pos] += STDP( stdp_tDiff, groupConfigsGPU[postGrpId].ALPHA_MINUS_EXC, groupConfigsGPU[postGrpId].TAU_MINUS_INV_EXC); // uncoalesced access</div>
<div class="line"><a name="l01291"></a><span class="lineno"> 1291</span>&#160;                   break;</div>
<div class="line"><a name="l01292"></a><span class="lineno"> 1292</span>&#160;               default:</div>
<div class="line"><a name="l01293"></a><span class="lineno"> 1293</span>&#160;                   break;</div>
<div class="line"><a name="l01294"></a><span class="lineno"> 1294</span>&#160;               }</div>
<div class="line"><a name="l01295"></a><span class="lineno"> 1295</span>&#160;           }</div>
<div class="line"><a name="l01296"></a><span class="lineno"> 1296</span>&#160;           if (groupConfigsGPU[postGrpId].WithISTDP) {</div>
<div class="line"><a name="l01297"></a><span class="lineno"> 1297</span>&#160;               // Handle I-STDP curves</div>
<div class="line"><a name="l01298"></a><span class="lineno"> 1298</span>&#160;               switch (groupConfigsGPU[postGrpId].WithISTDPcurve) {</div>
<div class="line"><a name="l01299"></a><span class="lineno"> 1299</span>&#160;               case EXP_CURVE: // exponential curve</div>
<div class="line"><a name="l01300"></a><span class="lineno"> 1300</span>&#160;                   if ((stdp_tDiff * groupConfigsGPU[postGrpId].TAU_MINUS_INV_INB) &lt; 25.0f) { // LTD of inhibitory syanpse, which increase synapse weight</div>
<div class="line"><a name="l01301"></a><span class="lineno"> 1301</span>&#160;                       runtimeDataGPU.wtChange[pos] -= STDP(stdp_tDiff, groupConfigsGPU[postGrpId].ALPHA_MINUS_INB, groupConfigsGPU[postGrpId].TAU_MINUS_INV_INB);</div>
<div class="line"><a name="l01302"></a><span class="lineno"> 1302</span>&#160;                   }</div>
<div class="line"><a name="l01303"></a><span class="lineno"> 1303</span>&#160;                   break;</div>
<div class="line"><a name="l01304"></a><span class="lineno"> 1304</span>&#160;               case PULSE_CURVE: // pulse curve</div>
<div class="line"><a name="l01305"></a><span class="lineno"> 1305</span>&#160;                   if (stdp_tDiff &lt;= groupConfigsGPU[postGrpId].LAMBDA) { // LTP of inhibitory synapse, which decreases synapse weight</div>
<div class="line"><a name="l01306"></a><span class="lineno"> 1306</span>&#160;                       runtimeDataGPU.wtChange[pos] -= groupConfigsGPU[postGrpId].BETA_LTP;</div>
<div class="line"><a name="l01307"></a><span class="lineno"> 1307</span>&#160;                   } else if (stdp_tDiff &lt;= groupConfigsGPU[postGrpId].DELTA) { // LTD of inhibitory syanpse, which increase synapse weight</div>
<div class="line"><a name="l01308"></a><span class="lineno"> 1308</span>&#160;                       runtimeDataGPU.wtChange[pos] -= groupConfigsGPU[postGrpId].BETA_LTD;</div>
<div class="line"><a name="l01309"></a><span class="lineno"> 1309</span>&#160;                   }</div>
<div class="line"><a name="l01310"></a><span class="lineno"> 1310</span>&#160;                   break;</div>
<div class="line"><a name="l01311"></a><span class="lineno"> 1311</span>&#160;               default:</div>
<div class="line"><a name="l01312"></a><span class="lineno"> 1312</span>&#160;                   break;</div>
<div class="line"><a name="l01313"></a><span class="lineno"> 1313</span>&#160;               }</div>
<div class="line"><a name="l01314"></a><span class="lineno"> 1314</span>&#160;           }</div>
<div class="line"><a name="l01315"></a><span class="lineno"> 1315</span>&#160;       }</div>
<div class="line"><a name="l01316"></a><span class="lineno"> 1316</span>&#160;   }</div>
<div class="line"><a name="l01317"></a><span class="lineno"> 1317</span>&#160;}</div>
<div class="line"><a name="l01318"></a><span class="lineno"> 1318</span>&#160;</div>
<div class="line"><a name="l01319"></a><span class="lineno"> 1319</span>&#160;#define READ_CHUNK_SZ 64</div>
<div class="line"><a name="l01320"></a><span class="lineno"> 1320</span>&#160;/*!</div>
<div class="line"><a name="l01321"></a><span class="lineno"> 1321</span>&#160; * \brief This kernel updates and generates spikes for delays greater than 1 from the fired neuron. </div>
<div class="line"><a name="l01322"></a><span class="lineno"> 1322</span>&#160; *</div>
<div class="line"><a name="l01323"></a><span class="lineno"> 1323</span>&#160; * The LTD computation is also executed by this kernel.</div>
<div class="line"><a name="l01324"></a><span class="lineno"> 1324</span>&#160; *</div>
<div class="line"><a name="l01325"></a><span class="lineno"> 1325</span>&#160; * net access: maxDelay, I_setPitch, sim_in_testing</div>
<div class="line"><a name="l01326"></a><span class="lineno"> 1326</span>&#160; * grp access: Type, WithSTDP, WithESTDP, WithESTDPcurve, WithISDP, WithISTDPcurve, all STDP parameters</div>
<div class="line"><a name="l01327"></a><span class="lineno"> 1327</span>&#160; * rtd access: firingTableD2, cumulativePost, postDelayInfo, postSynapticIds, cumulativePre, grpIds,</div>
<div class="line"><a name="l01328"></a><span class="lineno"> 1328</span>&#160; *             grpDA, I_set, (W)synSpikeTime, (R)lastSpikeTime, wtChange</div>
<div class="line"><a name="l01329"></a><span class="lineno"> 1329</span>&#160; * glb access: spikeCountD2SecGPU, timeTableD2GPU_tex, timeTableD2GPU_tex_offset</div>
<div class="line"><a name="l01330"></a><span class="lineno"> 1330</span>&#160; */</div>
<div class="line"><a name="l01331"></a><span class="lineno"> 1331</span>&#160;__global__ void kernel_doCurrentUpdateD2(int simTimeMs, int simTimeSec, int simTime) {</div>
<div class="line"><a name="l01332"></a><span class="lineno"> 1332</span>&#160;   __shared__  volatile int sh_neuronOffsetTable[READ_CHUNK_SZ + 2];</div>
<div class="line"><a name="l01333"></a><span class="lineno"> 1333</span>&#160;   __shared__  int sh_delayLength[READ_CHUNK_SZ + 2];</div>
<div class="line"><a name="l01334"></a><span class="lineno"> 1334</span>&#160;   __shared__  int sh_delayIndexStart[READ_CHUNK_SZ + 2];</div>
<div class="line"><a name="l01335"></a><span class="lineno"> 1335</span>&#160;   __shared__  int sh_firingId[READ_CHUNK_SZ + 2];</div>
<div class="line"><a name="l01336"></a><span class="lineno"> 1336</span>&#160;   __shared__ volatile int sh_NeuronCnt;</div>
<div class="line"><a name="l01337"></a><span class="lineno"> 1337</span>&#160;</div>
<div class="line"><a name="l01338"></a><span class="lineno"> 1338</span>&#160;   const int threadIdWarp = (threadIdx.x % WARP_SIZE);</div>
<div class="line"><a name="l01339"></a><span class="lineno"> 1339</span>&#160;   const int warpId       = (threadIdx.x / WARP_SIZE);</div>
<div class="line"><a name="l01340"></a><span class="lineno"> 1340</span>&#160;</div>
<div class="line"><a name="l01341"></a><span class="lineno"> 1341</span>&#160;   // this variable is used to record the</div>
<div class="line"><a name="l01342"></a><span class="lineno"> 1342</span>&#160;   // number of updates done by different blocks</div>
<div class="line"><a name="l01343"></a><span class="lineno"> 1343</span>&#160;   if(threadIdx.x&lt;=0)   {</div>
<div class="line"><a name="l01344"></a><span class="lineno"> 1344</span>&#160;       sh_NeuronCnt = 0;</div>
<div class="line"><a name="l01345"></a><span class="lineno"> 1345</span>&#160;   }</div>
<div class="line"><a name="l01346"></a><span class="lineno"> 1346</span>&#160;</div>
<div class="line"><a name="l01347"></a><span class="lineno"> 1347</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l01348"></a><span class="lineno"> 1348</span>&#160;</div>
<div class="line"><a name="l01349"></a><span class="lineno"> 1349</span>&#160;   // stores the number of fired neurons at time t</div>
<div class="line"><a name="l01350"></a><span class="lineno"> 1350</span>&#160;   int k = tex1Dfetch(timeTableD2GPU_tex, simTimeMs + networkConfigGPU.maxDelay + 1 + timeTableD2GPU_tex_offset) - 1;</div>
<div class="line"><a name="l01351"></a><span class="lineno"> 1351</span>&#160;</div>
<div class="line"><a name="l01352"></a><span class="lineno"> 1352</span>&#160;   // stores the number of fired neurons at time (t - maxDelay_)</div>
<div class="line"><a name="l01353"></a><span class="lineno"> 1353</span>&#160;   int k_end = tex1Dfetch(timeTableD2GPU_tex, simTimeMs + 1 + timeTableD2GPU_tex_offset);</div>
<div class="line"><a name="l01354"></a><span class="lineno"> 1354</span>&#160;</div>
<div class="line"><a name="l01355"></a><span class="lineno"> 1355</span>&#160;   int t_pos  = simTimeMs;</div>
<div class="line"><a name="l01356"></a><span class="lineno"> 1356</span>&#160;</div>
<div class="line"><a name="l01357"></a><span class="lineno"> 1357</span>&#160;   // we need to read (k-k_end) neurons from the firing </div>
<div class="line"><a name="l01358"></a><span class="lineno"> 1358</span>&#160;   // table and do necesary updates for all these post-synaptic</div>
<div class="line"><a name="l01359"></a><span class="lineno"> 1359</span>&#160;   // connection in these neurons..</div>
<div class="line"><a name="l01360"></a><span class="lineno"> 1360</span>&#160;   while ((k &gt;= k_end) &amp;&amp; (k &gt;= 0)) {</div>
<div class="line"><a name="l01361"></a><span class="lineno"> 1361</span>&#160;       // at any point of time EXCIT_READ_CHUNK_SZ neurons</div>
<div class="line"><a name="l01362"></a><span class="lineno"> 1362</span>&#160;       // read different firing id from the firing table</div>
<div class="line"><a name="l01363"></a><span class="lineno"> 1363</span>&#160;       if (threadIdx.x &lt; READ_CHUNK_SZ) { // use 64 threads</div>
<div class="line"><a name="l01364"></a><span class="lineno"> 1364</span>&#160;           int fPos = k - (READ_CHUNK_SZ * blockIdx.x) - threadIdx.x; </div>
<div class="line"><a name="l01365"></a><span class="lineno"> 1365</span>&#160;           if ((fPos &gt;= 0) &amp;&amp; (fPos &gt;= k_end)) {</div>
<div class="line"><a name="l01366"></a><span class="lineno"> 1366</span>&#160;</div>
<div class="line"><a name="l01367"></a><span class="lineno"> 1367</span>&#160;               // get the neuron nid here....</div>
<div class="line"><a name="l01368"></a><span class="lineno"> 1368</span>&#160;               //int val = runtimeDataGPU.firingTableD2[fPos];</div>
<div class="line"><a name="l01369"></a><span class="lineno"> 1369</span>&#160;               //int nid = GET_FIRING_TABLE_NID(val);</div>
<div class="line"><a name="l01370"></a><span class="lineno"> 1370</span>&#160;               int nid = runtimeDataGPU.firingTableD2[fPos];</div>
<div class="line"><a name="l01371"></a><span class="lineno"> 1371</span>&#160;</div>
<div class="line"><a name="l01372"></a><span class="lineno"> 1372</span>&#160;               // find the time of firing based on the firing number fPos</div>
<div class="line"><a name="l01373"></a><span class="lineno"> 1373</span>&#160;               while ( !((fPos &gt;= tex1Dfetch(timeTableD2GPU_tex, t_pos + networkConfigGPU.maxDelay + timeTableD2GPU_tex_offset)) </div>
<div class="line"><a name="l01374"></a><span class="lineno"> 1374</span>&#160;                   &amp;&amp; (fPos &lt; tex1Dfetch(timeTableD2GPU_tex, t_pos + networkConfigGPU.maxDelay + 1 + timeTableD2GPU_tex_offset)))) {</div>
<div class="line"><a name="l01375"></a><span class="lineno"> 1375</span>&#160;                   t_pos--;</div>
<div class="line"><a name="l01376"></a><span class="lineno"> 1376</span>&#160;               }</div>
<div class="line"><a name="l01377"></a><span class="lineno"> 1377</span>&#160;</div>
<div class="line"><a name="l01378"></a><span class="lineno"> 1378</span>&#160;               // find the time difference between firing of the neuron and the current time</div>
<div class="line"><a name="l01379"></a><span class="lineno"> 1379</span>&#160;               int tD  = simTimeMs - t_pos;</div>
<div class="line"><a name="l01380"></a><span class="lineno"> 1380</span>&#160;</div>
<div class="line"><a name="l01381"></a><span class="lineno"> 1381</span>&#160;               // find the various delay parameters for neuron &#39;nid&#39;, with a delay of &#39;tD&#39;</div>
<div class="line"><a name="l01382"></a><span class="lineno"> 1382</span>&#160;               //sh_axonDelay[threadIdx.x]  = tD;</div>
<div class="line"><a name="l01383"></a><span class="lineno"> 1383</span>&#160;               int tPos = (networkConfigGPU.maxDelay + 1) * nid + tD;</div>
<div class="line"><a name="l01384"></a><span class="lineno"> 1384</span>&#160;               //sh_firingId[threadIdx.x]       = val;</div>
<div class="line"><a name="l01385"></a><span class="lineno"> 1385</span>&#160;               sh_firingId[threadIdx.x] = nid;</div>
<div class="line"><a name="l01386"></a><span class="lineno"> 1386</span>&#160;               sh_neuronOffsetTable[threadIdx.x]= runtimeDataGPU.cumulativePost[nid];</div>
<div class="line"><a name="l01387"></a><span class="lineno"> 1387</span>&#160;               sh_delayLength[threadIdx.x]      = runtimeDataGPU.postDelayInfo[tPos].delay_length;</div>
<div class="line"><a name="l01388"></a><span class="lineno"> 1388</span>&#160;               sh_delayIndexStart[threadIdx.x]  = runtimeDataGPU.postDelayInfo[tPos].delay_index_start;</div>
<div class="line"><a name="l01389"></a><span class="lineno"> 1389</span>&#160;</div>
<div class="line"><a name="l01390"></a><span class="lineno"> 1390</span>&#160;               // This is to indicate that the current thread</div>
<div class="line"><a name="l01391"></a><span class="lineno"> 1391</span>&#160;               // has a valid delay parameter for post-synaptic firing generation</div>
<div class="line"><a name="l01392"></a><span class="lineno"> 1392</span>&#160;               atomicAdd((int*)&amp;sh_NeuronCnt, 1);</div>
<div class="line"><a name="l01393"></a><span class="lineno"> 1393</span>&#160;           }</div>
<div class="line"><a name="l01394"></a><span class="lineno"> 1394</span>&#160;       }</div>
<div class="line"><a name="l01395"></a><span class="lineno"> 1395</span>&#160;</div>
<div class="line"><a name="l01396"></a><span class="lineno"> 1396</span>&#160;       __syncthreads();</div>
<div class="line"><a name="l01397"></a><span class="lineno"> 1397</span>&#160;</div>
<div class="line"><a name="l01398"></a><span class="lineno"> 1398</span>&#160;       // if cnt is zero than no more neurons need to generate</div>
<div class="line"><a name="l01399"></a><span class="lineno"> 1399</span>&#160;       // post-synaptic firing, then we break the loop.</div>
<div class="line"><a name="l01400"></a><span class="lineno"> 1400</span>&#160;       if (sh_NeuronCnt == 0) {</div>
<div class="line"><a name="l01401"></a><span class="lineno"> 1401</span>&#160;           break;</div>
<div class="line"><a name="l01402"></a><span class="lineno"> 1402</span>&#160;       }</div>
<div class="line"><a name="l01403"></a><span class="lineno"> 1403</span>&#160;</div>
<div class="line"><a name="l01404"></a><span class="lineno"> 1404</span>&#160;       // first WARP_SIZE threads the post synaptic</div>
<div class="line"><a name="l01405"></a><span class="lineno"> 1405</span>&#160;       // firing for first neuron, and so on. each of this group</div>
<div class="line"><a name="l01406"></a><span class="lineno"> 1406</span>&#160;       // needs to generate (numPostSynapses/maxDelay_) spikes for every fired neuron, every second</div>
<div class="line"><a name="l01407"></a><span class="lineno"> 1407</span>&#160;       // for numPostSynapses=500,maxDelay_=20, we need to generate 25 spikes for each fired neuron</div>
<div class="line"><a name="l01408"></a><span class="lineno"> 1408</span>&#160;       // for numPostSynapses=600,maxDelay_=20, we need to generate 30 spikes for each fired neuron </div>
<div class="line"><a name="l01409"></a><span class="lineno"> 1409</span>&#160;       for (int pos = warpId; pos &lt; sh_NeuronCnt; pos += (NUM_THREADS / WARP_SIZE)) {</div>
<div class="line"><a name="l01410"></a><span class="lineno"> 1410</span>&#160;</div>
<div class="line"><a name="l01411"></a><span class="lineno"> 1411</span>&#160;           int delId = threadIdWarp;</div>
<div class="line"><a name="l01412"></a><span class="lineno"> 1412</span>&#160;</div>
<div class="line"><a name="l01413"></a><span class="lineno"> 1413</span>&#160;           while (delId &lt; sh_delayLength[pos]) {</div>
<div class="line"><a name="l01414"></a><span class="lineno"> 1414</span>&#160;               // get the post synaptic information for specific delay</div>
<div class="line"><a name="l01415"></a><span class="lineno"> 1415</span>&#160;               SynInfo postInfo = runtimeDataGPU.postSynapticIds[sh_neuronOffsetTable[pos] + sh_delayIndexStart[pos] + delId];</div>
<div class="line"><a name="l01416"></a><span class="lineno"> 1416</span>&#160;               int postNId = GET_CONN_NEURON_ID(postInfo); // get post-neuron id</div>
<div class="line"><a name="l01417"></a><span class="lineno"> 1417</span>&#160;               int synId = GET_CONN_SYN_ID(postInfo);      // get synaptic id</div>
<div class="line"><a name="l01418"></a><span class="lineno"> 1418</span>&#160;</div>
<div class="line"><a name="l01419"></a><span class="lineno"> 1419</span>&#160;               if (postNId &lt; networkConfigGPU.numN) // test if post-neuron is a local neuron</div>
<div class="line"><a name="l01420"></a><span class="lineno"> 1420</span>&#160;                   generatePostSynapticSpike(simTime, sh_firingId[pos] /* preNId */, postNId, synId);</div>
<div class="line"><a name="l01421"></a><span class="lineno"> 1421</span>&#160;</div>
<div class="line"><a name="l01422"></a><span class="lineno"> 1422</span>&#160;               delId += WARP_SIZE;</div>
<div class="line"><a name="l01423"></a><span class="lineno"> 1423</span>&#160;           }</div>
<div class="line"><a name="l01424"></a><span class="lineno"> 1424</span>&#160;       } //(for all excitory neurons in table)</div>
<div class="line"><a name="l01425"></a><span class="lineno"> 1425</span>&#160;</div>
<div class="line"><a name="l01426"></a><span class="lineno"> 1426</span>&#160;       __syncthreads();</div>
<div class="line"><a name="l01427"></a><span class="lineno"> 1427</span>&#160;</div>
<div class="line"><a name="l01428"></a><span class="lineno"> 1428</span>&#160;       if(threadIdx.x == 0) {</div>
<div class="line"><a name="l01429"></a><span class="lineno"> 1429</span>&#160;           sh_NeuronCnt = 0;</div>
<div class="line"><a name="l01430"></a><span class="lineno"> 1430</span>&#160;       }</div>
<div class="line"><a name="l01431"></a><span class="lineno"> 1431</span>&#160;</div>
<div class="line"><a name="l01432"></a><span class="lineno"> 1432</span>&#160;       k = k - (gridDim.x * READ_CHUNK_SZ);</div>
<div class="line"><a name="l01433"></a><span class="lineno"> 1433</span>&#160;</div>
<div class="line"><a name="l01434"></a><span class="lineno"> 1434</span>&#160;       __syncthreads();</div>
<div class="line"><a name="l01435"></a><span class="lineno"> 1435</span>&#160;   }</div>
<div class="line"><a name="l01436"></a><span class="lineno"> 1436</span>&#160;</div>
<div class="line"><a name="l01437"></a><span class="lineno"> 1437</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l01438"></a><span class="lineno"> 1438</span>&#160;}</div>
<div class="line"><a name="l01439"></a><span class="lineno"> 1439</span>&#160;</div>
<div class="line"><a name="l01440"></a><span class="lineno"> 1440</span>&#160;/*!</div>
<div class="line"><a name="l01441"></a><span class="lineno"> 1441</span>&#160; * \brief This kernel updating and generating spikes on connections with a delay of 1ms from the fired neuron.</div>
<div class="line"><a name="l01442"></a><span class="lineno"> 1442</span>&#160; *</div>
<div class="line"><a name="l01443"></a><span class="lineno"> 1443</span>&#160; * This function looks mostly like kernel_doCurrentUpdateD2() but has been optimized for a fixed delay of 1ms. </div>
<div class="line"><a name="l01444"></a><span class="lineno"> 1444</span>&#160; * Ultimately we may merge this kernel with the kernel_doCurrentUpdateD2().</div>
<div class="line"><a name="l01445"></a><span class="lineno"> 1445</span>&#160; * The LTD computation is also executed by this kernel.</div>
<div class="line"><a name="l01446"></a><span class="lineno"> 1446</span>&#160; *</div>
<div class="line"><a name="l01447"></a><span class="lineno"> 1447</span>&#160; * net access: maxDelay, I_setPitch, sim_in_testing</div>
<div class="line"><a name="l01448"></a><span class="lineno"> 1448</span>&#160; * grp access: Type, grpDA, WithSTDP, WithESTDP, WithISTDP, WithESTDPcurve, WithISTDPcurve, all STDP parameters</div>
<div class="line"><a name="l01449"></a><span class="lineno"> 1449</span>&#160; * rtd access: postSynapticIds, cumulativePre, grpIds, I_set, wtChange, (R)lastSpikeTime, (W)synSpikeTime</div>
<div class="line"><a name="l01450"></a><span class="lineno"> 1450</span>&#160; * glb access: timeTableD1GPU, spikeCountD1SecGPU, firingTableD1</div>
<div class="line"><a name="l01451"></a><span class="lineno"> 1451</span>&#160; */</div>
<div class="line"><a name="l01452"></a><span class="lineno"> 1452</span>&#160;__global__ void kernel_doCurrentUpdateD1(int simTimeMs, int simTimeSec, int simTime) {</div>
<div class="line"><a name="l01453"></a><span class="lineno"> 1453</span>&#160;   __shared__ volatile int sh_NeuronCnt;</div>
<div class="line"><a name="l01454"></a><span class="lineno"> 1454</span>&#160;   __shared__ volatile int sh_neuronOffsetTable[NUM_THREADS / WARP_SIZE + 2];</div>
<div class="line"><a name="l01455"></a><span class="lineno"> 1455</span>&#160;   __shared__ int sh_delayLength[NUM_THREADS / WARP_SIZE + 2];</div>
<div class="line"><a name="l01456"></a><span class="lineno"> 1456</span>&#160;   __shared__ int sh_firingId[NUM_THREADS / WARP_SIZE + 2];</div>
<div class="line"><a name="l01457"></a><span class="lineno"> 1457</span>&#160;   __shared__ int sh_delayIndexStart[NUM_THREADS / WARP_SIZE + 2];</div>
<div class="line"><a name="l01458"></a><span class="lineno"> 1458</span>&#160;   __shared__ int sh_timing;</div>
<div class="line"><a name="l01459"></a><span class="lineno"> 1459</span>&#160;   __shared__ int kPosEnd;</div>
<div class="line"><a name="l01460"></a><span class="lineno"> 1460</span>&#160;</div>
<div class="line"><a name="l01461"></a><span class="lineno"> 1461</span>&#160;   const int warpId       = threadIdx.x / WARP_SIZE;  // warp id</div>
<div class="line"><a name="l01462"></a><span class="lineno"> 1462</span>&#160;   const int numWarps     = blockDim.x / WARP_SIZE;   // number of warp</div>
<div class="line"><a name="l01463"></a><span class="lineno"> 1463</span>&#160;   const int threadIdWarp = threadIdx.x % WARP_SIZE;  // thread id within a warp</div>
<div class="line"><a name="l01464"></a><span class="lineno"> 1464</span>&#160;</div>
<div class="line"><a name="l01465"></a><span class="lineno"> 1465</span>&#160;   // load the time table for neuron firing</div>
<div class="line"><a name="l01466"></a><span class="lineno"> 1466</span>&#160;   if (threadIdx.x == 0) {</div>
<div class="line"><a name="l01467"></a><span class="lineno"> 1467</span>&#160;       sh_timing = timeTableD1GPU[simTimeMs + networkConfigGPU.maxDelay];   // number of fired neurons at simTimeMs - 1</div>
<div class="line"><a name="l01468"></a><span class="lineno"> 1468</span>&#160;       kPosEnd = timeTableD1GPU[simTimeMs + networkConfigGPU.maxDelay + 1]; // number of fired neurons at simTimeMs, which is equal to spikeCountD1SecGPU</div>
<div class="line"><a name="l01469"></a><span class="lineno"> 1469</span>&#160;   }</div>
<div class="line"><a name="l01470"></a><span class="lineno"> 1470</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l01471"></a><span class="lineno"> 1471</span>&#160;</div>
<div class="line"><a name="l01472"></a><span class="lineno"> 1472</span>&#160;   int kPos = sh_timing + (blockIdx.x * numWarps);</div>
<div class="line"><a name="l01473"></a><span class="lineno"> 1473</span>&#160;</div>
<div class="line"><a name="l01474"></a><span class="lineno"> 1474</span>&#160;   __syncthreads();</div>
<div class="line"><a name="l01475"></a><span class="lineno"> 1475</span>&#160;</div>
<div class="line"><a name="l01476"></a><span class="lineno"> 1476</span>&#160;   // Do current update as long as we have some valid neuron</div>
<div class="line"><a name="l01477"></a><span class="lineno"> 1477</span>&#160;   while ((kPos &gt;= 0) &amp;&amp; (kPos &lt; kPosEnd)) {</div>
<div class="line"><a name="l01478"></a><span class="lineno"> 1478</span>&#160;       int fPos = -1;</div>
<div class="line"><a name="l01479"></a><span class="lineno"> 1479</span>&#160;       // a group of threads (4 threads) loads the delay information</div>
<div class="line"><a name="l01480"></a><span class="lineno"> 1480</span>&#160;       if (threadIdx.x &lt; numWarps) {</div>
<div class="line"><a name="l01481"></a><span class="lineno"> 1481</span>&#160;           sh_neuronOffsetTable[threadIdx.x] = -1;</div>
<div class="line"><a name="l01482"></a><span class="lineno"> 1482</span>&#160;           fPos = kPos + threadIdx.x;</div>
<div class="line"><a name="l01483"></a><span class="lineno"> 1483</span>&#160;</div>
<div class="line"><a name="l01484"></a><span class="lineno"> 1484</span>&#160;           // find the neuron nid and also delay information from fPos</div>
<div class="line"><a name="l01485"></a><span class="lineno"> 1485</span>&#160;           if ((fPos &gt;= 0) &amp;&amp; (fPos &lt; kPosEnd)) {</div>
<div class="line"><a name="l01486"></a><span class="lineno"> 1486</span>&#160;               atomicAdd((int*)&amp;sh_NeuronCnt, 1);</div>
<div class="line"><a name="l01487"></a><span class="lineno"> 1487</span>&#160;               //int val  = runtimeDataGPU.firingTableD1[fPos];</div>
<div class="line"><a name="l01488"></a><span class="lineno"> 1488</span>&#160;               //int nid  = GET_FIRING_TABLE_NID(val);</div>
<div class="line"><a name="l01489"></a><span class="lineno"> 1489</span>&#160;               int nid = runtimeDataGPU.firingTableD1[fPos];</div>
<div class="line"><a name="l01490"></a><span class="lineno"> 1490</span>&#160;               int tPos = (networkConfigGPU.maxDelay + 1) * nid;</div>
<div class="line"><a name="l01491"></a><span class="lineno"> 1491</span>&#160;               //sh_firingId[threadIdx.x]       = val;</div>
<div class="line"><a name="l01492"></a><span class="lineno"> 1492</span>&#160;               sh_firingId[threadIdx.x] = nid;</div>
<div class="line"><a name="l01493"></a><span class="lineno"> 1493</span>&#160;               sh_neuronOffsetTable[threadIdx.x] = runtimeDataGPU.cumulativePost[nid];</div>
<div class="line"><a name="l01494"></a><span class="lineno"> 1494</span>&#160;               sh_delayLength[threadIdx.x]       = runtimeDataGPU.postDelayInfo[tPos].delay_length;</div>
<div class="line"><a name="l01495"></a><span class="lineno"> 1495</span>&#160;               sh_delayIndexStart[threadIdx.x]   = runtimeDataGPU.postDelayInfo[tPos].delay_index_start;</div>
<div class="line"><a name="l01496"></a><span class="lineno"> 1496</span>&#160;           }</div>
<div class="line"><a name="l01497"></a><span class="lineno"> 1497</span>&#160;       }</div>
<div class="line"><a name="l01498"></a><span class="lineno"> 1498</span>&#160;</div>
<div class="line"><a name="l01499"></a><span class="lineno"> 1499</span>&#160;       __syncthreads();</div>
<div class="line"><a name="l01500"></a><span class="lineno"> 1500</span>&#160;</div>
<div class="line"><a name="l01501"></a><span class="lineno"> 1501</span>&#160;       // no more fired neuron from table... we just break from loop</div>
<div class="line"><a name="l01502"></a><span class="lineno"> 1502</span>&#160;       if (sh_NeuronCnt == 0) {</div>
<div class="line"><a name="l01503"></a><span class="lineno"> 1503</span>&#160;           break;</div>
<div class="line"><a name="l01504"></a><span class="lineno"> 1504</span>&#160;       }</div>
<div class="line"><a name="l01505"></a><span class="lineno"> 1505</span>&#160;</div>
<div class="line"><a name="l01506"></a><span class="lineno"> 1506</span>&#160;       __syncthreads();</div>
<div class="line"><a name="l01507"></a><span class="lineno"> 1507</span>&#160;</div>
<div class="line"><a name="l01508"></a><span class="lineno"> 1508</span>&#160;       int offset = sh_neuronOffsetTable[warpId];</div>
<div class="line"><a name="l01509"></a><span class="lineno"> 1509</span>&#160;</div>
<div class="line"><a name="l01510"></a><span class="lineno"> 1510</span>&#160;       if (threadIdx.x == 0) {</div>
<div class="line"><a name="l01511"></a><span class="lineno"> 1511</span>&#160;           sh_NeuronCnt = 0;</div>
<div class="line"><a name="l01512"></a><span class="lineno"> 1512</span>&#160;       }</div>
<div class="line"><a name="l01513"></a><span class="lineno"> 1513</span>&#160;</div>
<div class="line"><a name="l01514"></a><span class="lineno"> 1514</span>&#160;       // 32 threads for generatePostSynapticSpike()</div>
<div class="line"><a name="l01515"></a><span class="lineno"> 1515</span>&#160;       if (offset &gt;= 0) {</div>
<div class="line"><a name="l01516"></a><span class="lineno"> 1516</span>&#160;           int delId = threadIdWarp;</div>
<div class="line"><a name="l01517"></a><span class="lineno"> 1517</span>&#160;</div>
<div class="line"><a name="l01518"></a><span class="lineno"> 1518</span>&#160;           while (delId &lt; sh_delayLength[warpId]) {</div>
<div class="line"><a name="l01519"></a><span class="lineno"> 1519</span>&#160;               // get the post synaptic information for specific delay</div>
<div class="line"><a name="l01520"></a><span class="lineno"> 1520</span>&#160;               SynInfo postInfo = runtimeDataGPU.postSynapticIds[offset + sh_delayIndexStart[warpId] + delId];</div>
<div class="line"><a name="l01521"></a><span class="lineno"> 1521</span>&#160;               int postNId = GET_CONN_NEURON_ID(postInfo); // get post-neuron id</div>
<div class="line"><a name="l01522"></a><span class="lineno"> 1522</span>&#160;               int synId = GET_CONN_SYN_ID(postInfo);      // get synaptic id</div>
<div class="line"><a name="l01523"></a><span class="lineno"> 1523</span>&#160;</div>
<div class="line"><a name="l01524"></a><span class="lineno"> 1524</span>&#160;               if (postNId &lt; networkConfigGPU.numN) // test if post-neuron is a local neuron</div>
<div class="line"><a name="l01525"></a><span class="lineno"> 1525</span>&#160;                   generatePostSynapticSpike(simTime, sh_firingId[warpId] /* preNId */, postNId, synId);</div>
<div class="line"><a name="l01526"></a><span class="lineno"> 1526</span>&#160;</div>
<div class="line"><a name="l01527"></a><span class="lineno"> 1527</span>&#160;               delId += WARP_SIZE;</div>
<div class="line"><a name="l01528"></a><span class="lineno"> 1528</span>&#160;           }</div>
<div class="line"><a name="l01529"></a><span class="lineno"> 1529</span>&#160;       }</div>
<div class="line"><a name="l01530"></a><span class="lineno"> 1530</span>&#160;</div>
<div class="line"><a name="l01531"></a><span class="lineno"> 1531</span>&#160;       __syncthreads();</div>
<div class="line"><a name="l01532"></a><span class="lineno"> 1532</span>&#160;</div>
<div class="line"><a name="l01533"></a><span class="lineno"> 1533</span>&#160;       kPos = kPos + (gridDim.x * numWarps);</div>
<div class="line"><a name="l01534"></a><span class="lineno"> 1534</span>&#160;   }</div>
<div class="line"><a name="l01535"></a><span class="lineno"> 1535</span>&#160;}</div>
<div class="line"><a name="l01536"></a><span class="lineno"> 1536</span>&#160;</div>
<div class="line"><a name="l01537"></a><span class="lineno"> 1537</span>&#160;__global__ void kernel_convertExtSpikesD2(int startIdx, int endIdx, int GtoLOffset) {</div>
<div class="line"><a name="l01538"></a><span class="lineno"> 1538</span>&#160;   int firingTableIdx = startIdx + blockIdx.x * blockDim.x + threadIdx.x;</div>
<div class="line"><a name="l01539"></a><span class="lineno"> 1539</span>&#160;   int spikeCountExtRx = endIdx - startIdx; // received external spike count</div>
<div class="line"><a name="l01540"></a><span class="lineno"> 1540</span>&#160;</div>
<div class="line"><a name="l01541"></a><span class="lineno"> 1541</span>&#160;   if (threadIdx.x == 0 &amp;&amp; blockIdx.x == 0) {</div>
<div class="line"><a name="l01542"></a><span class="lineno"> 1542</span>&#160;       secD2fireCntTest += spikeCountExtRx;</div>
<div class="line"><a name="l01543"></a><span class="lineno"> 1543</span>&#160;       spikeCountD2SecGPU += spikeCountExtRx;</div>
<div class="line"><a name="l01544"></a><span class="lineno"> 1544</span>&#160;       spikeCountExtRxD2GPU += spikeCountExtRx;</div>
<div class="line"><a name="l01545"></a><span class="lineno"> 1545</span>&#160;       spikeCountExtRxD2SecGPU += spikeCountExtRx;</div>
<div class="line"><a name="l01546"></a><span class="lineno"> 1546</span>&#160;   }</div>
<div class="line"><a name="l01547"></a><span class="lineno"> 1547</span>&#160;</div>
<div class="line"><a name="l01548"></a><span class="lineno"> 1548</span>&#160;   // FIXME: if endIdx - startIdx &gt; 64 * 128</div>
<div class="line"><a name="l01549"></a><span class="lineno"> 1549</span>&#160;   if (firingTableIdx &lt; endIdx)</div>
<div class="line"><a name="l01550"></a><span class="lineno"> 1550</span>&#160;       runtimeDataGPU.firingTableD2[firingTableIdx] += GtoLOffset;</div>
<div class="line"><a name="l01551"></a><span class="lineno"> 1551</span>&#160;}</div>
<div class="line"><a name="l01552"></a><span class="lineno"> 1552</span>&#160;</div>
<div class="line"><a name="l01553"></a><span class="lineno"> 1553</span>&#160;__global__ void kernel_convertExtSpikesD1(int startIdx, int endIdx, int GtoLOffset) {</div>
<div class="line"><a name="l01554"></a><span class="lineno"> 1554</span>&#160;   int firingTableIdx = startIdx + blockIdx.x * blockDim.x + threadIdx.x;</div>
<div class="line"><a name="l01555"></a><span class="lineno"> 1555</span>&#160;   int spikeCountExtRx = endIdx - startIdx; // received external spike count</div>
<div class="line"><a name="l01556"></a><span class="lineno"> 1556</span>&#160;</div>
<div class="line"><a name="l01557"></a><span class="lineno"> 1557</span>&#160;   if (threadIdx.x == 0 &amp;&amp; blockIdx.x == 0) {</div>
<div class="line"><a name="l01558"></a><span class="lineno"> 1558</span>&#160;       secD1fireCntTest += spikeCountExtRx;</div>
<div class="line"><a name="l01559"></a><span class="lineno"> 1559</span>&#160;       spikeCountD1SecGPU += spikeCountExtRx;</div>
<div class="line"><a name="l01560"></a><span class="lineno"> 1560</span>&#160;       spikeCountExtRxD1GPU += spikeCountExtRx;</div>
<div class="line"><a name="l01561"></a><span class="lineno"> 1561</span>&#160;       spikeCountExtRxD1SecGPU += spikeCountExtRx;</div>
<div class="line"><a name="l01562"></a><span class="lineno"> 1562</span>&#160;   }</div>
<div class="line"><a name="l01563"></a><span class="lineno"> 1563</span>&#160;</div>
<div class="line"><a name="l01564"></a><span class="lineno"> 1564</span>&#160;   // FIXME: if endIdx - startIdx &gt; 64 * 128</div>
<div class="line"><a name="l01565"></a><span class="lineno"> 1565</span>&#160;   if (firingTableIdx &lt; endIdx)</div>
<div class="line"><a name="l01566"></a><span class="lineno"> 1566</span>&#160;       runtimeDataGPU.firingTableD1[firingTableIdx] += GtoLOffset;</div>
<div class="line"><a name="l01567"></a><span class="lineno"> 1567</span>&#160;}</div>
<div class="line"><a name="l01568"></a><span class="lineno"> 1568</span>&#160;</div>
<div class="line"><a name="l01569"></a><span class="lineno"> 1569</span>&#160;/*!</div>
<div class="line"><a name="l01570"></a><span class="lineno"> 1570</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies information of pre-connections to it</div>
<div class="line"><a name="l01571"></a><span class="lineno"> 1571</span>&#160; *</div>
<div class="line"><a name="l01572"></a><span class="lineno"> 1572</span>&#160; * This function:</div>
<div class="line"><a name="l01573"></a><span class="lineno"> 1573</span>&#160; * initialize Npre_plasticInv</div>
<div class="line"><a name="l01574"></a><span class="lineno"> 1574</span>&#160; * (allocate and) copy Npre, Npre_plastic, Npre_plasticInv, cumulativePre, preSynapticIds</div>
<div class="line"><a name="l01575"></a><span class="lineno"> 1575</span>&#160; * (allocate and) copy Npost, cumulativePost, postSynapticIds, postDelayInfo</div>
<div class="line"><a name="l01576"></a><span class="lineno"> 1576</span>&#160; *</div>
<div class="line"><a name="l01577"></a><span class="lineno"> 1577</span>&#160; *</div>
<div class="line"><a name="l01578"></a><span class="lineno"> 1578</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l01579"></a><span class="lineno"> 1579</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l01580"></a><span class="lineno"> 1580</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l01581"></a><span class="lineno"> 1581</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l01582"></a><span class="lineno"> 1582</span>&#160; * \param[in] kind the direction of copying</div>
<div class="line"><a name="l01583"></a><span class="lineno"> 1583</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l01584"></a><span class="lineno"> 1584</span>&#160; *</div>
<div class="line"><a name="l01585"></a><span class="lineno"> 1585</span>&#160; * \sa allocateSNN_GPU</div>
<div class="line"><a name="l01586"></a><span class="lineno"> 1586</span>&#160; * \since v4.0</div>
<div class="line"><a name="l01587"></a><span class="lineno"> 1587</span>&#160; */</div>
<div class="line"><a name="l01588"></a><span class="lineno"> 1588</span>&#160;void SNN::copyPreConnectionInfo(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l01589"></a><span class="lineno"> 1589</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l01590"></a><span class="lineno"> 1590</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, 0); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l01591"></a><span class="lineno"> 1591</span>&#160;</div>
<div class="line"><a name="l01592"></a><span class="lineno"> 1592</span>&#160;   int lengthN, lengthSyn, posN, posSyn;</div>
<div class="line"><a name="l01593"></a><span class="lineno"> 1593</span>&#160;</div>
<div class="line"><a name="l01594"></a><span class="lineno"> 1594</span>&#160;   if (lGrpId == ALL) {</div>
<div class="line"><a name="l01595"></a><span class="lineno"> 1595</span>&#160;       lengthN = networkConfigs[netId].numNAssigned;</div>
<div class="line"><a name="l01596"></a><span class="lineno"> 1596</span>&#160;       posN = 0;</div>
<div class="line"><a name="l01597"></a><span class="lineno"> 1597</span>&#160;   } else {</div>
<div class="line"><a name="l01598"></a><span class="lineno"> 1598</span>&#160;       lengthN = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l01599"></a><span class="lineno"> 1599</span>&#160;       posN = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l01600"></a><span class="lineno"> 1600</span>&#160;   }</div>
<div class="line"><a name="l01601"></a><span class="lineno"> 1601</span>&#160;</div>
<div class="line"><a name="l01602"></a><span class="lineno"> 1602</span>&#160;   // connection synaptic lengths and cumulative lengths...</div>
<div class="line"><a name="l01603"></a><span class="lineno"> 1603</span>&#160;   if(allocateMem) </div>
<div class="line"><a name="l01604"></a><span class="lineno"> 1604</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Npre, sizeof(short) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l01605"></a><span class="lineno"> 1605</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;Npre[posN], &amp;src-&gt;Npre[posN], sizeof(short) * lengthN, kind));</div>
<div class="line"><a name="l01606"></a><span class="lineno"> 1606</span>&#160;</div>
<div class="line"><a name="l01607"></a><span class="lineno"> 1607</span>&#160;   // we don&#39;t need these data structures if the network doesn&#39;t have any plastic synapses at all</div>
<div class="line"><a name="l01608"></a><span class="lineno"> 1608</span>&#160;   if (!sim_with_fixedwts) {</div>
<div class="line"><a name="l01609"></a><span class="lineno"> 1609</span>&#160;       // presyn excitatory connections</div>
<div class="line"><a name="l01610"></a><span class="lineno"> 1610</span>&#160;       if(allocateMem) </div>
<div class="line"><a name="l01611"></a><span class="lineno"> 1611</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Npre_plastic, sizeof(short) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l01612"></a><span class="lineno"> 1612</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;Npre_plastic[posN], &amp;src-&gt;Npre_plastic[posN], sizeof(short) * lengthN, kind));</div>
<div class="line"><a name="l01613"></a><span class="lineno"> 1613</span>&#160;</div>
<div class="line"><a name="l01614"></a><span class="lineno"> 1614</span>&#160;       // Npre_plasticInv is only used on GPUs, only allocate and copy it during initialization</div>
<div class="line"><a name="l01615"></a><span class="lineno"> 1615</span>&#160;       if(allocateMem) {</div>
<div class="line"><a name="l01616"></a><span class="lineno"> 1616</span>&#160;           float* Npre_plasticInv = new float[networkConfigs[netId].numNAssigned];</div>
<div class="line"><a name="l01617"></a><span class="lineno"> 1617</span>&#160;</div>
<div class="line"><a name="l01618"></a><span class="lineno"> 1618</span>&#160;           for (int i = 0; i &lt; networkConfigs[netId].numNAssigned; i++)</div>
<div class="line"><a name="l01619"></a><span class="lineno"> 1619</span>&#160;               Npre_plasticInv[i] = 1.0f / managerRuntimeData.Npre_plastic[i];</div>
<div class="line"><a name="l01620"></a><span class="lineno"> 1620</span>&#160;</div>
<div class="line"><a name="l01621"></a><span class="lineno"> 1621</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Npre_plasticInv, sizeof(float) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l01622"></a><span class="lineno"> 1622</span>&#160;           CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;Npre_plasticInv, Npre_plasticInv, sizeof(float) * networkConfigs[netId].numNAssigned, kind));</div>
<div class="line"><a name="l01623"></a><span class="lineno"> 1623</span>&#160;</div>
<div class="line"><a name="l01624"></a><span class="lineno"> 1624</span>&#160;           delete[] Npre_plasticInv;</div>
<div class="line"><a name="l01625"></a><span class="lineno"> 1625</span>&#160;       }</div>
<div class="line"><a name="l01626"></a><span class="lineno"> 1626</span>&#160;   }</div>
<div class="line"><a name="l01627"></a><span class="lineno"> 1627</span>&#160;       </div>
<div class="line"><a name="l01628"></a><span class="lineno"> 1628</span>&#160;   // beginning position for the pre-synaptic information</div>
<div class="line"><a name="l01629"></a><span class="lineno"> 1629</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l01630"></a><span class="lineno"> 1630</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;cumulativePre, sizeof(int) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l01631"></a><span class="lineno"> 1631</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;cumulativePre[posN], &amp;src-&gt;cumulativePre[posN], sizeof(int) * lengthN, kind));</div>
<div class="line"><a name="l01632"></a><span class="lineno"> 1632</span>&#160;</div>
<div class="line"><a name="l01633"></a><span class="lineno"> 1633</span>&#160;   // Npre, cumulativePre has been copied to destination</div>
<div class="line"><a name="l01634"></a><span class="lineno"> 1634</span>&#160;   if (lGrpId == ALL) {</div>
<div class="line"><a name="l01635"></a><span class="lineno"> 1635</span>&#160;       lengthSyn = networkConfigs[netId].numPreSynNet;</div>
<div class="line"><a name="l01636"></a><span class="lineno"> 1636</span>&#160;       posSyn = 0;</div>
<div class="line"><a name="l01637"></a><span class="lineno"> 1637</span>&#160;   } else {</div>
<div class="line"><a name="l01638"></a><span class="lineno"> 1638</span>&#160;       lengthSyn = 0;</div>
<div class="line"><a name="l01639"></a><span class="lineno"> 1639</span>&#160;       for (int lNId = groupConfigs[netId][lGrpId].lStartN; lNId &lt;= groupConfigs[netId][lGrpId].lEndN; lNId++)</div>
<div class="line"><a name="l01640"></a><span class="lineno"> 1640</span>&#160;           lengthSyn += dest-&gt;Npre[lNId];</div>
<div class="line"><a name="l01641"></a><span class="lineno"> 1641</span>&#160;</div>
<div class="line"><a name="l01642"></a><span class="lineno"> 1642</span>&#160;       posSyn = dest-&gt;cumulativePre[groupConfigs[netId][lGrpId].lStartN];</div>
<div class="line"><a name="l01643"></a><span class="lineno"> 1643</span>&#160;   }</div>
<div class="line"><a name="l01644"></a><span class="lineno"> 1644</span>&#160;</div>
<div class="line"><a name="l01645"></a><span class="lineno"> 1645</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l01646"></a><span class="lineno"> 1646</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;preSynapticIds, sizeof(SynInfo) * networkConfigs[netId].numPreSynNet));</div>
<div class="line"><a name="l01647"></a><span class="lineno"> 1647</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;preSynapticIds[posSyn], &amp;src-&gt;preSynapticIds[posSyn], sizeof(SynInfo) * lengthSyn, kind));</div>
<div class="line"><a name="l01648"></a><span class="lineno"> 1648</span>&#160;}</div>
<div class="line"><a name="l01649"></a><span class="lineno"> 1649</span>&#160;</div>
<div class="line"><a name="l01650"></a><span class="lineno"> 1650</span>&#160;/*!</div>
<div class="line"><a name="l01651"></a><span class="lineno"> 1651</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies information of post-connections to it</div>
<div class="line"><a name="l01652"></a><span class="lineno"> 1652</span>&#160; *</div>
<div class="line"><a name="l01653"></a><span class="lineno"> 1653</span>&#160; * This function:</div>
<div class="line"><a name="l01654"></a><span class="lineno"> 1654</span>&#160; * (allocate and) copy Npost, cumulativePost, postSynapticIds, postDelayInfo</div>
<div class="line"><a name="l01655"></a><span class="lineno"> 1655</span>&#160; *</div>
<div class="line"><a name="l01656"></a><span class="lineno"> 1656</span>&#160; *</div>
<div class="line"><a name="l01657"></a><span class="lineno"> 1657</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l01658"></a><span class="lineno"> 1658</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l01659"></a><span class="lineno"> 1659</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l01660"></a><span class="lineno"> 1660</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l01661"></a><span class="lineno"> 1661</span>&#160; * \param[in] kind the direction of copying</div>
<div class="line"><a name="l01662"></a><span class="lineno"> 1662</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l01663"></a><span class="lineno"> 1663</span>&#160; *</div>
<div class="line"><a name="l01664"></a><span class="lineno"> 1664</span>&#160; * \sa allocateSNN_GPU</div>
<div class="line"><a name="l01665"></a><span class="lineno"> 1665</span>&#160; * \since v4.0</div>
<div class="line"><a name="l01666"></a><span class="lineno"> 1666</span>&#160; */</div>
<div class="line"><a name="l01667"></a><span class="lineno"> 1667</span>&#160;void SNN::copyPostConnectionInfo(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l01668"></a><span class="lineno"> 1668</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l01669"></a><span class="lineno"> 1669</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, 0);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l01670"></a><span class="lineno"> 1670</span>&#160;</div>
<div class="line"><a name="l01671"></a><span class="lineno"> 1671</span>&#160;   int lengthN, lengthSyn, posN, posSyn;</div>
<div class="line"><a name="l01672"></a><span class="lineno"> 1672</span>&#160;</div>
<div class="line"><a name="l01673"></a><span class="lineno"> 1673</span>&#160;   if (lGrpId == ALL) {</div>
<div class="line"><a name="l01674"></a><span class="lineno"> 1674</span>&#160;       lengthN = networkConfigs[netId].numNAssigned;</div>
<div class="line"><a name="l01675"></a><span class="lineno"> 1675</span>&#160;       posN = 0;</div>
<div class="line"><a name="l01676"></a><span class="lineno"> 1676</span>&#160;   } else {</div>
<div class="line"><a name="l01677"></a><span class="lineno"> 1677</span>&#160;       lengthN = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l01678"></a><span class="lineno"> 1678</span>&#160;       posN = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l01679"></a><span class="lineno"> 1679</span>&#160;   }</div>
<div class="line"><a name="l01680"></a><span class="lineno"> 1680</span>&#160;</div>
<div class="line"><a name="l01681"></a><span class="lineno"> 1681</span>&#160;   // number of postsynaptic connections</div>
<div class="line"><a name="l01682"></a><span class="lineno"> 1682</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l01683"></a><span class="lineno"> 1683</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Npost, sizeof(short) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l01684"></a><span class="lineno"> 1684</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;Npost[posN], &amp;src-&gt;Npost[posN], sizeof(short) * lengthN, kind));</div>
<div class="line"><a name="l01685"></a><span class="lineno"> 1685</span>&#160;   </div>
<div class="line"><a name="l01686"></a><span class="lineno"> 1686</span>&#160;   // beginning position for the post-synaptic information</div>
<div class="line"><a name="l01687"></a><span class="lineno"> 1687</span>&#160;   if(allocateMem) </div>
<div class="line"><a name="l01688"></a><span class="lineno"> 1688</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;cumulativePost, sizeof(int) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l01689"></a><span class="lineno"> 1689</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;cumulativePost[posN], &amp;src-&gt;cumulativePost[posN], sizeof(int) * lengthN, kind));</div>
<div class="line"><a name="l01690"></a><span class="lineno"> 1690</span>&#160;</div>
<div class="line"><a name="l01691"></a><span class="lineno"> 1691</span>&#160;   </div>
<div class="line"><a name="l01692"></a><span class="lineno"> 1692</span>&#160;   // Npost, cumulativePost has been copied to destination</div>
<div class="line"><a name="l01693"></a><span class="lineno"> 1693</span>&#160;   if (lGrpId == ALL) {</div>
<div class="line"><a name="l01694"></a><span class="lineno"> 1694</span>&#160;       lengthSyn = networkConfigs[netId].numPostSynNet;</div>
<div class="line"><a name="l01695"></a><span class="lineno"> 1695</span>&#160;       posSyn = 0;</div>
<div class="line"><a name="l01696"></a><span class="lineno"> 1696</span>&#160;   } else {</div>
<div class="line"><a name="l01697"></a><span class="lineno"> 1697</span>&#160;       lengthSyn = 0;</div>
<div class="line"><a name="l01698"></a><span class="lineno"> 1698</span>&#160;       for (int lNId = groupConfigs[netId][lGrpId].lStartN; lNId &lt;= groupConfigs[netId][lGrpId].lEndN; lNId++)</div>
<div class="line"><a name="l01699"></a><span class="lineno"> 1699</span>&#160;           lengthSyn += dest-&gt;Npost[lNId];</div>
<div class="line"><a name="l01700"></a><span class="lineno"> 1700</span>&#160;</div>
<div class="line"><a name="l01701"></a><span class="lineno"> 1701</span>&#160;       posSyn = dest-&gt;cumulativePost[groupConfigs[netId][lGrpId].lStartN];</div>
<div class="line"><a name="l01702"></a><span class="lineno"> 1702</span>&#160;   }</div>
<div class="line"><a name="l01703"></a><span class="lineno"> 1703</span>&#160;</div>
<div class="line"><a name="l01704"></a><span class="lineno"> 1704</span>&#160;   // actual post synaptic connection information...</div>
<div class="line"><a name="l01705"></a><span class="lineno"> 1705</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l01706"></a><span class="lineno"> 1706</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;postSynapticIds, sizeof(SynInfo) * networkConfigs[netId].numPostSynNet));</div>
<div class="line"><a name="l01707"></a><span class="lineno"> 1707</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;postSynapticIds[posSyn], &amp;src-&gt;postSynapticIds[posSyn], sizeof(SynInfo) * lengthSyn, kind));</div>
<div class="line"><a name="l01708"></a><span class="lineno"> 1708</span>&#160;</div>
<div class="line"><a name="l01709"></a><span class="lineno"> 1709</span>&#160;   // static specific mapping and actual post-synaptic delay metric</div>
<div class="line"><a name="l01710"></a><span class="lineno"> 1710</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l01711"></a><span class="lineno"> 1711</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;postDelayInfo, sizeof(DelayInfo) * networkConfigs[netId].numNAssigned * (glbNetworkConfig.maxDelay + 1)));</div>
<div class="line"><a name="l01712"></a><span class="lineno"> 1712</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;postDelayInfo[posN * (glbNetworkConfig.maxDelay + 1)], &amp;src-&gt;postDelayInfo[posN * (glbNetworkConfig.maxDelay + 1)], sizeof(DelayInfo) * lengthN * (glbNetworkConfig.maxDelay + 1), kind));</div>
<div class="line"><a name="l01713"></a><span class="lineno"> 1713</span>&#160;}</div>
<div class="line"><a name="l01714"></a><span class="lineno"> 1714</span>&#160;</div>
<div class="line"><a name="l01715"></a><span class="lineno"> 1715</span>&#160;void SNN::checkDestSrcPtrs(RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem, int lGrpId, int destOffset) {</div>
<div class="line"><a name="l01716"></a><span class="lineno"> 1716</span>&#160;   // source should always be allocated</div>
<div class="line"><a name="l01717"></a><span class="lineno"> 1717</span>&#160;   assert(src-&gt;allocated);</div>
<div class="line"><a name="l01718"></a><span class="lineno"> 1718</span>&#160;</div>
<div class="line"><a name="l01719"></a><span class="lineno"> 1719</span>&#160;   if(kind == cudaMemcpyHostToDevice) {</div>
<div class="line"><a name="l01720"></a><span class="lineno"> 1720</span>&#160;       assert(src-&gt;memType  == CPU_MEM);</div>
<div class="line"><a name="l01721"></a><span class="lineno"> 1721</span>&#160;       assert(dest-&gt;memType == GPU_MEM);</div>
<div class="line"><a name="l01722"></a><span class="lineno"> 1722</span>&#160;</div>
<div class="line"><a name="l01723"></a><span class="lineno"> 1723</span>&#160;       if (allocateMem) {</div>
<div class="line"><a name="l01724"></a><span class="lineno"> 1724</span>&#160;           assert(!dest-&gt;allocated); // if allocateMem = true, then the destination must be empty without allocation.</div>
<div class="line"><a name="l01725"></a><span class="lineno"> 1725</span>&#160;           assert(lGrpId == ALL); // if allocateMem = true, then we should not specify any specific group.</div>
<div class="line"><a name="l01726"></a><span class="lineno"> 1726</span>&#160;       } else {</div>
<div class="line"><a name="l01727"></a><span class="lineno"> 1727</span>&#160;           assert(dest-&gt;allocated); // if allocateMem = false, then the destination must be allocated.</div>
<div class="line"><a name="l01728"></a><span class="lineno"> 1728</span>&#160;       }</div>
<div class="line"><a name="l01729"></a><span class="lineno"> 1729</span>&#160;</div>
<div class="line"><a name="l01730"></a><span class="lineno"> 1730</span>&#160;       assert(destOffset == 0); // H-to-D only allows local-to-local copy</div>
<div class="line"><a name="l01731"></a><span class="lineno"> 1731</span>&#160;   } else if (kind == cudaMemcpyDeviceToHost) {</div>
<div class="line"><a name="l01732"></a><span class="lineno"> 1732</span>&#160;       assert(src-&gt;memType  == GPU_MEM);</div>
<div class="line"><a name="l01733"></a><span class="lineno"> 1733</span>&#160;       assert(dest-&gt;memType == CPU_MEM);</div>
<div class="line"><a name="l01734"></a><span class="lineno"> 1734</span>&#160;</div>
<div class="line"><a name="l01735"></a><span class="lineno"> 1735</span>&#160;       assert(dest-&gt;allocated);</div>
<div class="line"><a name="l01736"></a><span class="lineno"> 1736</span>&#160;</div>
<div class="line"><a name="l01737"></a><span class="lineno"> 1737</span>&#160;       if (lGrpId == ALL)</div>
<div class="line"><a name="l01738"></a><span class="lineno"> 1738</span>&#160;           assert(destOffset == 0); // if copy all content, only local-to-local is allowed</div>
<div class="line"><a name="l01739"></a><span class="lineno"> 1739</span>&#160;   } else {</div>
<div class="line"><a name="l01740"></a><span class="lineno"> 1740</span>&#160;       KERNEL_ERROR(&quot;Wrong Host-Device copy direction&quot;);</div>
<div class="line"><a name="l01741"></a><span class="lineno"> 1741</span>&#160;       exitSimulation(1);</div>
<div class="line"><a name="l01742"></a><span class="lineno"> 1742</span>&#160;   }</div>
<div class="line"><a name="l01743"></a><span class="lineno"> 1743</span>&#160;}</div>
<div class="line"><a name="l01744"></a><span class="lineno"> 1744</span>&#160;</div>
<div class="line"><a name="l01745"></a><span class="lineno"> 1745</span>&#160;/*!</div>
<div class="line"><a name="l01746"></a><span class="lineno"> 1746</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies AMPA conductance to it</div>
<div class="line"><a name="l01747"></a><span class="lineno"> 1747</span>&#160; *</div>
<div class="line"><a name="l01748"></a><span class="lineno"> 1748</span>&#160; * This function:</div>
<div class="line"><a name="l01749"></a><span class="lineno"> 1749</span>&#160; * (allocate and) copy gAMPA</div>
<div class="line"><a name="l01750"></a><span class="lineno"> 1750</span>&#160; *</div>
<div class="line"><a name="l01751"></a><span class="lineno"> 1751</span>&#160; * This funcion is called by copyNeuronState() and fetchConductanceAMPA(). It supports bi-directional copying</div>
<div class="line"><a name="l01752"></a><span class="lineno"> 1752</span>&#160; *</div>
<div class="line"><a name="l01753"></a><span class="lineno"> 1753</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l01754"></a><span class="lineno"> 1754</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l01755"></a><span class="lineno"> 1755</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l01756"></a><span class="lineno"> 1756</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l01757"></a><span class="lineno"> 1757</span>&#160; * \param[in] kind the direction of copy</div>
<div class="line"><a name="l01758"></a><span class="lineno"> 1758</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copy</div>
<div class="line"><a name="l01759"></a><span class="lineno"> 1759</span>&#160; * \param[in] destOffset the offset of data destination, which is used in local-to-global copy </div>
<div class="line"><a name="l01760"></a><span class="lineno"> 1760</span>&#160; *</div>
<div class="line"><a name="l01761"></a><span class="lineno"> 1761</span>&#160; * \sa copyNeuronState fetchConductanceAMPA</div>
<div class="line"><a name="l01762"></a><span class="lineno"> 1762</span>&#160; * \since v3.0</div>
<div class="line"><a name="l01763"></a><span class="lineno"> 1763</span>&#160; */</div>
<div class="line"><a name="l01764"></a><span class="lineno"> 1764</span>&#160;void SNN::copyConductanceAMPA(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem, int destOffset) {</div>
<div class="line"><a name="l01765"></a><span class="lineno"> 1765</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l01766"></a><span class="lineno"> 1766</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, destOffset);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l01767"></a><span class="lineno"> 1767</span>&#160;   </div>
<div class="line"><a name="l01768"></a><span class="lineno"> 1768</span>&#160;   assert(isSimulationWithCOBA());</div>
<div class="line"><a name="l01769"></a><span class="lineno"> 1769</span>&#160;</div>
<div class="line"><a name="l01770"></a><span class="lineno"> 1770</span>&#160;   int ptrPos, length;</div>
<div class="line"><a name="l01771"></a><span class="lineno"> 1771</span>&#160;</div>
<div class="line"><a name="l01772"></a><span class="lineno"> 1772</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l01773"></a><span class="lineno"> 1773</span>&#160;       ptrPos = 0;</div>
<div class="line"><a name="l01774"></a><span class="lineno"> 1774</span>&#160;       length = networkConfigs[netId].numNReg;</div>
<div class="line"><a name="l01775"></a><span class="lineno"> 1775</span>&#160;   } else {</div>
<div class="line"><a name="l01776"></a><span class="lineno"> 1776</span>&#160;       ptrPos = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l01777"></a><span class="lineno"> 1777</span>&#160;       length = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l01778"></a><span class="lineno"> 1778</span>&#160;   }</div>
<div class="line"><a name="l01779"></a><span class="lineno"> 1779</span>&#160;   assert(length &lt;= networkConfigs[netId].numNReg);</div>
<div class="line"><a name="l01780"></a><span class="lineno"> 1780</span>&#160;   assert(length &gt; 0);</div>
<div class="line"><a name="l01781"></a><span class="lineno"> 1781</span>&#160;</div>
<div class="line"><a name="l01782"></a><span class="lineno"> 1782</span>&#160;   //conductance information</div>
<div class="line"><a name="l01783"></a><span class="lineno"> 1783</span>&#160;   assert(src-&gt;gAMPA  != NULL);</div>
<div class="line"><a name="l01784"></a><span class="lineno"> 1784</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;gAMPA, sizeof(float) * length));</div>
<div class="line"><a name="l01785"></a><span class="lineno"> 1785</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gAMPA[ptrPos + destOffset], &amp;src-&gt;gAMPA[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01786"></a><span class="lineno"> 1786</span>&#160;}</div>
<div class="line"><a name="l01787"></a><span class="lineno"> 1787</span>&#160;</div>
<div class="line"><a name="l01788"></a><span class="lineno"> 1788</span>&#160;/*!</div>
<div class="line"><a name="l01789"></a><span class="lineno"> 1789</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies NMDA conductance to it</div>
<div class="line"><a name="l01790"></a><span class="lineno"> 1790</span>&#160; *</div>
<div class="line"><a name="l01791"></a><span class="lineno"> 1791</span>&#160; * This function:</div>
<div class="line"><a name="l01792"></a><span class="lineno"> 1792</span>&#160; * (allocate and) copy gNMDA, gNMDA_r, gNMDA_d</div>
<div class="line"><a name="l01793"></a><span class="lineno"> 1793</span>&#160; *</div>
<div class="line"><a name="l01794"></a><span class="lineno"> 1794</span>&#160; * This funcion is called by copyNeuronState() and fetchConductanceNMDA(). It supports bi-directional copying</div>
<div class="line"><a name="l01795"></a><span class="lineno"> 1795</span>&#160; *</div>
<div class="line"><a name="l01796"></a><span class="lineno"> 1796</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l01797"></a><span class="lineno"> 1797</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l01798"></a><span class="lineno"> 1798</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l01799"></a><span class="lineno"> 1799</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l01800"></a><span class="lineno"> 1800</span>&#160; * \param[in] kind the direction of copy</div>
<div class="line"><a name="l01801"></a><span class="lineno"> 1801</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copy</div>
<div class="line"><a name="l01802"></a><span class="lineno"> 1802</span>&#160; * \param[in] destOffset the offset of data destination, which is used in local-to-global copy </div>
<div class="line"><a name="l01803"></a><span class="lineno"> 1803</span>&#160; *</div>
<div class="line"><a name="l01804"></a><span class="lineno"> 1804</span>&#160; * \sa copyNeuronState fetchConductanceNMDA</div>
<div class="line"><a name="l01805"></a><span class="lineno"> 1805</span>&#160; * \since v3.0</div>
<div class="line"><a name="l01806"></a><span class="lineno"> 1806</span>&#160; */</div>
<div class="line"><a name="l01807"></a><span class="lineno"> 1807</span>&#160;void SNN::copyConductanceNMDA(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem, int destOffset) {</div>
<div class="line"><a name="l01808"></a><span class="lineno"> 1808</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l01809"></a><span class="lineno"> 1809</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, destOffset);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l01810"></a><span class="lineno"> 1810</span>&#160;   assert(isSimulationWithCOBA());</div>
<div class="line"><a name="l01811"></a><span class="lineno"> 1811</span>&#160;</div>
<div class="line"><a name="l01812"></a><span class="lineno"> 1812</span>&#160;   int ptrPos, length;</div>
<div class="line"><a name="l01813"></a><span class="lineno"> 1813</span>&#160;</div>
<div class="line"><a name="l01814"></a><span class="lineno"> 1814</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l01815"></a><span class="lineno"> 1815</span>&#160;       ptrPos  = 0;</div>
<div class="line"><a name="l01816"></a><span class="lineno"> 1816</span>&#160;       length  = networkConfigs[netId].numNReg;</div>
<div class="line"><a name="l01817"></a><span class="lineno"> 1817</span>&#160;   } else {</div>
<div class="line"><a name="l01818"></a><span class="lineno"> 1818</span>&#160;       ptrPos  = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l01819"></a><span class="lineno"> 1819</span>&#160;       length  = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l01820"></a><span class="lineno"> 1820</span>&#160;   }</div>
<div class="line"><a name="l01821"></a><span class="lineno"> 1821</span>&#160;   assert(length  &lt;= networkConfigs[netId].numNReg);</div>
<div class="line"><a name="l01822"></a><span class="lineno"> 1822</span>&#160;   assert(length &gt; 0);</div>
<div class="line"><a name="l01823"></a><span class="lineno"> 1823</span>&#160;</div>
<div class="line"><a name="l01824"></a><span class="lineno"> 1824</span>&#160;   if (isSimulationWithNMDARise()) {</div>
<div class="line"><a name="l01825"></a><span class="lineno"> 1825</span>&#160;       assert(src-&gt;gNMDA_r != NULL);</div>
<div class="line"><a name="l01826"></a><span class="lineno"> 1826</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;gNMDA_r, sizeof(float) * length));</div>
<div class="line"><a name="l01827"></a><span class="lineno"> 1827</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gNMDA_r[ptrPos], &amp;src-&gt;gNMDA_r[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01828"></a><span class="lineno"> 1828</span>&#160;</div>
<div class="line"><a name="l01829"></a><span class="lineno"> 1829</span>&#160;       assert(src-&gt;gNMDA_d != NULL);</div>
<div class="line"><a name="l01830"></a><span class="lineno"> 1830</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;gNMDA_d, sizeof(float) * length));</div>
<div class="line"><a name="l01831"></a><span class="lineno"> 1831</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gNMDA_d[ptrPos], &amp;src-&gt;gNMDA_d[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01832"></a><span class="lineno"> 1832</span>&#160;   } else {</div>
<div class="line"><a name="l01833"></a><span class="lineno"> 1833</span>&#160;       assert(src-&gt;gNMDA != NULL);</div>
<div class="line"><a name="l01834"></a><span class="lineno"> 1834</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;gNMDA, sizeof(float) * length));</div>
<div class="line"><a name="l01835"></a><span class="lineno"> 1835</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gNMDA[ptrPos + destOffset], &amp;src-&gt;gNMDA[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01836"></a><span class="lineno"> 1836</span>&#160;   }</div>
<div class="line"><a name="l01837"></a><span class="lineno"> 1837</span>&#160;}</div>
<div class="line"><a name="l01838"></a><span class="lineno"> 1838</span>&#160;</div>
<div class="line"><a name="l01839"></a><span class="lineno"> 1839</span>&#160;/*!</div>
<div class="line"><a name="l01840"></a><span class="lineno"> 1840</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies GABAa conductance to it</div>
<div class="line"><a name="l01841"></a><span class="lineno"> 1841</span>&#160; *</div>
<div class="line"><a name="l01842"></a><span class="lineno"> 1842</span>&#160; * This function:</div>
<div class="line"><a name="l01843"></a><span class="lineno"> 1843</span>&#160; * (allocate and) copy gGABAa</div>
<div class="line"><a name="l01844"></a><span class="lineno"> 1844</span>&#160; *</div>
<div class="line"><a name="l01845"></a><span class="lineno"> 1845</span>&#160; * This funcion is called by copyNeuronState() and fetchConductanceGABAa(). It supports bi-directional copying</div>
<div class="line"><a name="l01846"></a><span class="lineno"> 1846</span>&#160; *</div>
<div class="line"><a name="l01847"></a><span class="lineno"> 1847</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l01848"></a><span class="lineno"> 1848</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l01849"></a><span class="lineno"> 1849</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l01850"></a><span class="lineno"> 1850</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l01851"></a><span class="lineno"> 1851</span>&#160; * \param[in] kind the direction of copy</div>
<div class="line"><a name="l01852"></a><span class="lineno"> 1852</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copy</div>
<div class="line"><a name="l01853"></a><span class="lineno"> 1853</span>&#160; * \param[in] destOffset the offset of data destination, which is used in local-to-global copy </div>
<div class="line"><a name="l01854"></a><span class="lineno"> 1854</span>&#160; *</div>
<div class="line"><a name="l01855"></a><span class="lineno"> 1855</span>&#160; * \sa copyNeuronState fetchConductanceGABAa</div>
<div class="line"><a name="l01856"></a><span class="lineno"> 1856</span>&#160; * \since v3.0</div>
<div class="line"><a name="l01857"></a><span class="lineno"> 1857</span>&#160; */</div>
<div class="line"><a name="l01858"></a><span class="lineno"> 1858</span>&#160;void SNN::copyConductanceGABAa(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem, int destOffset) {</div>
<div class="line"><a name="l01859"></a><span class="lineno"> 1859</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l01860"></a><span class="lineno"> 1860</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, destOffset); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l01861"></a><span class="lineno"> 1861</span>&#160;   assert(isSimulationWithCOBA());</div>
<div class="line"><a name="l01862"></a><span class="lineno"> 1862</span>&#160;</div>
<div class="line"><a name="l01863"></a><span class="lineno"> 1863</span>&#160;   int ptrPos, length;</div>
<div class="line"><a name="l01864"></a><span class="lineno"> 1864</span>&#160;</div>
<div class="line"><a name="l01865"></a><span class="lineno"> 1865</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l01866"></a><span class="lineno"> 1866</span>&#160;       ptrPos  = 0;</div>
<div class="line"><a name="l01867"></a><span class="lineno"> 1867</span>&#160;       length  = networkConfigs[netId].numNReg;</div>
<div class="line"><a name="l01868"></a><span class="lineno"> 1868</span>&#160;   } else {</div>
<div class="line"><a name="l01869"></a><span class="lineno"> 1869</span>&#160;       ptrPos  = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l01870"></a><span class="lineno"> 1870</span>&#160;       length  = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l01871"></a><span class="lineno"> 1871</span>&#160;   }</div>
<div class="line"><a name="l01872"></a><span class="lineno"> 1872</span>&#160;   assert(length  &lt;= networkConfigs[netId].numNReg);</div>
<div class="line"><a name="l01873"></a><span class="lineno"> 1873</span>&#160;   assert(length &gt; 0);</div>
<div class="line"><a name="l01874"></a><span class="lineno"> 1874</span>&#160;</div>
<div class="line"><a name="l01875"></a><span class="lineno"> 1875</span>&#160;   assert(src-&gt;gGABAa != NULL);</div>
<div class="line"><a name="l01876"></a><span class="lineno"> 1876</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;gGABAa, sizeof(float) * length));</div>
<div class="line"><a name="l01877"></a><span class="lineno"> 1877</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gGABAa[ptrPos + destOffset], &amp;src-&gt;gGABAa[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01878"></a><span class="lineno"> 1878</span>&#160;}</div>
<div class="line"><a name="l01879"></a><span class="lineno"> 1879</span>&#160;</div>
<div class="line"><a name="l01880"></a><span class="lineno"> 1880</span>&#160;/*!</div>
<div class="line"><a name="l01881"></a><span class="lineno"> 1881</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies GABAb conductance to it</div>
<div class="line"><a name="l01882"></a><span class="lineno"> 1882</span>&#160; *</div>
<div class="line"><a name="l01883"></a><span class="lineno"> 1883</span>&#160; * This function:</div>
<div class="line"><a name="l01884"></a><span class="lineno"> 1884</span>&#160; * (allocate and) copy gGABAb, gGABAb_r, gGABAb_d</div>
<div class="line"><a name="l01885"></a><span class="lineno"> 1885</span>&#160; *</div>
<div class="line"><a name="l01886"></a><span class="lineno"> 1886</span>&#160; * This funcion is called by copyNeuronState() and fetchConductanceGABAb(). It supports bi-directional copying</div>
<div class="line"><a name="l01887"></a><span class="lineno"> 1887</span>&#160; *</div>
<div class="line"><a name="l01888"></a><span class="lineno"> 1888</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l01889"></a><span class="lineno"> 1889</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l01890"></a><span class="lineno"> 1890</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l01891"></a><span class="lineno"> 1891</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l01892"></a><span class="lineno"> 1892</span>&#160; * \param[in] kind the direction of copy</div>
<div class="line"><a name="l01893"></a><span class="lineno"> 1893</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copy</div>
<div class="line"><a name="l01894"></a><span class="lineno"> 1894</span>&#160; * \param[in] destOffset the offset of data destination, which is used in local-to-global copy </div>
<div class="line"><a name="l01895"></a><span class="lineno"> 1895</span>&#160; *</div>
<div class="line"><a name="l01896"></a><span class="lineno"> 1896</span>&#160; * \sa copyNeuronState fetchConductanceGABAb</div>
<div class="line"><a name="l01897"></a><span class="lineno"> 1897</span>&#160; * \since v3.0</div>
<div class="line"><a name="l01898"></a><span class="lineno"> 1898</span>&#160; */</div>
<div class="line"><a name="l01899"></a><span class="lineno"> 1899</span>&#160;void SNN::copyConductanceGABAb(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem, int destOffset) {</div>
<div class="line"><a name="l01900"></a><span class="lineno"> 1900</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l01901"></a><span class="lineno"> 1901</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, destOffset); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l01902"></a><span class="lineno"> 1902</span>&#160;   assert(isSimulationWithCOBA());</div>
<div class="line"><a name="l01903"></a><span class="lineno"> 1903</span>&#160;</div>
<div class="line"><a name="l01904"></a><span class="lineno"> 1904</span>&#160;   int ptrPos, length;</div>
<div class="line"><a name="l01905"></a><span class="lineno"> 1905</span>&#160;</div>
<div class="line"><a name="l01906"></a><span class="lineno"> 1906</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l01907"></a><span class="lineno"> 1907</span>&#160;       ptrPos  = 0;</div>
<div class="line"><a name="l01908"></a><span class="lineno"> 1908</span>&#160;       length  = networkConfigs[netId].numNReg;</div>
<div class="line"><a name="l01909"></a><span class="lineno"> 1909</span>&#160;   } else {</div>
<div class="line"><a name="l01910"></a><span class="lineno"> 1910</span>&#160;       ptrPos  = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l01911"></a><span class="lineno"> 1911</span>&#160;       length  = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l01912"></a><span class="lineno"> 1912</span>&#160;   }</div>
<div class="line"><a name="l01913"></a><span class="lineno"> 1913</span>&#160;   assert(length &lt;= networkConfigs[netId].numNReg);</div>
<div class="line"><a name="l01914"></a><span class="lineno"> 1914</span>&#160;   assert(length &gt; 0);</div>
<div class="line"><a name="l01915"></a><span class="lineno"> 1915</span>&#160;</div>
<div class="line"><a name="l01916"></a><span class="lineno"> 1916</span>&#160;   if (isSimulationWithGABAbRise()) {</div>
<div class="line"><a name="l01917"></a><span class="lineno"> 1917</span>&#160;       assert(src-&gt;gGABAb_r != NULL);</div>
<div class="line"><a name="l01918"></a><span class="lineno"> 1918</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;gGABAb_r, sizeof(float) * length));</div>
<div class="line"><a name="l01919"></a><span class="lineno"> 1919</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gGABAb_r[ptrPos], &amp;src-&gt;gGABAb_r[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01920"></a><span class="lineno"> 1920</span>&#160;</div>
<div class="line"><a name="l01921"></a><span class="lineno"> 1921</span>&#160;       assert(src-&gt;gGABAb_d != NULL);</div>
<div class="line"><a name="l01922"></a><span class="lineno"> 1922</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;gGABAb_d, sizeof(float) * length));</div>
<div class="line"><a name="l01923"></a><span class="lineno"> 1923</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gGABAb_d[ptrPos], &amp;src-&gt;gGABAb_d[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01924"></a><span class="lineno"> 1924</span>&#160;   } else {</div>
<div class="line"><a name="l01925"></a><span class="lineno"> 1925</span>&#160;       assert(src-&gt;gGABAb != NULL);</div>
<div class="line"><a name="l01926"></a><span class="lineno"> 1926</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;gGABAb, sizeof(float) * length));</div>
<div class="line"><a name="l01927"></a><span class="lineno"> 1927</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;gGABAb[ptrPos + destOffset], &amp;src-&gt;gGABAb[ptrPos], sizeof(float) * length, kind));</div>
<div class="line"><a name="l01928"></a><span class="lineno"> 1928</span>&#160;   }</div>
<div class="line"><a name="l01929"></a><span class="lineno"> 1929</span>&#160;}</div>
<div class="line"><a name="l01930"></a><span class="lineno"> 1930</span>&#160;</div>
<div class="line"><a name="l01931"></a><span class="lineno"> 1931</span>&#160;/*!</div>
<div class="line"><a name="l01932"></a><span class="lineno"> 1932</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies variables related to nueron state to it</div>
<div class="line"><a name="l01933"></a><span class="lineno"> 1933</span>&#160; *</div>
<div class="line"><a name="l01934"></a><span class="lineno"> 1934</span>&#160; * This function:</div>
<div class="line"><a name="l01935"></a><span class="lineno"> 1935</span>&#160; * (allocate and) copy voltage, recovery, current, avgFiring </div>
<div class="line"><a name="l01936"></a><span class="lineno"> 1936</span>&#160; *</div>
<div class="line"><a name="l01937"></a><span class="lineno"> 1937</span>&#160; * This funcion is called by allocateSNN_GPU(). Only copying from host to device is required</div>
<div class="line"><a name="l01938"></a><span class="lineno"> 1938</span>&#160; *</div>
<div class="line"><a name="l01939"></a><span class="lineno"> 1939</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l01940"></a><span class="lineno"> 1940</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l01941"></a><span class="lineno"> 1941</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l01942"></a><span class="lineno"> 1942</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l01943"></a><span class="lineno"> 1943</span>&#160; *</div>
<div class="line"><a name="l01944"></a><span class="lineno"> 1944</span>&#160; * \sa allocateSNN_GPU fetchNeuronState</div>
<div class="line"><a name="l01945"></a><span class="lineno"> 1945</span>&#160; * \since v3.0</div>
<div class="line"><a name="l01946"></a><span class="lineno"> 1946</span>&#160; */</div>
<div class="line"><a name="l01947"></a><span class="lineno"> 1947</span>&#160;void SNN::copyNeuronState(int netId, int lGrpId, RuntimeData* dest, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l01948"></a><span class="lineno"> 1948</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l01949"></a><span class="lineno"> 1949</span>&#160;   checkDestSrcPtrs(dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, lGrpId, 0); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l01950"></a><span class="lineno"> 1950</span>&#160;   assert(kind == cudaMemcpyHostToDevice);</div>
<div class="line"><a name="l01951"></a><span class="lineno"> 1951</span>&#160;</div>
<div class="line"><a name="l01952"></a><span class="lineno"> 1952</span>&#160;   int ptrPos, length;</div>
<div class="line"><a name="l01953"></a><span class="lineno"> 1953</span>&#160;</div>
<div class="line"><a name="l01954"></a><span class="lineno"> 1954</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l01955"></a><span class="lineno"> 1955</span>&#160;       ptrPos  = 0;</div>
<div class="line"><a name="l01956"></a><span class="lineno"> 1956</span>&#160;       length  = networkConfigs[netId].numNReg;</div>
<div class="line"><a name="l01957"></a><span class="lineno"> 1957</span>&#160;   }</div>
<div class="line"><a name="l01958"></a><span class="lineno"> 1958</span>&#160;   else {</div>
<div class="line"><a name="l01959"></a><span class="lineno"> 1959</span>&#160;       ptrPos  = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l01960"></a><span class="lineno"> 1960</span>&#160;       length  = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l01961"></a><span class="lineno"> 1961</span>&#160;   }</div>
<div class="line"><a name="l01962"></a><span class="lineno"> 1962</span>&#160;</div>
<div class="line"><a name="l01963"></a><span class="lineno"> 1963</span>&#160;   assert(length &lt;= networkConfigs[netId].numNReg);</div>
<div class="line"><a name="l01964"></a><span class="lineno"> 1964</span>&#160;   </div>
<div class="line"><a name="l01965"></a><span class="lineno"> 1965</span>&#160;   if (length == 0)</div>
<div class="line"><a name="l01966"></a><span class="lineno"> 1966</span>&#160;       return;</div>
<div class="line"><a name="l01967"></a><span class="lineno"> 1967</span>&#160;</div>
<div class="line"><a name="l01968"></a><span class="lineno"> 1968</span>&#160;   if(!allocateMem &amp;&amp; groupConfigs[netId][lGrpId].Type &amp; POISSON_NEURON)</div>
<div class="line"><a name="l01969"></a><span class="lineno"> 1969</span>&#160;       return;</div>
<div class="line"><a name="l01970"></a><span class="lineno"> 1970</span>&#160;</div>
<div class="line"><a name="l01971"></a><span class="lineno"> 1971</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;recovery, sizeof(float) * length));</div>
<div class="line"><a name="l01972"></a><span class="lineno"> 1972</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;recovery[ptrPos], &amp;managerRuntimeData.recovery[ptrPos], sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l01973"></a><span class="lineno"> 1973</span>&#160;</div>
<div class="line"><a name="l01974"></a><span class="lineno"> 1974</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;voltage, sizeof(float) * length));</div>
<div class="line"><a name="l01975"></a><span class="lineno"> 1975</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;voltage[ptrPos], &amp;managerRuntimeData.voltage[ptrPos], sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l01976"></a><span class="lineno"> 1976</span>&#160;</div>
<div class="line"><a name="l01977"></a><span class="lineno"> 1977</span>&#160;   //neuron input current...</div>
<div class="line"><a name="l01978"></a><span class="lineno"> 1978</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;current, sizeof(float) * length));</div>
<div class="line"><a name="l01979"></a><span class="lineno"> 1979</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;current[ptrPos], &amp;managerRuntimeData.current[ptrPos], sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l01980"></a><span class="lineno"> 1980</span>&#160;</div>
<div class="line"><a name="l01981"></a><span class="lineno"> 1981</span>&#160;   if (sim_with_conductances) {</div>
<div class="line"><a name="l01982"></a><span class="lineno"> 1982</span>&#160;       //conductance information</div>
<div class="line"><a name="l01983"></a><span class="lineno"> 1983</span>&#160;       copyConductanceAMPA(netId, lGrpId, dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, 0);</div>
<div class="line"><a name="l01984"></a><span class="lineno"> 1984</span>&#160;       copyConductanceNMDA(netId, lGrpId, dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, 0);</div>
<div class="line"><a name="l01985"></a><span class="lineno"> 1985</span>&#160;       copyConductanceGABAa(netId, lGrpId, dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, 0);</div>
<div class="line"><a name="l01986"></a><span class="lineno"> 1986</span>&#160;       copyConductanceGABAb(netId, lGrpId, dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, 0);</div>
<div class="line"><a name="l01987"></a><span class="lineno"> 1987</span>&#160;   }</div>
<div class="line"><a name="l01988"></a><span class="lineno"> 1988</span>&#160;</div>
<div class="line"><a name="l01989"></a><span class="lineno"> 1989</span>&#160;   // copying external current needs to be done separately because setExternalCurrent needs to call it, too</div>
<div class="line"><a name="l01990"></a><span class="lineno"> 1990</span>&#160;   // do it only from host to device</div>
<div class="line"><a name="l01991"></a><span class="lineno"> 1991</span>&#160;   copyExternalCurrent(netId, lGrpId, dest, cudaMemcpyHostToDevice, allocateMem);</div>
<div class="line"><a name="l01992"></a><span class="lineno"> 1992</span>&#160;   </div>
<div class="line"><a name="l01993"></a><span class="lineno"> 1993</span>&#160;   copyNeuronParameters(netId, lGrpId, dest, cudaMemcpyHostToDevice, allocateMem);</div>
<div class="line"><a name="l01994"></a><span class="lineno"> 1994</span>&#160;</div>
<div class="line"><a name="l01995"></a><span class="lineno"> 1995</span>&#160;   if (sim_with_homeostasis) {</div>
<div class="line"><a name="l01996"></a><span class="lineno"> 1996</span>&#160;       //Included to enable homeostasis in GPU_MODE.</div>
<div class="line"><a name="l01997"></a><span class="lineno"> 1997</span>&#160;       // Avg. Firing...</div>
<div class="line"><a name="l01998"></a><span class="lineno"> 1998</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;avgFiring, sizeof(float) * length));</div>
<div class="line"><a name="l01999"></a><span class="lineno"> 1999</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;avgFiring[ptrPos], &amp;managerRuntimeData.avgFiring[ptrPos], sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02000"></a><span class="lineno"> 2000</span>&#160;   }</div>
<div class="line"><a name="l02001"></a><span class="lineno"> 2001</span>&#160;}</div>
<div class="line"><a name="l02002"></a><span class="lineno"> 2002</span>&#160;</div>
<div class="line"><a name="l02003"></a><span class="lineno"> 2003</span>&#160;/*!</div>
<div class="line"><a name="l02004"></a><span class="lineno"> 2004</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies the spike count of each neuron to it</div>
<div class="line"><a name="l02005"></a><span class="lineno"> 2005</span>&#160; *</div>
<div class="line"><a name="l02006"></a><span class="lineno"> 2006</span>&#160; * This function:</div>
<div class="line"><a name="l02007"></a><span class="lineno"> 2007</span>&#160; * (allocate and) copy nSpikeCnt</div>
<div class="line"><a name="l02008"></a><span class="lineno"> 2008</span>&#160; *</div>
<div class="line"><a name="l02009"></a><span class="lineno"> 2009</span>&#160; * This funcion is called by copyAuxiliaryData() and fetchNeuronSpikeCount(). It supports bi-directional copying</div>
<div class="line"><a name="l02010"></a><span class="lineno"> 2010</span>&#160; *</div>
<div class="line"><a name="l02011"></a><span class="lineno"> 2011</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l02012"></a><span class="lineno"> 2012</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l02013"></a><span class="lineno"> 2013</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l02014"></a><span class="lineno"> 2014</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l02015"></a><span class="lineno"> 2015</span>&#160; * \param[in] kind the direction of copy</div>
<div class="line"><a name="l02016"></a><span class="lineno"> 2016</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copy</div>
<div class="line"><a name="l02017"></a><span class="lineno"> 2017</span>&#160; * \param[in] destOffset the offset of data destination, which is used in local-to-global copy </div>
<div class="line"><a name="l02018"></a><span class="lineno"> 2018</span>&#160; *</div>
<div class="line"><a name="l02019"></a><span class="lineno"> 2019</span>&#160; * \sa copyAuxiliaryData fetchNeuronSpikeCount</div>
<div class="line"><a name="l02020"></a><span class="lineno"> 2020</span>&#160; * \since v4.0</div>
<div class="line"><a name="l02021"></a><span class="lineno"> 2021</span>&#160; */</div>
<div class="line"><a name="l02022"></a><span class="lineno"> 2022</span>&#160;void SNN::copyNeuronSpikeCount(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem, int destOffset) {</div>
<div class="line"><a name="l02023"></a><span class="lineno"> 2023</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02024"></a><span class="lineno"> 2024</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, destOffset);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02025"></a><span class="lineno"> 2025</span>&#160;</div>
<div class="line"><a name="l02026"></a><span class="lineno"> 2026</span>&#160;   int posN, lengthN;</div>
<div class="line"><a name="l02027"></a><span class="lineno"> 2027</span>&#160;</div>
<div class="line"><a name="l02028"></a><span class="lineno"> 2028</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l02029"></a><span class="lineno"> 2029</span>&#160;       posN = 0;</div>
<div class="line"><a name="l02030"></a><span class="lineno"> 2030</span>&#160;       lengthN = networkConfigs[netId].numN;</div>
<div class="line"><a name="l02031"></a><span class="lineno"> 2031</span>&#160;   } else {</div>
<div class="line"><a name="l02032"></a><span class="lineno"> 2032</span>&#160;       posN = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l02033"></a><span class="lineno"> 2033</span>&#160;       lengthN = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l02034"></a><span class="lineno"> 2034</span>&#160;   }</div>
<div class="line"><a name="l02035"></a><span class="lineno"> 2035</span>&#160;   assert(lengthN &gt; 0 &amp;&amp; lengthN &lt;= networkConfigs[netId].numN);</div>
<div class="line"><a name="l02036"></a><span class="lineno"> 2036</span>&#160;</div>
<div class="line"><a name="l02037"></a><span class="lineno"> 2037</span>&#160;   // spike count information</div>
<div class="line"><a name="l02038"></a><span class="lineno"> 2038</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02039"></a><span class="lineno"> 2039</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;nSpikeCnt, sizeof(int) * lengthN));</div>
<div class="line"><a name="l02040"></a><span class="lineno"> 2040</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;nSpikeCnt[posN + destOffset], &amp;src-&gt;nSpikeCnt[posN], sizeof(int) * lengthN, kind));</div>
<div class="line"><a name="l02041"></a><span class="lineno"> 2041</span>&#160;}</div>
<div class="line"><a name="l02042"></a><span class="lineno"> 2042</span>&#160;</div>
<div class="line"><a name="l02043"></a><span class="lineno"> 2043</span>&#160;// FIXME: move grpDA(5HT, ACh, NE)Buffer to copyAuxiliaryData</div>
<div class="line"><a name="l02044"></a><span class="lineno"> 2044</span>&#160;/*!</div>
<div class="line"><a name="l02045"></a><span class="lineno"> 2045</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies variables related to group state to it</div>
<div class="line"><a name="l02046"></a><span class="lineno"> 2046</span>&#160; *</div>
<div class="line"><a name="l02047"></a><span class="lineno"> 2047</span>&#160; * This function:</div>
<div class="line"><a name="l02048"></a><span class="lineno"> 2048</span>&#160; * (allocate and) copy grpDA, grp5HT, grpACh, grpNE, grpDABuffer, grp5HTBuffer, grpAChBuffer, grpNEBuffer</div>
<div class="line"><a name="l02049"></a><span class="lineno"> 2049</span>&#160; *</div>
<div class="line"><a name="l02050"></a><span class="lineno"> 2050</span>&#160; * This funcion is called by allocateSNN_GPU() and fetchGroupState(). It supports bi-directional copying</div>
<div class="line"><a name="l02051"></a><span class="lineno"> 2051</span>&#160; *</div>
<div class="line"><a name="l02052"></a><span class="lineno"> 2052</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l02053"></a><span class="lineno"> 2053</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l02054"></a><span class="lineno"> 2054</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l02055"></a><span class="lineno"> 2055</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l02056"></a><span class="lineno"> 2056</span>&#160; * \param[in] kind the direction of copying</div>
<div class="line"><a name="l02057"></a><span class="lineno"> 2057</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l02058"></a><span class="lineno"> 2058</span>&#160; *</div>
<div class="line"><a name="l02059"></a><span class="lineno"> 2059</span>&#160; * \sa allocateSNN_GPU fetchGroupState</div>
<div class="line"><a name="l02060"></a><span class="lineno"> 2060</span>&#160; * \since v3.0</div>
<div class="line"><a name="l02061"></a><span class="lineno"> 2061</span>&#160; */</div>
<div class="line"><a name="l02062"></a><span class="lineno"> 2062</span>&#160;void SNN::copyGroupState(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l02063"></a><span class="lineno"> 2063</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02064"></a><span class="lineno"> 2064</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, 0);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02065"></a><span class="lineno"> 2065</span>&#160;</div>
<div class="line"><a name="l02066"></a><span class="lineno"> 2066</span>&#160;   if (allocateMem) {</div>
<div class="line"><a name="l02067"></a><span class="lineno"> 2067</span>&#160;       assert(dest-&gt;memType == GPU_MEM &amp;&amp; !dest-&gt;allocated);</div>
<div class="line"><a name="l02068"></a><span class="lineno"> 2068</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grpDA, sizeof(float) * networkConfigs[netId].numGroups)); </div>
<div class="line"><a name="l02069"></a><span class="lineno"> 2069</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grp5HT, sizeof(float) * networkConfigs[netId].numGroups)); </div>
<div class="line"><a name="l02070"></a><span class="lineno"> 2070</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grpACh, sizeof(float) * networkConfigs[netId].numGroups)); </div>
<div class="line"><a name="l02071"></a><span class="lineno"> 2071</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grpNE, sizeof(float) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02072"></a><span class="lineno"> 2072</span>&#160;   }</div>
<div class="line"><a name="l02073"></a><span class="lineno"> 2073</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grpDA, src-&gt;grpDA, sizeof(float) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02074"></a><span class="lineno"> 2074</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grp5HT, src-&gt;grp5HT, sizeof(float) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02075"></a><span class="lineno"> 2075</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grpACh, src-&gt;grpACh, sizeof(float) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02076"></a><span class="lineno"> 2076</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grpNE, src-&gt;grpNE, sizeof(float) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02077"></a><span class="lineno"> 2077</span>&#160;</div>
<div class="line"><a name="l02078"></a><span class="lineno"> 2078</span>&#160;   if (lGrpId &lt; 0) {</div>
<div class="line"><a name="l02079"></a><span class="lineno"> 2079</span>&#160;       if (allocateMem) {</div>
<div class="line"><a name="l02080"></a><span class="lineno"> 2080</span>&#160;           assert(dest-&gt;memType == GPU_MEM &amp;&amp; !dest-&gt;allocated);</div>
<div class="line"><a name="l02081"></a><span class="lineno"> 2081</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grpDABuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups)); </div>
<div class="line"><a name="l02082"></a><span class="lineno"> 2082</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grp5HTBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups)); </div>
<div class="line"><a name="l02083"></a><span class="lineno"> 2083</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grpAChBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups)); </div>
<div class="line"><a name="l02084"></a><span class="lineno"> 2084</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**) &amp;dest-&gt;grpNEBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02085"></a><span class="lineno"> 2085</span>&#160;       }</div>
<div class="line"><a name="l02086"></a><span class="lineno"> 2086</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grpDABuffer, src-&gt;grpDABuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02087"></a><span class="lineno"> 2087</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grp5HTBuffer, src-&gt;grp5HTBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02088"></a><span class="lineno"> 2088</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grpAChBuffer, src-&gt;grpAChBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02089"></a><span class="lineno"> 2089</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;grpNEBuffer, src-&gt;grpNEBuffer, sizeof(float) * 1000 * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l02090"></a><span class="lineno"> 2090</span>&#160;   } else {</div>
<div class="line"><a name="l02091"></a><span class="lineno"> 2091</span>&#160;       assert(!allocateMem);</div>
<div class="line"><a name="l02092"></a><span class="lineno"> 2092</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;grpDABuffer[lGrpId * 1000], &amp;src-&gt;grpDABuffer[lGrpId * 1000], sizeof(float) * 1000, kind));</div>
<div class="line"><a name="l02093"></a><span class="lineno"> 2093</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;grp5HTBuffer[lGrpId * 1000], &amp;src-&gt;grp5HTBuffer[lGrpId * 1000], sizeof(float) * 1000, kind));</div>
<div class="line"><a name="l02094"></a><span class="lineno"> 2094</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;grpAChBuffer[lGrpId * 1000], &amp;src-&gt;grpAChBuffer[lGrpId * 1000], sizeof(float) * 1000, kind));</div>
<div class="line"><a name="l02095"></a><span class="lineno"> 2095</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;grpNEBuffer[lGrpId * 1000], &amp;src-&gt;grpNEBuffer[lGrpId * 1000], sizeof(float) * 1000, kind));</div>
<div class="line"><a name="l02096"></a><span class="lineno"> 2096</span>&#160;   }</div>
<div class="line"><a name="l02097"></a><span class="lineno"> 2097</span>&#160;}</div>
<div class="line"><a name="l02098"></a><span class="lineno"> 2098</span>&#160;</div>
<div class="line"><a name="l02099"></a><span class="lineno"> 2099</span>&#160;/*!</div>
<div class="line"><a name="l02100"></a><span class="lineno"> 2100</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies neural parameters to it</div>
<div class="line"><a name="l02101"></a><span class="lineno"> 2101</span>&#160; *</div>
<div class="line"><a name="l02102"></a><span class="lineno"> 2102</span>&#160; * This function:</div>
<div class="line"><a name="l02103"></a><span class="lineno"> 2103</span>&#160; * (allocate and) copy Izh_a, Izh_b, Izh_c, Izh_d</div>
<div class="line"><a name="l02104"></a><span class="lineno"> 2104</span>&#160; * initialize baseFiringInv</div>
<div class="line"><a name="l02105"></a><span class="lineno"> 2105</span>&#160; * (allocate and) copy baseFiring, baseFiringInv</div>
<div class="line"><a name="l02106"></a><span class="lineno"> 2106</span>&#160; *</div>
<div class="line"><a name="l02107"></a><span class="lineno"> 2107</span>&#160; * This funcion is only called by copyNeuronState(). Only copying direction from host to device is required.</div>
<div class="line"><a name="l02108"></a><span class="lineno"> 2108</span>&#160; *</div>
<div class="line"><a name="l02109"></a><span class="lineno"> 2109</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l02110"></a><span class="lineno"> 2110</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l02111"></a><span class="lineno"> 2111</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l02112"></a><span class="lineno"> 2112</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l02113"></a><span class="lineno"> 2113</span>&#160; *</div>
<div class="line"><a name="l02114"></a><span class="lineno"> 2114</span>&#160; * \sa copyNeuronState</div>
<div class="line"><a name="l02115"></a><span class="lineno"> 2115</span>&#160; * \since v3.0</div>
<div class="line"><a name="l02116"></a><span class="lineno"> 2116</span>&#160; */</div>
<div class="line"><a name="l02117"></a><span class="lineno"> 2117</span>&#160;void SNN::copyNeuronParameters(int netId, int lGrpId, RuntimeData* dest, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l02118"></a><span class="lineno"> 2118</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02119"></a><span class="lineno"> 2119</span>&#160;   assert(kind == cudaMemcpyHostToDevice);</div>
<div class="line"><a name="l02120"></a><span class="lineno"> 2120</span>&#160;</div>
<div class="line"><a name="l02121"></a><span class="lineno"> 2121</span>&#160;   int ptrPos, length;</div>
<div class="line"><a name="l02122"></a><span class="lineno"> 2122</span>&#160;</div>
<div class="line"><a name="l02123"></a><span class="lineno"> 2123</span>&#160;   // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02124"></a><span class="lineno"> 2124</span>&#160;   checkDestSrcPtrs(dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, lGrpId, 0);</div>
<div class="line"><a name="l02125"></a><span class="lineno"> 2125</span>&#160;</div>
<div class="line"><a name="l02126"></a><span class="lineno"> 2126</span>&#160;   // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02127"></a><span class="lineno"> 2127</span>&#160;   // cannot use checkDestSrcPtrs here because src pointer would be NULL</div>
<div class="line"><a name="l02128"></a><span class="lineno"> 2128</span>&#160;   if (dest-&gt;allocated &amp;&amp; allocateMem) {</div>
<div class="line"><a name="l02129"></a><span class="lineno"> 2129</span>&#160;       KERNEL_ERROR(&quot;GPU Memory already allocated...&quot;);</div>
<div class="line"><a name="l02130"></a><span class="lineno"> 2130</span>&#160;       exitSimulation(1);</div>
<div class="line"><a name="l02131"></a><span class="lineno"> 2131</span>&#160;   }</div>
<div class="line"><a name="l02132"></a><span class="lineno"> 2132</span>&#160;</div>
<div class="line"><a name="l02133"></a><span class="lineno"> 2133</span>&#160;   // when allocating we are allocating the memory.. we need to do it completely... to avoid memory fragmentation..</div>
<div class="line"><a name="l02134"></a><span class="lineno"> 2134</span>&#160;   if (allocateMem) {</div>
<div class="line"><a name="l02135"></a><span class="lineno"> 2135</span>&#160;       assert(lGrpId == ALL);</div>
<div class="line"><a name="l02136"></a><span class="lineno"> 2136</span>&#160;       assert(dest-&gt;Izh_a == NULL);</div>
<div class="line"><a name="l02137"></a><span class="lineno"> 2137</span>&#160;       assert(dest-&gt;Izh_b == NULL);</div>
<div class="line"><a name="l02138"></a><span class="lineno"> 2138</span>&#160;       assert(dest-&gt;Izh_c == NULL);</div>
<div class="line"><a name="l02139"></a><span class="lineno"> 2139</span>&#160;       assert(dest-&gt;Izh_d == NULL);</div>
<div class="line"><a name="l02140"></a><span class="lineno"> 2140</span>&#160;   }</div>
<div class="line"><a name="l02141"></a><span class="lineno"> 2141</span>&#160;</div>
<div class="line"><a name="l02142"></a><span class="lineno"> 2142</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l02143"></a><span class="lineno"> 2143</span>&#160;       ptrPos = 0;</div>
<div class="line"><a name="l02144"></a><span class="lineno"> 2144</span>&#160;       length = networkConfigs[netId].numNReg;</div>
<div class="line"><a name="l02145"></a><span class="lineno"> 2145</span>&#160;   }</div>
<div class="line"><a name="l02146"></a><span class="lineno"> 2146</span>&#160;   else {</div>
<div class="line"><a name="l02147"></a><span class="lineno"> 2147</span>&#160;       ptrPos = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l02148"></a><span class="lineno"> 2148</span>&#160;       length = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l02149"></a><span class="lineno"> 2149</span>&#160;   }</div>
<div class="line"><a name="l02150"></a><span class="lineno"> 2150</span>&#160;</div>
<div class="line"><a name="l02151"></a><span class="lineno"> 2151</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Izh_a, sizeof(float) * length));</div>
<div class="line"><a name="l02152"></a><span class="lineno"> 2152</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;Izh_a[ptrPos], &amp;(managerRuntimeData.Izh_a[ptrPos]), sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02153"></a><span class="lineno"> 2153</span>&#160;</div>
<div class="line"><a name="l02154"></a><span class="lineno"> 2154</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Izh_b, sizeof(float) * length));</div>
<div class="line"><a name="l02155"></a><span class="lineno"> 2155</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;Izh_b[ptrPos], &amp;(managerRuntimeData.Izh_b[ptrPos]), sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02156"></a><span class="lineno"> 2156</span>&#160;</div>
<div class="line"><a name="l02157"></a><span class="lineno"> 2157</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Izh_c, sizeof(float) * length));</div>
<div class="line"><a name="l02158"></a><span class="lineno"> 2158</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;Izh_c[ptrPos], &amp;(managerRuntimeData.Izh_c[ptrPos]), sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02159"></a><span class="lineno"> 2159</span>&#160;</div>
<div class="line"><a name="l02160"></a><span class="lineno"> 2160</span>&#160;   if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;Izh_d, sizeof(float) * length));</div>
<div class="line"><a name="l02161"></a><span class="lineno"> 2161</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;Izh_d[ptrPos], &amp;(managerRuntimeData.Izh_d[ptrPos]), sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02162"></a><span class="lineno"> 2162</span>&#160;</div>
<div class="line"><a name="l02163"></a><span class="lineno"> 2163</span>&#160;   // pre-compute baseFiringInv for fast computation on GPUs.</div>
<div class="line"><a name="l02164"></a><span class="lineno"> 2164</span>&#160;   if (sim_with_homeostasis) {</div>
<div class="line"><a name="l02165"></a><span class="lineno"> 2165</span>&#160;       float* baseFiringInv = new float[length];</div>
<div class="line"><a name="l02166"></a><span class="lineno"> 2166</span>&#160;       for(int nid = 0; nid &lt; length; nid++) {</div>
<div class="line"><a name="l02167"></a><span class="lineno"> 2167</span>&#160;           if (managerRuntimeData.baseFiring[nid] != 0.0f)</div>
<div class="line"><a name="l02168"></a><span class="lineno"> 2168</span>&#160;               baseFiringInv[nid] = 1.0f / managerRuntimeData.baseFiring[ptrPos + nid];</div>
<div class="line"><a name="l02169"></a><span class="lineno"> 2169</span>&#160;           else</div>
<div class="line"><a name="l02170"></a><span class="lineno"> 2170</span>&#160;               baseFiringInv[nid] = 0.0;</div>
<div class="line"><a name="l02171"></a><span class="lineno"> 2171</span>&#160;       }</div>
<div class="line"><a name="l02172"></a><span class="lineno"> 2172</span>&#160;</div>
<div class="line"><a name="l02173"></a><span class="lineno"> 2173</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;baseFiringInv, sizeof(float) * length));</div>
<div class="line"><a name="l02174"></a><span class="lineno"> 2174</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;baseFiringInv[ptrPos], baseFiringInv, sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02175"></a><span class="lineno"> 2175</span>&#160;</div>
<div class="line"><a name="l02176"></a><span class="lineno"> 2176</span>&#160;       if(allocateMem) CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;baseFiring, sizeof(float) * length));</div>
<div class="line"><a name="l02177"></a><span class="lineno"> 2177</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;baseFiring[ptrPos], managerRuntimeData.baseFiring, sizeof(float) * length, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02178"></a><span class="lineno"> 2178</span>&#160;</div>
<div class="line"><a name="l02179"></a><span class="lineno"> 2179</span>&#160;       delete [] baseFiringInv;</div>
<div class="line"><a name="l02180"></a><span class="lineno"> 2180</span>&#160;   }</div>
<div class="line"><a name="l02181"></a><span class="lineno"> 2181</span>&#160;}</div>
<div class="line"><a name="l02182"></a><span class="lineno"> 2182</span>&#160;</div>
<div class="line"><a name="l02183"></a><span class="lineno"> 2183</span>&#160;/*!</div>
<div class="line"><a name="l02184"></a><span class="lineno"> 2184</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies short-term plasticity (STP) state to it</div>
<div class="line"><a name="l02185"></a><span class="lineno"> 2185</span>&#160; *</div>
<div class="line"><a name="l02186"></a><span class="lineno"> 2186</span>&#160; * This function:</div>
<div class="line"><a name="l02187"></a><span class="lineno"> 2187</span>&#160; * initialize STP_Pitch</div>
<div class="line"><a name="l02188"></a><span class="lineno"> 2188</span>&#160; * (allocate and) copy stpu, stpx</div>
<div class="line"><a name="l02189"></a><span class="lineno"> 2189</span>&#160; *</div>
<div class="line"><a name="l02190"></a><span class="lineno"> 2190</span>&#160; * This funcion is called by allocateSNN_GPU() and fetchSTPState(). It supports bi-directional copying</div>
<div class="line"><a name="l02191"></a><span class="lineno"> 2191</span>&#160; *</div>
<div class="line"><a name="l02192"></a><span class="lineno"> 2192</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l02193"></a><span class="lineno"> 2193</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l02194"></a><span class="lineno"> 2194</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l02195"></a><span class="lineno"> 2195</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l02196"></a><span class="lineno"> 2196</span>&#160; * \param[in] kind the direction of copying</div>
<div class="line"><a name="l02197"></a><span class="lineno"> 2197</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l02198"></a><span class="lineno"> 2198</span>&#160; *</div>
<div class="line"><a name="l02199"></a><span class="lineno"> 2199</span>&#160; * \sa allocateSNN_GPU fetchSTPState</div>
<div class="line"><a name="l02200"></a><span class="lineno"> 2200</span>&#160; * \since v3.0</div>
<div class="line"><a name="l02201"></a><span class="lineno"> 2201</span>&#160; */</div>
<div class="line"><a name="l02202"></a><span class="lineno"> 2202</span>&#160;void SNN::copySTPState(int netId, int lGrpId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l02203"></a><span class="lineno"> 2203</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02204"></a><span class="lineno"> 2204</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, lGrpId, 0); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02205"></a><span class="lineno"> 2205</span>&#160;   </div>
<div class="line"><a name="l02206"></a><span class="lineno"> 2206</span>&#160;   </div>
<div class="line"><a name="l02207"></a><span class="lineno"> 2207</span>&#160;   // STP feature is optional, do addtional check for memory space</div>
<div class="line"><a name="l02208"></a><span class="lineno"> 2208</span>&#160;   if(allocateMem) {</div>
<div class="line"><a name="l02209"></a><span class="lineno"> 2209</span>&#160;       assert(dest-&gt;stpu == NULL);</div>
<div class="line"><a name="l02210"></a><span class="lineno"> 2210</span>&#160;       assert(dest-&gt;stpx == NULL);</div>
<div class="line"><a name="l02211"></a><span class="lineno"> 2211</span>&#160;   } else {</div>
<div class="line"><a name="l02212"></a><span class="lineno"> 2212</span>&#160;       assert(dest-&gt;stpu != NULL);</div>
<div class="line"><a name="l02213"></a><span class="lineno"> 2213</span>&#160;       assert(dest-&gt;stpx != NULL);</div>
<div class="line"><a name="l02214"></a><span class="lineno"> 2214</span>&#160;   }</div>
<div class="line"><a name="l02215"></a><span class="lineno"> 2215</span>&#160;   assert(src-&gt;stpu != NULL); assert(src-&gt;stpx != NULL);</div>
<div class="line"><a name="l02216"></a><span class="lineno"> 2216</span>&#160;</div>
<div class="line"><a name="l02217"></a><span class="lineno"> 2217</span>&#160;   size_t STP_Pitch;</div>
<div class="line"><a name="l02218"></a><span class="lineno"> 2218</span>&#160;   size_t widthInBytes = sizeof(float) * networkConfigs[netId].numN;</div>
<div class="line"><a name="l02219"></a><span class="lineno"> 2219</span>&#160;</div>
<div class="line"><a name="l02220"></a><span class="lineno"> 2220</span>&#160;// if(allocateMem)     CUDA_CHECK_ERRORS( cudaMalloc( (void**) &amp;dest-&gt;stpu, sizeof(float)*networkConfigs[0].numN));</div>
<div class="line"><a name="l02221"></a><span class="lineno"> 2221</span>&#160;// CUDA_CHECK_ERRORS( cudaMemcpy( &amp;dest-&gt;stpu[0], &amp;src-&gt;stpu[0], sizeof(float)*networkConfigs[0].numN, kind));</div>
<div class="line"><a name="l02222"></a><span class="lineno"> 2222</span>&#160;</div>
<div class="line"><a name="l02223"></a><span class="lineno"> 2223</span>&#160;// if(allocateMem)     CUDA_CHECK_ERRORS( cudaMalloc( (void**) &amp;dest-&gt;stpx, sizeof(float)*networkConfigs[0].numN));</div>
<div class="line"><a name="l02224"></a><span class="lineno"> 2224</span>&#160;// CUDA_CHECK_ERRORS( cudaMemcpy( &amp;dest-&gt;stpx[0], &amp;src-&gt;stpx[0], sizeof(float)*networkConfigs[0].numN, kind));</div>
<div class="line"><a name="l02225"></a><span class="lineno"> 2225</span>&#160;</div>
<div class="line"><a name="l02226"></a><span class="lineno"> 2226</span>&#160;   // allocate the stpu and stpx variable</div>
<div class="line"><a name="l02227"></a><span class="lineno"> 2227</span>&#160;   if (allocateMem)</div>
<div class="line"><a name="l02228"></a><span class="lineno"> 2228</span>&#160;       CUDA_CHECK_ERRORS(cudaMallocPitch((void**)&amp;dest-&gt;stpu, &amp;networkConfigs[netId].STP_Pitch, widthInBytes, networkConfigs[netId].maxDelay + 1));</div>
<div class="line"><a name="l02229"></a><span class="lineno"> 2229</span>&#160;   if (allocateMem)</div>
<div class="line"><a name="l02230"></a><span class="lineno"> 2230</span>&#160;       CUDA_CHECK_ERRORS(cudaMallocPitch((void**)&amp;dest-&gt;stpx, &amp;STP_Pitch, widthInBytes, networkConfigs[netId].maxDelay + 1));</div>
<div class="line"><a name="l02231"></a><span class="lineno"> 2231</span>&#160;</div>
<div class="line"><a name="l02232"></a><span class="lineno"> 2232</span>&#160;   assert(networkConfigs[netId].STP_Pitch &gt; 0);</div>
<div class="line"><a name="l02233"></a><span class="lineno"> 2233</span>&#160;   assert(STP_Pitch &gt; 0);              // stp_pitch should be greater than zero</div>
<div class="line"><a name="l02234"></a><span class="lineno"> 2234</span>&#160;   assert(STP_Pitch == networkConfigs[netId].STP_Pitch);   // we want same Pitch for stpu and stpx</div>
<div class="line"><a name="l02235"></a><span class="lineno"> 2235</span>&#160;   assert(networkConfigs[netId].STP_Pitch &gt;= widthInBytes);    // stp_pitch should be greater than the width</div>
<div class="line"><a name="l02236"></a><span class="lineno"> 2236</span>&#160;   // convert the Pitch value to multiples of float</div>
<div class="line"><a name="l02237"></a><span class="lineno"> 2237</span>&#160;   assert(networkConfigs[netId].STP_Pitch % (sizeof(float)) == 0);</div>
<div class="line"><a name="l02238"></a><span class="lineno"> 2238</span>&#160;   if (allocateMem)</div>
<div class="line"><a name="l02239"></a><span class="lineno"> 2239</span>&#160;       networkConfigs[netId].STP_Pitch = networkConfigs[netId].STP_Pitch/sizeof(float);</div>
<div class="line"><a name="l02240"></a><span class="lineno"> 2240</span>&#160;</div>
<div class="line"><a name="l02241"></a><span class="lineno"> 2241</span>&#160;// fprintf(stderr, &quot;STP_Pitch = %ld, STP_witdhInBytes = %d\n&quot;, networkConfigs[0].STP_Pitch, widthInBytes);</div>
<div class="line"><a name="l02242"></a><span class="lineno"> 2242</span>&#160;</div>
<div class="line"><a name="l02243"></a><span class="lineno"> 2243</span>&#160;   float* tmp_stp = new float[networkConfigs[netId].numN];</div>
<div class="line"><a name="l02244"></a><span class="lineno"> 2244</span>&#160;   // copy the already generated values of stpx and stpu to the GPU</div>
<div class="line"><a name="l02245"></a><span class="lineno"> 2245</span>&#160;   for(int t = 0; t &lt; networkConfigs[netId].maxDelay + 1; t++) {</div>
<div class="line"><a name="l02246"></a><span class="lineno"> 2246</span>&#160;       if (kind == cudaMemcpyHostToDevice) {</div>
<div class="line"><a name="l02247"></a><span class="lineno"> 2247</span>&#160;           // stpu in the CPU might be mapped in a specific way. we want to change the format</div>
<div class="line"><a name="l02248"></a><span class="lineno"> 2248</span>&#160;           // to something that is okay with the GPU STP_U and STP_X variable implementation..</div>
<div class="line"><a name="l02249"></a><span class="lineno"> 2249</span>&#160;           for (int n = 0; n &lt; networkConfigs[netId].numN; n++) {</div>
<div class="line"><a name="l02250"></a><span class="lineno"> 2250</span>&#160;               int idx = STP_BUF_POS(n, t, glbNetworkConfig.maxDelay);</div>
<div class="line"><a name="l02251"></a><span class="lineno"> 2251</span>&#160;               tmp_stp[n] = managerRuntimeData.stpu[idx];</div>
<div class="line"><a name="l02252"></a><span class="lineno"> 2252</span>&#160;               //assert(tmp_stp[n] == 0.0f); // STP is not enabled for all groups</div>
<div class="line"><a name="l02253"></a><span class="lineno"> 2253</span>&#160;           }</div>
<div class="line"><a name="l02254"></a><span class="lineno"> 2254</span>&#160;           CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;stpu[t * networkConfigs[netId].STP_Pitch], tmp_stp, sizeof(float) * networkConfigs[netId].numN, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02255"></a><span class="lineno"> 2255</span>&#160;           for (int n = 0; n &lt; networkConfigs[netId].numN; n++) {</div>
<div class="line"><a name="l02256"></a><span class="lineno"> 2256</span>&#160;               int idx = STP_BUF_POS(n, t, glbNetworkConfig.maxDelay);</div>
<div class="line"><a name="l02257"></a><span class="lineno"> 2257</span>&#160;               tmp_stp[n] = managerRuntimeData.stpx[idx];</div>
<div class="line"><a name="l02258"></a><span class="lineno"> 2258</span>&#160;               //assert(tmp_stp[n] == 1.0f); // STP is not enabled for all groups</div>
<div class="line"><a name="l02259"></a><span class="lineno"> 2259</span>&#160;           }</div>
<div class="line"><a name="l02260"></a><span class="lineno"> 2260</span>&#160;           CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;stpx[t * networkConfigs[netId].STP_Pitch], tmp_stp, sizeof(float) * networkConfigs[netId].numN, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02261"></a><span class="lineno"> 2261</span>&#160;       } else {</div>
<div class="line"><a name="l02262"></a><span class="lineno"> 2262</span>&#160;           CUDA_CHECK_ERRORS(cudaMemcpy(tmp_stp, &amp;dest-&gt;stpu[t * networkConfigs[netId].STP_Pitch], sizeof(float) * networkConfigs[netId].numN, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02263"></a><span class="lineno"> 2263</span>&#160;           for (int n = 0; n &lt; networkConfigs[netId].numN; n++)</div>
<div class="line"><a name="l02264"></a><span class="lineno"> 2264</span>&#160;               managerRuntimeData.stpu[STP_BUF_POS(n, t, glbNetworkConfig.maxDelay)] = tmp_stp[n];</div>
<div class="line"><a name="l02265"></a><span class="lineno"> 2265</span>&#160;           CUDA_CHECK_ERRORS(cudaMemcpy(tmp_stp, &amp;dest-&gt;stpx[t * networkConfigs[netId].STP_Pitch], sizeof(float) * networkConfigs[netId].numN, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02266"></a><span class="lineno"> 2266</span>&#160;           for (int n = 0; n &lt; networkConfigs[netId].numN; n++)</div>
<div class="line"><a name="l02267"></a><span class="lineno"> 2267</span>&#160;               managerRuntimeData.stpx[STP_BUF_POS(n, t, glbNetworkConfig.maxDelay)] = tmp_stp[n];</div>
<div class="line"><a name="l02268"></a><span class="lineno"> 2268</span>&#160;       }</div>
<div class="line"><a name="l02269"></a><span class="lineno"> 2269</span>&#160;   }</div>
<div class="line"><a name="l02270"></a><span class="lineno"> 2270</span>&#160;   delete [] tmp_stp;</div>
<div class="line"><a name="l02271"></a><span class="lineno"> 2271</span>&#160;}</div>
<div class="line"><a name="l02272"></a><span class="lineno"> 2272</span>&#160;</div>
<div class="line"><a name="l02273"></a><span class="lineno"> 2273</span>&#160;/*!</div>
<div class="line"><a name="l02274"></a><span class="lineno"> 2274</span>&#160; * \brief This function copies networkConfig form host to device</div>
<div class="line"><a name="l02275"></a><span class="lineno"> 2275</span>&#160; *</div>
<div class="line"><a name="l02276"></a><span class="lineno"> 2276</span>&#160; * This function:</div>
<div class="line"><a name="l02277"></a><span class="lineno"> 2277</span>&#160; * copy networkConfig</div>
<div class="line"><a name="l02278"></a><span class="lineno"> 2278</span>&#160; *</div>
<div class="line"><a name="l02279"></a><span class="lineno"> 2279</span>&#160; * \param[in] netId the id of a local network whose networkConfig will be copied to device (GPU) memory</div>
<div class="line"><a name="l02280"></a><span class="lineno"> 2280</span>&#160; *</div>
<div class="line"><a name="l02281"></a><span class="lineno"> 2281</span>&#160; * \since v4.0</div>
<div class="line"><a name="l02282"></a><span class="lineno"> 2282</span>&#160; */</div>
<div class="line"><a name="l02283"></a><span class="lineno"> 2283</span>&#160;void SNN::copyNetworkConfig(int netId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l02284"></a><span class="lineno"> 2284</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02285"></a><span class="lineno"> 2285</span>&#160;   assert(kind == cudaMemcpyHostToDevice);</div>
<div class="line"><a name="l02286"></a><span class="lineno"> 2286</span>&#160;</div>
<div class="line"><a name="l02287"></a><span class="lineno"> 2287</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(networkConfigGPU, &amp;networkConfigs[netId], sizeof(NetworkConfigRT), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02288"></a><span class="lineno"> 2288</span>&#160;}</div>
<div class="line"><a name="l02289"></a><span class="lineno"> 2289</span>&#160;</div>
<div class="line"><a name="l02290"></a><span class="lineno"> 2290</span>&#160;/*!</div>
<div class="line"><a name="l02291"></a><span class="lineno"> 2291</span>&#160; * \brief This function copies groupConfigs form host to device</div>
<div class="line"><a name="l02292"></a><span class="lineno"> 2292</span>&#160; *</div>
<div class="line"><a name="l02293"></a><span class="lineno"> 2293</span>&#160; * This function:</div>
<div class="line"><a name="l02294"></a><span class="lineno"> 2294</span>&#160; * copy groupConfigs</div>
<div class="line"><a name="l02295"></a><span class="lineno"> 2295</span>&#160; *</div>
<div class="line"><a name="l02296"></a><span class="lineno"> 2296</span>&#160; * \param[in] netId the id of a local network whose groupConfigs will be copied to device (GPU) memory</div>
<div class="line"><a name="l02297"></a><span class="lineno"> 2297</span>&#160; *</div>
<div class="line"><a name="l02298"></a><span class="lineno"> 2298</span>&#160; * \since v4.0</div>
<div class="line"><a name="l02299"></a><span class="lineno"> 2299</span>&#160; */</div>
<div class="line"><a name="l02300"></a><span class="lineno"> 2300</span>&#160;void SNN::copyGroupConfigs(int netId) {</div>
<div class="line"><a name="l02301"></a><span class="lineno"> 2301</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02302"></a><span class="lineno"> 2302</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(groupConfigsGPU, groupConfigs[netId], (networkConfigs[netId].numGroupsAssigned) * sizeof(GroupConfigRT), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02303"></a><span class="lineno"> 2303</span>&#160;}</div>
<div class="line"><a name="l02304"></a><span class="lineno"> 2304</span>&#160;</div>
<div class="line"><a name="l02305"></a><span class="lineno"> 2305</span>&#160;/*!</div>
<div class="line"><a name="l02306"></a><span class="lineno"> 2306</span>&#160; * \brief this function copy weight state in device (GPU) memory sapce to main (CPU) memory space</div>
<div class="line"><a name="l02307"></a><span class="lineno"> 2307</span>&#160; *</div>
<div class="line"><a name="l02308"></a><span class="lineno"> 2308</span>&#160; * This function:</div>
<div class="line"><a name="l02309"></a><span class="lineno"> 2309</span>&#160; * copy wt, wtChange synSpikeTime</div>
<div class="line"><a name="l02310"></a><span class="lineno"> 2310</span>&#160; *</div>
<div class="line"><a name="l02311"></a><span class="lineno"> 2311</span>&#160; * This funcion is only called by fetchWeightState(). Only copying direction from device to host is required.</div>
<div class="line"><a name="l02312"></a><span class="lineno"> 2312</span>&#160; *</div>
<div class="line"><a name="l02313"></a><span class="lineno"> 2313</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l02314"></a><span class="lineno"> 2314</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l02315"></a><span class="lineno"> 2315</span>&#160; *</div>
<div class="line"><a name="l02316"></a><span class="lineno"> 2316</span>&#160; * \sa fetchWeightState</div>
<div class="line"><a name="l02317"></a><span class="lineno"> 2317</span>&#160; * \since v4.0</div>
<div class="line"><a name="l02318"></a><span class="lineno"> 2318</span>&#160; */</div>
<div class="line"><a name="l02319"></a><span class="lineno"> 2319</span>&#160;void SNN::copyWeightState(int netId, int lGrpId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l02320"></a><span class="lineno"> 2320</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02321"></a><span class="lineno"> 2321</span>&#160;   checkDestSrcPtrs(&amp;managerRuntimeData, &amp;runtimeData[netId], cudaMemcpyDeviceToHost, false, lGrpId, 0); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02322"></a><span class="lineno"> 2322</span>&#160;   assert(kind == cudaMemcpyDeviceToHost);</div>
<div class="line"><a name="l02323"></a><span class="lineno"> 2323</span>&#160;</div>
<div class="line"><a name="l02324"></a><span class="lineno"> 2324</span>&#160;   int lengthSyn, posSyn;</div>
<div class="line"><a name="l02325"></a><span class="lineno"> 2325</span>&#160;</div>
<div class="line"><a name="l02326"></a><span class="lineno"> 2326</span>&#160;   // first copy pre-connections info</div>
<div class="line"><a name="l02327"></a><span class="lineno"> 2327</span>&#160;   copyPreConnectionInfo(netId, lGrpId, &amp;managerRuntimeData, &amp;runtimeData[netId], cudaMemcpyDeviceToHost, false);</div>
<div class="line"><a name="l02328"></a><span class="lineno"> 2328</span>&#160;</div>
<div class="line"><a name="l02329"></a><span class="lineno"> 2329</span>&#160;   if (lGrpId == ALL) {</div>
<div class="line"><a name="l02330"></a><span class="lineno"> 2330</span>&#160;       lengthSyn = networkConfigs[netId].numPreSynNet;</div>
<div class="line"><a name="l02331"></a><span class="lineno"> 2331</span>&#160;       posSyn = 0;</div>
<div class="line"><a name="l02332"></a><span class="lineno"> 2332</span>&#160;   } else {</div>
<div class="line"><a name="l02333"></a><span class="lineno"> 2333</span>&#160;       lengthSyn = 0;</div>
<div class="line"><a name="l02334"></a><span class="lineno"> 2334</span>&#160;       for (int lNId = groupConfigs[netId][lGrpId].lStartN; lNId &lt;= groupConfigs[netId][lGrpId].lEndN; lNId++)</div>
<div class="line"><a name="l02335"></a><span class="lineno"> 2335</span>&#160;           lengthSyn += managerRuntimeData.Npre[lNId];</div>
<div class="line"><a name="l02336"></a><span class="lineno"> 2336</span>&#160;</div>
<div class="line"><a name="l02337"></a><span class="lineno"> 2337</span>&#160;       posSyn = managerRuntimeData.cumulativePre[groupConfigs[netId][lGrpId].lStartN];</div>
<div class="line"><a name="l02338"></a><span class="lineno"> 2338</span>&#160;   }</div>
<div class="line"><a name="l02339"></a><span class="lineno"> 2339</span>&#160;</div>
<div class="line"><a name="l02340"></a><span class="lineno"> 2340</span>&#160;   assert(posSyn &lt; networkConfigs[netId].numPreSynNet || networkConfigs[netId].numPreSynNet == 0);</div>
<div class="line"><a name="l02341"></a><span class="lineno"> 2341</span>&#160;   assert(lengthSyn &lt;= networkConfigs[netId].numPreSynNet);</div>
<div class="line"><a name="l02342"></a><span class="lineno"> 2342</span>&#160;</div>
<div class="line"><a name="l02343"></a><span class="lineno"> 2343</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;managerRuntimeData.wt[posSyn], &amp;runtimeData[netId].wt[posSyn], sizeof(float) * lengthSyn, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02344"></a><span class="lineno"> 2344</span>&#160;</div>
<div class="line"><a name="l02345"></a><span class="lineno"> 2345</span>&#160;   // copy firing time for individual synapses</div>
<div class="line"><a name="l02346"></a><span class="lineno"> 2346</span>&#160;   //CUDA_CHECK_ERRORS(cudaMemcpy(&amp;managerRuntimeData.synSpikeTime[cumPos_syn], &amp;runtimeData[netId].synSpikeTime[cumPos_syn], sizeof(int) * length_wt, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02347"></a><span class="lineno"> 2347</span>&#160;</div>
<div class="line"><a name="l02348"></a><span class="lineno"> 2348</span>&#160;   if ((!sim_with_fixedwts) || sim_with_stdp) {</div>
<div class="line"><a name="l02349"></a><span class="lineno"> 2349</span>&#160;       // copy synaptic weight derivative</div>
<div class="line"><a name="l02350"></a><span class="lineno"> 2350</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy( &amp;managerRuntimeData.wtChange[posSyn], &amp;runtimeData[netId].wtChange[posSyn], sizeof(float) * lengthSyn, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02351"></a><span class="lineno"> 2351</span>&#160;   }</div>
<div class="line"><a name="l02352"></a><span class="lineno"> 2352</span>&#160;}</div>
<div class="line"><a name="l02353"></a><span class="lineno"> 2353</span>&#160;</div>
<div class="line"><a name="l02354"></a><span class="lineno"> 2354</span>&#160;</div>
<div class="line"><a name="l02355"></a><span class="lineno"> 2355</span>&#160;/*!</div>
<div class="line"><a name="l02356"></a><span class="lineno"> 2356</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies variables related to syanpses to it</div>
<div class="line"><a name="l02357"></a><span class="lineno"> 2357</span>&#160; *</div>
<div class="line"><a name="l02358"></a><span class="lineno"> 2358</span>&#160; * This function:</div>
<div class="line"><a name="l02359"></a><span class="lineno"> 2359</span>&#160; * (allocate and) copy wt, wtChange, maxSynWt</div>
<div class="line"><a name="l02360"></a><span class="lineno"> 2360</span>&#160; *</div>
<div class="line"><a name="l02361"></a><span class="lineno"> 2361</span>&#160; *</div>
<div class="line"><a name="l02362"></a><span class="lineno"> 2362</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l02363"></a><span class="lineno"> 2363</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l02364"></a><span class="lineno"> 2364</span>&#160; * \param[in] src pointer to runtime data source</div>
<div class="line"><a name="l02365"></a><span class="lineno"> 2365</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l02366"></a><span class="lineno"> 2366</span>&#160; *</div>
<div class="line"><a name="l02367"></a><span class="lineno"> 2367</span>&#160; * \sa allocateSNN_GPU</div>
<div class="line"><a name="l02368"></a><span class="lineno"> 2368</span>&#160; * \since v4.0</div>
<div class="line"><a name="l02369"></a><span class="lineno"> 2369</span>&#160; */</div>
<div class="line"><a name="l02370"></a><span class="lineno"> 2370</span>&#160;void SNN::copySynapseState(int netId, RuntimeData* dest, RuntimeData* src, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l02371"></a><span class="lineno"> 2371</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02372"></a><span class="lineno"> 2372</span>&#160;   checkDestSrcPtrs(dest, src, kind, allocateMem, ALL, 0); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02373"></a><span class="lineno"> 2373</span>&#160;   </div>
<div class="line"><a name="l02374"></a><span class="lineno"> 2374</span>&#160;   assert(networkConfigs[netId].numPreSynNet &gt; 0);</div>
<div class="line"><a name="l02375"></a><span class="lineno"> 2375</span>&#160;</div>
<div class="line"><a name="l02376"></a><span class="lineno"> 2376</span>&#160;   // synaptic information based</div>
<div class="line"><a name="l02377"></a><span class="lineno"> 2377</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02378"></a><span class="lineno"> 2378</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;wt, sizeof(float) * networkConfigs[netId].numPreSynNet));</div>
<div class="line"><a name="l02379"></a><span class="lineno"> 2379</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;wt, src-&gt;wt, sizeof(float) * networkConfigs[netId].numPreSynNet, kind));</div>
<div class="line"><a name="l02380"></a><span class="lineno"> 2380</span>&#160;</div>
<div class="line"><a name="l02381"></a><span class="lineno"> 2381</span>&#160;   // we don&#39;t need these data structures if the network doesn&#39;t have any plastic synapses at all</div>
<div class="line"><a name="l02382"></a><span class="lineno"> 2382</span>&#160;   // they show up in gpuUpdateLTP() and updateSynapticWeights(), two functions that do not get called if</div>
<div class="line"><a name="l02383"></a><span class="lineno"> 2383</span>&#160;   // sim_with_fixedwts is set</div>
<div class="line"><a name="l02384"></a><span class="lineno"> 2384</span>&#160;   if (!sim_with_fixedwts) {</div>
<div class="line"><a name="l02385"></a><span class="lineno"> 2385</span>&#160;       // synaptic weight derivative</div>
<div class="line"><a name="l02386"></a><span class="lineno"> 2386</span>&#160;       if(allocateMem)</div>
<div class="line"><a name="l02387"></a><span class="lineno"> 2387</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;wtChange, sizeof(float) * networkConfigs[netId].numPreSynNet));</div>
<div class="line"><a name="l02388"></a><span class="lineno"> 2388</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;wtChange, src-&gt;wtChange, sizeof(float) * networkConfigs[netId].numPreSynNet, kind));</div>
<div class="line"><a name="l02389"></a><span class="lineno"> 2389</span>&#160;</div>
<div class="line"><a name="l02390"></a><span class="lineno"> 2390</span>&#160;       // synaptic weight maximum value</div>
<div class="line"><a name="l02391"></a><span class="lineno"> 2391</span>&#160;       if(allocateMem)</div>
<div class="line"><a name="l02392"></a><span class="lineno"> 2392</span>&#160;           CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;maxSynWt, sizeof(float) * networkConfigs[netId].numPreSynNet));</div>
<div class="line"><a name="l02393"></a><span class="lineno"> 2393</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;maxSynWt, src-&gt;maxSynWt, sizeof(float) * networkConfigs[netId].numPreSynNet, kind));</div>
<div class="line"><a name="l02394"></a><span class="lineno"> 2394</span>&#160;   }</div>
<div class="line"><a name="l02395"></a><span class="lineno"> 2395</span>&#160;}</div>
<div class="line"><a name="l02396"></a><span class="lineno"> 2396</span>&#160;</div>
<div class="line"><a name="l02397"></a><span class="lineno"> 2397</span>&#160;/*!</div>
<div class="line"><a name="l02398"></a><span class="lineno"> 2398</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies auxiliary runtime data to it</div>
<div class="line"><a name="l02399"></a><span class="lineno"> 2399</span>&#160; *</div>
<div class="line"><a name="l02400"></a><span class="lineno"> 2400</span>&#160; * This function:</div>
<div class="line"><a name="l02401"></a><span class="lineno"> 2401</span>&#160; * (allocate and) reset spikeGenBits, poissonFireRate</div>
<div class="line"><a name="l02402"></a><span class="lineno"> 2402</span>&#160; * initialize I_setLength, I_setPitch; (allocate and) reset I_set</div>
<div class="line"><a name="l02403"></a><span class="lineno"> 2403</span>&#160; * (allocate and) copy synSpikeTime, lastSpikeTime</div>
<div class="line"><a name="l02404"></a><span class="lineno"> 2404</span>&#160; * (allocate and) copy nSpikeCnt</div>
<div class="line"><a name="l02405"></a><span class="lineno"> 2405</span>&#160; * (allocate and) copy grpIds, connIdsPreIdx</div>
<div class="line"><a name="l02406"></a><span class="lineno"> 2406</span>&#160; * (allocate and) copy firingTableD1, firingTableD2</div>
<div class="line"><a name="l02407"></a><span class="lineno"> 2407</span>&#160; * This funcion is only called by allocateSNN_GPU. Therefore, only copying direction from host to device is required</div>
<div class="line"><a name="l02408"></a><span class="lineno"> 2408</span>&#160; *</div>
<div class="line"><a name="l02409"></a><span class="lineno"> 2409</span>&#160; * \param[in] netId the id of local network, which is the same as device (GPU) id</div>
<div class="line"><a name="l02410"></a><span class="lineno"> 2410</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l02411"></a><span class="lineno"> 2411</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l02412"></a><span class="lineno"> 2412</span>&#160; *</div>
<div class="line"><a name="l02413"></a><span class="lineno"> 2413</span>&#160; * \sa allocateSNN_GPU</div>
<div class="line"><a name="l02414"></a><span class="lineno"> 2414</span>&#160; * \since v4.0</div>
<div class="line"><a name="l02415"></a><span class="lineno"> 2415</span>&#160; */</div>
<div class="line"><a name="l02416"></a><span class="lineno"> 2416</span>&#160;void SNN::copyAuxiliaryData(int netId, int lGrpId, RuntimeData* dest, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l02417"></a><span class="lineno"> 2417</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02418"></a><span class="lineno"> 2418</span>&#160;   checkDestSrcPtrs(dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, ALL, 0); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02419"></a><span class="lineno"> 2419</span>&#160;   assert(kind == cudaMemcpyHostToDevice);</div>
<div class="line"><a name="l02420"></a><span class="lineno"> 2420</span>&#160;</div>
<div class="line"><a name="l02421"></a><span class="lineno"> 2421</span>&#160;   assert(networkConfigs[netId].numN &gt; 0);</div>
<div class="line"><a name="l02422"></a><span class="lineno"> 2422</span>&#160;</div>
<div class="line"><a name="l02423"></a><span class="lineno"> 2423</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02424"></a><span class="lineno"> 2424</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;spikeGenBits, sizeof(int) * (networkConfigs[netId].numNSpikeGen / 32 + 1)));</div>
<div class="line"><a name="l02425"></a><span class="lineno"> 2425</span>&#160;   CUDA_CHECK_ERRORS(cudaMemset(dest-&gt;spikeGenBits, 0, sizeof(int) * (networkConfigs[netId].numNSpikeGen / 32 + 1)));</div>
<div class="line"><a name="l02426"></a><span class="lineno"> 2426</span>&#160;</div>
<div class="line"><a name="l02427"></a><span class="lineno"> 2427</span>&#160;   // allocate the poisson neuron poissonFireRate</div>
<div class="line"><a name="l02428"></a><span class="lineno"> 2428</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02429"></a><span class="lineno"> 2429</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;poissonFireRate, sizeof(float) * networkConfigs[netId].numNPois));</div>
<div class="line"><a name="l02430"></a><span class="lineno"> 2430</span>&#160;   CUDA_CHECK_ERRORS(cudaMemset(dest-&gt;poissonFireRate, 0, sizeof(float) * networkConfigs[netId].numNPois));</div>
<div class="line"><a name="l02431"></a><span class="lineno"> 2431</span>&#160;</div>
<div class="line"><a name="l02432"></a><span class="lineno"> 2432</span>&#160;   // synaptic auxiliary data</div>
<div class="line"><a name="l02433"></a><span class="lineno"> 2433</span>&#160;   // I_set: a bit vector indicates which synapse got a spike</div>
<div class="line"><a name="l02434"></a><span class="lineno"> 2434</span>&#160;</div>
<div class="line"><a name="l02435"></a><span class="lineno"> 2435</span>&#160;   if(allocateMem) {</div>
<div class="line"><a name="l02436"></a><span class="lineno"> 2436</span>&#160;       networkConfigs[netId].I_setLength = ceil(((networkConfigs[netId].maxNumPreSynN) / 32.0f));</div>
<div class="line"><a name="l02437"></a><span class="lineno"> 2437</span>&#160;       CUDA_CHECK_ERRORS(cudaMallocPitch((void**)&amp;dest-&gt;I_set, &amp;networkConfigs[netId].I_setPitch, sizeof(int) * networkConfigs[netId].numNReg, networkConfigs[netId].I_setLength));</div>
<div class="line"><a name="l02438"></a><span class="lineno"> 2438</span>&#160;   }</div>
<div class="line"><a name="l02439"></a><span class="lineno"> 2439</span>&#160;   assert(networkConfigs[netId].I_setPitch &gt; 0 || networkConfigs[netId].maxNumPreSynN == 0);</div>
<div class="line"><a name="l02440"></a><span class="lineno"> 2440</span>&#160;   CUDA_CHECK_ERRORS(cudaMemset(dest-&gt;I_set, 0, networkConfigs[netId].I_setPitch * networkConfigs[netId].I_setLength));</div>
<div class="line"><a name="l02441"></a><span class="lineno"> 2441</span>&#160;</div>
<div class="line"><a name="l02442"></a><span class="lineno"> 2442</span>&#160;   // synSpikeTime: an array indicates the last time when a synapse got a spike</div>
<div class="line"><a name="l02443"></a><span class="lineno"> 2443</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02444"></a><span class="lineno"> 2444</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;synSpikeTime, sizeof(int) * networkConfigs[netId].numPreSynNet));</div>
<div class="line"><a name="l02445"></a><span class="lineno"> 2445</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;synSpikeTime, managerRuntimeData.synSpikeTime, sizeof(int) * networkConfigs[netId].numPreSynNet, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02446"></a><span class="lineno"> 2446</span>&#160;</div>
<div class="line"><a name="l02447"></a><span class="lineno"> 2447</span>&#160;   // neural auxiliary data</div>
<div class="line"><a name="l02448"></a><span class="lineno"> 2448</span>&#160;   // lastSpikeTime: an array indicates the last time of a neuron emitting a spike</div>
<div class="line"><a name="l02449"></a><span class="lineno"> 2449</span>&#160;   // neuron firing time</div>
<div class="line"><a name="l02450"></a><span class="lineno"> 2450</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02451"></a><span class="lineno"> 2451</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;lastSpikeTime, sizeof(int) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l02452"></a><span class="lineno"> 2452</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;lastSpikeTime, managerRuntimeData.lastSpikeTime, sizeof(int) * networkConfigs[netId].numNAssigned, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02453"></a><span class="lineno"> 2453</span>&#160;</div>
<div class="line"><a name="l02454"></a><span class="lineno"> 2454</span>&#160;   // auxiliary data for recording spike count of each neuron</div>
<div class="line"><a name="l02455"></a><span class="lineno"> 2455</span>&#160;   copyNeuronSpikeCount(netId, lGrpId, dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, true, 0);</div>
<div class="line"><a name="l02456"></a><span class="lineno"> 2456</span>&#160;</div>
<div class="line"><a name="l02457"></a><span class="lineno"> 2457</span>&#160;   // quick lookup array for local group ids</div>
<div class="line"><a name="l02458"></a><span class="lineno"> 2458</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02459"></a><span class="lineno"> 2459</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc( (void**)&amp;dest-&gt;grpIds, sizeof(short int) * networkConfigs[netId].numNAssigned));</div>
<div class="line"><a name="l02460"></a><span class="lineno"> 2460</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy( dest-&gt;grpIds, managerRuntimeData.grpIds, sizeof(short int) * networkConfigs[netId].numNAssigned, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02461"></a><span class="lineno"> 2461</span>&#160;</div>
<div class="line"><a name="l02462"></a><span class="lineno"> 2462</span>&#160;   // quick lookup array for conn ids</div>
<div class="line"><a name="l02463"></a><span class="lineno"> 2463</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02464"></a><span class="lineno"> 2464</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;connIdsPreIdx, sizeof(short int) * networkConfigs[netId].numPreSynNet));</div>
<div class="line"><a name="l02465"></a><span class="lineno"> 2465</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;connIdsPreIdx, managerRuntimeData.connIdsPreIdx, sizeof(short int) * networkConfigs[netId].numPreSynNet, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02466"></a><span class="lineno"> 2466</span>&#160;</div>
<div class="line"><a name="l02467"></a><span class="lineno"> 2467</span>&#160;   // firing table</div>
<div class="line"><a name="l02468"></a><span class="lineno"> 2468</span>&#160;   if(allocateMem) {</div>
<div class="line"><a name="l02469"></a><span class="lineno"> 2469</span>&#160;       assert(dest-&gt;firingTableD1 == NULL);</div>
<div class="line"><a name="l02470"></a><span class="lineno"> 2470</span>&#160;       assert(dest-&gt;firingTableD2 == NULL);</div>
<div class="line"><a name="l02471"></a><span class="lineno"> 2471</span>&#160;   }</div>
<div class="line"><a name="l02472"></a><span class="lineno"> 2472</span>&#160;</div>
<div class="line"><a name="l02473"></a><span class="lineno"> 2473</span>&#160;   // allocate 1ms firing table</div>
<div class="line"><a name="l02474"></a><span class="lineno"> 2474</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02475"></a><span class="lineno"> 2475</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;firingTableD1, sizeof(int) * networkConfigs[netId].maxSpikesD1));</div>
<div class="line"><a name="l02476"></a><span class="lineno"> 2476</span>&#160;   if (networkConfigs[netId].maxSpikesD1 &gt; 0)</div>
<div class="line"><a name="l02477"></a><span class="lineno"> 2477</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;firingTableD1, managerRuntimeData.firingTableD1, sizeof(int) * networkConfigs[netId].maxSpikesD1, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02478"></a><span class="lineno"> 2478</span>&#160;</div>
<div class="line"><a name="l02479"></a><span class="lineno"> 2479</span>&#160;   // allocate 2+ms firing table</div>
<div class="line"><a name="l02480"></a><span class="lineno"> 2480</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l02481"></a><span class="lineno"> 2481</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;firingTableD2, sizeof(int) * networkConfigs[netId].maxSpikesD2));</div>
<div class="line"><a name="l02482"></a><span class="lineno"> 2482</span>&#160;   if (networkConfigs[netId].maxSpikesD2 &gt; 0)</div>
<div class="line"><a name="l02483"></a><span class="lineno"> 2483</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(dest-&gt;firingTableD2, managerRuntimeData.firingTableD2, sizeof(int) * networkConfigs[netId].maxSpikesD2, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02484"></a><span class="lineno"> 2484</span>&#160;</div>
<div class="line"><a name="l02485"></a><span class="lineno"> 2485</span>&#160;   // allocate external 1ms firing table</div>
<div class="line"><a name="l02486"></a><span class="lineno"> 2486</span>&#160;   if (allocateMem) {</div>
<div class="line"><a name="l02487"></a><span class="lineno"> 2487</span>&#160;       void* devPtr;</div>
<div class="line"><a name="l02488"></a><span class="lineno"> 2488</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;extFiringTableD1, sizeof(int*) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02489"></a><span class="lineno"> 2489</span>&#160;       CUDA_CHECK_ERRORS(cudaMemset(dest-&gt;extFiringTableD1, 0 /* NULL */, sizeof(int*) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02490"></a><span class="lineno"> 2490</span>&#160;       for (int lGrpId = 0; lGrpId &lt; networkConfigs[netId].numGroups; lGrpId++) {</div>
<div class="line"><a name="l02491"></a><span class="lineno"> 2491</span>&#160;           if (groupConfigs[netId][lGrpId].hasExternalConnect) {</div>
<div class="line"><a name="l02492"></a><span class="lineno"> 2492</span>&#160;               CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;devPtr, sizeof(int) * groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE));</div>
<div class="line"><a name="l02493"></a><span class="lineno"> 2493</span>&#160;               CUDA_CHECK_ERRORS(cudaMemset(devPtr, 0 , sizeof(int) * groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE));</div>
<div class="line"><a name="l02494"></a><span class="lineno"> 2494</span>&#160;               CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;extFiringTableD1[lGrpId], &amp;devPtr, sizeof(int*), cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02495"></a><span class="lineno"> 2495</span>&#160;           }</div>
<div class="line"><a name="l02496"></a><span class="lineno"> 2496</span>&#160;       }</div>
<div class="line"><a name="l02497"></a><span class="lineno"> 2497</span>&#160;   }</div>
<div class="line"><a name="l02498"></a><span class="lineno"> 2498</span>&#160;</div>
<div class="line"><a name="l02499"></a><span class="lineno"> 2499</span>&#160;   // allocate external 2+ms firing table</div>
<div class="line"><a name="l02500"></a><span class="lineno"> 2500</span>&#160;   if (allocateMem) {</div>
<div class="line"><a name="l02501"></a><span class="lineno"> 2501</span>&#160;       void* devPtr;</div>
<div class="line"><a name="l02502"></a><span class="lineno"> 2502</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;extFiringTableD2, sizeof(int*) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02503"></a><span class="lineno"> 2503</span>&#160;       CUDA_CHECK_ERRORS(cudaMemset(dest-&gt;extFiringTableD2, 0 /* NULL */, sizeof(int*) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02504"></a><span class="lineno"> 2504</span>&#160;       for (int lGrpId = 0; lGrpId &lt; networkConfigs[netId].numGroups; lGrpId++) {</div>
<div class="line"><a name="l02505"></a><span class="lineno"> 2505</span>&#160;           if (groupConfigs[netId][lGrpId].hasExternalConnect) {</div>
<div class="line"><a name="l02506"></a><span class="lineno"> 2506</span>&#160;               CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;devPtr, sizeof(int) * groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE));</div>
<div class="line"><a name="l02507"></a><span class="lineno"> 2507</span>&#160;               CUDA_CHECK_ERRORS(cudaMemset(devPtr, 0 , sizeof(int) * groupConfigs[netId][lGrpId].numN * NEURON_MAX_FIRING_RATE));</div>
<div class="line"><a name="l02508"></a><span class="lineno"> 2508</span>&#160;               CUDA_CHECK_ERRORS(cudaMemcpy(&amp;dest-&gt;extFiringTableD2[lGrpId], &amp;devPtr, sizeof(int*), cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02509"></a><span class="lineno"> 2509</span>&#160;           }</div>
<div class="line"><a name="l02510"></a><span class="lineno"> 2510</span>&#160;       }</div>
<div class="line"><a name="l02511"></a><span class="lineno"> 2511</span>&#160;   }</div>
<div class="line"><a name="l02512"></a><span class="lineno"> 2512</span>&#160;</div>
<div class="line"><a name="l02513"></a><span class="lineno"> 2513</span>&#160;   // allocate external 1ms firing table index</div>
<div class="line"><a name="l02514"></a><span class="lineno"> 2514</span>&#160;   if (allocateMem)</div>
<div class="line"><a name="l02515"></a><span class="lineno"> 2515</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;extFiringTableEndIdxD1, sizeof(int) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02516"></a><span class="lineno"> 2516</span>&#160;   CUDA_CHECK_ERRORS(cudaMemset(dest-&gt;extFiringTableEndIdxD1, 0, sizeof(int) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02517"></a><span class="lineno"> 2517</span>&#160;</div>
<div class="line"><a name="l02518"></a><span class="lineno"> 2518</span>&#160;</div>
<div class="line"><a name="l02519"></a><span class="lineno"> 2519</span>&#160;   // allocate external 2+ms firing table index</div>
<div class="line"><a name="l02520"></a><span class="lineno"> 2520</span>&#160;   if (allocateMem)</div>
<div class="line"><a name="l02521"></a><span class="lineno"> 2521</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;extFiringTableEndIdxD2, sizeof(int) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02522"></a><span class="lineno"> 2522</span>&#160;   CUDA_CHECK_ERRORS(cudaMemset(dest-&gt;extFiringTableEndIdxD2, 0, sizeof(int) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02523"></a><span class="lineno"> 2523</span>&#160;}</div>
<div class="line"><a name="l02524"></a><span class="lineno"> 2524</span>&#160;</div>
<div class="line"><a name="l02525"></a><span class="lineno"> 2525</span>&#160;void SNN::copyGrpIdsLookupArray(int netId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l02526"></a><span class="lineno"> 2526</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02527"></a><span class="lineno"> 2527</span>&#160;   checkDestSrcPtrs(&amp;managerRuntimeData, &amp;runtimeData[netId], cudaMemcpyDeviceToHost, false, ALL, 0);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02528"></a><span class="lineno"> 2528</span>&#160;   assert(kind == cudaMemcpyDeviceToHost);</div>
<div class="line"><a name="l02529"></a><span class="lineno"> 2529</span>&#160;</div>
<div class="line"><a name="l02530"></a><span class="lineno"> 2530</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(managerRuntimeData.grpIds, runtimeData[netId].grpIds, sizeof(short int) *  networkConfigs[netId].numNAssigned, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02531"></a><span class="lineno"> 2531</span>&#160;}</div>
<div class="line"><a name="l02532"></a><span class="lineno"> 2532</span>&#160;</div>
<div class="line"><a name="l02533"></a><span class="lineno"> 2533</span>&#160;void SNN::copyConnIdsLookupArray(int netId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l02534"></a><span class="lineno"> 2534</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02535"></a><span class="lineno"> 2535</span>&#160;   checkDestSrcPtrs(&amp;managerRuntimeData, &amp;runtimeData[netId], cudaMemcpyDeviceToHost, false, ALL, 0);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02536"></a><span class="lineno"> 2536</span>&#160;   assert(kind == cudaMemcpyDeviceToHost);</div>
<div class="line"><a name="l02537"></a><span class="lineno"> 2537</span>&#160;</div>
<div class="line"><a name="l02538"></a><span class="lineno"> 2538</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(managerRuntimeData.connIdsPreIdx, runtimeData[netId].connIdsPreIdx, sizeof(short int) *  networkConfigs[netId].numPreSynNet, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02539"></a><span class="lineno"> 2539</span>&#160;}</div>
<div class="line"><a name="l02540"></a><span class="lineno"> 2540</span>&#160;</div>
<div class="line"><a name="l02541"></a><span class="lineno"> 2541</span>&#160;void SNN::copyLastSpikeTime(int netId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l02542"></a><span class="lineno"> 2542</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02543"></a><span class="lineno"> 2543</span>&#160;   checkDestSrcPtrs(&amp;managerRuntimeData, &amp;runtimeData[netId], cudaMemcpyDeviceToHost, false, ALL, 0); // check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l02544"></a><span class="lineno"> 2544</span>&#160;   assert(kind == cudaMemcpyDeviceToHost);</div>
<div class="line"><a name="l02545"></a><span class="lineno"> 2545</span>&#160;</div>
<div class="line"><a name="l02546"></a><span class="lineno"> 2546</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(managerRuntimeData.lastSpikeTime, runtimeData[netId].lastSpikeTime, sizeof(int) *  networkConfigs[netId].numN, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02547"></a><span class="lineno"> 2547</span>&#160;}</div>
<div class="line"><a name="l02548"></a><span class="lineno"> 2548</span>&#160;</div>
<div class="line"><a name="l02549"></a><span class="lineno"> 2549</span>&#160;// spikeGeneratorUpdate on GPUs..</div>
<div class="line"><a name="l02550"></a><span class="lineno"> 2550</span>&#160;void SNN::spikeGeneratorUpdate_GPU(int netId) {</div>
<div class="line"><a name="l02551"></a><span class="lineno"> 2551</span>&#160;   assert(runtimeData[netId].allocated);</div>
<div class="line"><a name="l02552"></a><span class="lineno"> 2552</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02553"></a><span class="lineno"> 2553</span>&#160;   checkAndSetGPUDevice(netId);        </div>
<div class="line"><a name="l02554"></a><span class="lineno"> 2554</span>&#160;</div>
<div class="line"><a name="l02555"></a><span class="lineno"> 2555</span>&#160;   // update the random number for poisson spike generator (spikes generated by rate)</div>
<div class="line"><a name="l02556"></a><span class="lineno"> 2556</span>&#160;   if((networkConfigs[netId].numNPois &gt; 0) &amp;&amp; (runtimeData[netId].gpuRandGen != NULL)) {</div>
<div class="line"><a name="l02557"></a><span class="lineno"> 2557</span>&#160;       curandGenerateUniform(runtimeData[netId].gpuRandGen, runtimeData[netId].randNum, networkConfigs[netId].numNPois);</div>
<div class="line"><a name="l02558"></a><span class="lineno"> 2558</span>&#160;   }</div>
<div class="line"><a name="l02559"></a><span class="lineno"> 2559</span>&#160;</div>
<div class="line"><a name="l02560"></a><span class="lineno"> 2560</span>&#160;   // Use spike generators (user-defined callback function)</div>
<div class="line"><a name="l02561"></a><span class="lineno"> 2561</span>&#160;   if (networkConfigs[netId].numNSpikeGen &gt; 0) {</div>
<div class="line"><a name="l02562"></a><span class="lineno"> 2562</span>&#160;       assert(managerRuntimeData.spikeGenBits != NULL);</div>
<div class="line"><a name="l02563"></a><span class="lineno"> 2563</span>&#160;</div>
<div class="line"><a name="l02564"></a><span class="lineno"> 2564</span>&#160;       // reset the bit status of the spikeGenBits...</div>
<div class="line"><a name="l02565"></a><span class="lineno"> 2565</span>&#160;       memset(managerRuntimeData.spikeGenBits, 0, sizeof(int) * (networkConfigs[netId].numNSpikeGen / 32 + 1));</div>
<div class="line"><a name="l02566"></a><span class="lineno"> 2566</span>&#160;</div>
<div class="line"><a name="l02567"></a><span class="lineno"> 2567</span>&#160;       // fill spikeGenBits from SpikeBuffer</div>
<div class="line"><a name="l02568"></a><span class="lineno"> 2568</span>&#160;       fillSpikeGenBits(netId);</div>
<div class="line"><a name="l02569"></a><span class="lineno"> 2569</span>&#160;</div>
<div class="line"><a name="l02570"></a><span class="lineno"> 2570</span>&#160;       // copy the spikeGenBits from the manager to the GPU..</div>
<div class="line"><a name="l02571"></a><span class="lineno"> 2571</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpy(runtimeData[netId].spikeGenBits, managerRuntimeData.spikeGenBits, sizeof(int) * (networkConfigs[netId].numNSpikeGen / 32 + 1), cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02572"></a><span class="lineno"> 2572</span>&#160;   }</div>
<div class="line"><a name="l02573"></a><span class="lineno"> 2573</span>&#160;}</div>
<div class="line"><a name="l02574"></a><span class="lineno"> 2574</span>&#160;</div>
<div class="line"><a name="l02575"></a><span class="lineno"> 2575</span>&#160;void SNN::findFiring_GPU(int netId) {</div>
<div class="line"><a name="l02576"></a><span class="lineno"> 2576</span>&#160;   assert(runtimeData[netId].allocated);</div>
<div class="line"><a name="l02577"></a><span class="lineno"> 2577</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02578"></a><span class="lineno"> 2578</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02579"></a><span class="lineno"> 2579</span>&#160;</div>
<div class="line"><a name="l02580"></a><span class="lineno"> 2580</span>&#160;   kernel_findFiring&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(simTime);</div>
<div class="line"><a name="l02581"></a><span class="lineno"> 2581</span>&#160;   CUDA_GET_LAST_ERROR(&quot;findFiring kernel failed\n&quot;);</div>
<div class="line"><a name="l02582"></a><span class="lineno"> 2582</span>&#160;}</div>
<div class="line"><a name="l02583"></a><span class="lineno"> 2583</span>&#160;</div>
<div class="line"><a name="l02584"></a><span class="lineno"> 2584</span>&#160;void SNN::updateTimingTable_GPU(int netId) {</div>
<div class="line"><a name="l02585"></a><span class="lineno"> 2585</span>&#160;   assert(runtimeData[netId].allocated);</div>
<div class="line"><a name="l02586"></a><span class="lineno"> 2586</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02587"></a><span class="lineno"> 2587</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02588"></a><span class="lineno"> 2588</span>&#160;</div>
<div class="line"><a name="l02589"></a><span class="lineno"> 2589</span>&#160;   kernel_updateTimeTable&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(simTimeMs);</div>
<div class="line"><a name="l02590"></a><span class="lineno"> 2590</span>&#160;   CUDA_GET_LAST_ERROR(&quot;timing Table update kernel failed\n&quot;);</div>
<div class="line"><a name="l02591"></a><span class="lineno"> 2591</span>&#160;}</div>
<div class="line"><a name="l02592"></a><span class="lineno"> 2592</span>&#160;</div>
<div class="line"><a name="l02593"></a><span class="lineno"> 2593</span>&#160;void SNN::doCurrentUpdateD2_GPU(int netId) {</div>
<div class="line"><a name="l02594"></a><span class="lineno"> 2594</span>&#160;   assert(runtimeData[netId].allocated);</div>
<div class="line"><a name="l02595"></a><span class="lineno"> 2595</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02596"></a><span class="lineno"> 2596</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02597"></a><span class="lineno"> 2597</span>&#160;</div>
<div class="line"><a name="l02598"></a><span class="lineno"> 2598</span>&#160;   if (networkConfigs[netId].maxDelay &gt; 1) {</div>
<div class="line"><a name="l02599"></a><span class="lineno"> 2599</span>&#160;       kernel_doCurrentUpdateD2 &lt;&lt; &lt;NUM_BLOCKS, NUM_THREADS &gt;&gt; &gt; (simTimeMs, simTimeSec, simTime);</div>
<div class="line"><a name="l02600"></a><span class="lineno"> 2600</span>&#160;       CUDA_GET_LAST_ERROR(&quot;Kernel execution failed&quot;);</div>
<div class="line"><a name="l02601"></a><span class="lineno"> 2601</span>&#160;   }</div>
<div class="line"><a name="l02602"></a><span class="lineno"> 2602</span>&#160;}</div>
<div class="line"><a name="l02603"></a><span class="lineno"> 2603</span>&#160;</div>
<div class="line"><a name="l02604"></a><span class="lineno"> 2604</span>&#160;void SNN::doCurrentUpdateD1_GPU(int netId) {</div>
<div class="line"><a name="l02605"></a><span class="lineno"> 2605</span>&#160;   assert(runtimeData[netId].allocated);</div>
<div class="line"><a name="l02606"></a><span class="lineno"> 2606</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02607"></a><span class="lineno"> 2607</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02608"></a><span class="lineno"> 2608</span>&#160;</div>
<div class="line"><a name="l02609"></a><span class="lineno"> 2609</span>&#160;   kernel_doCurrentUpdateD1&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(simTimeMs,simTimeSec,simTime);</div>
<div class="line"><a name="l02610"></a><span class="lineno"> 2610</span>&#160;   CUDA_GET_LAST_ERROR(&quot;Kernel execution failed&quot;);</div>
<div class="line"><a name="l02611"></a><span class="lineno"> 2611</span>&#160;}</div>
<div class="line"><a name="l02612"></a><span class="lineno"> 2612</span>&#160;</div>
<div class="line"><a name="l02613"></a><span class="lineno"> 2613</span>&#160;void SNN::doSTPUpdateAndDecayCond_GPU(int netId) {</div>
<div class="line"><a name="l02614"></a><span class="lineno"> 2614</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02615"></a><span class="lineno"> 2615</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02616"></a><span class="lineno"> 2616</span>&#160;           </div>
<div class="line"><a name="l02617"></a><span class="lineno"> 2617</span>&#160;   if (sim_with_stp || sim_with_conductances) {</div>
<div class="line"><a name="l02618"></a><span class="lineno"> 2618</span>&#160;       kernel_STPUpdateAndDecayConductances&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(simTimeMs, simTimeSec, simTime);</div>
<div class="line"><a name="l02619"></a><span class="lineno"> 2619</span>&#160;       CUDA_GET_LAST_ERROR(&quot;STP update\n&quot;);</div>
<div class="line"><a name="l02620"></a><span class="lineno"> 2620</span>&#160;   }</div>
<div class="line"><a name="l02621"></a><span class="lineno"> 2621</span>&#160;}</div>
<div class="line"><a name="l02622"></a><span class="lineno"> 2622</span>&#160;</div>
<div class="line"><a name="l02623"></a><span class="lineno"> 2623</span>&#160;void SNN::initGPU(int netId) {</div>
<div class="line"><a name="l02624"></a><span class="lineno"> 2624</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02625"></a><span class="lineno"> 2625</span>&#160;</div>
<div class="line"><a name="l02626"></a><span class="lineno"> 2626</span>&#160;   assert(runtimeData[netId].allocated);</div>
<div class="line"><a name="l02627"></a><span class="lineno"> 2627</span>&#160;</div>
<div class="line"><a name="l02628"></a><span class="lineno"> 2628</span>&#160;   kernel_initGPUMemory&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;();</div>
<div class="line"><a name="l02629"></a><span class="lineno"> 2629</span>&#160;   CUDA_GET_LAST_ERROR(&quot;initGPUMemory kernel failed\n&quot;);</div>
<div class="line"><a name="l02630"></a><span class="lineno"> 2630</span>&#160;}</div>
<div class="line"><a name="l02631"></a><span class="lineno"> 2631</span>&#160;</div>
<div class="line"><a name="l02632"></a><span class="lineno"> 2632</span>&#160;void SNN::deleteRuntimeData_GPU(int netId) {</div>
<div class="line"><a name="l02633"></a><span class="lineno"> 2633</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02634"></a><span class="lineno"> 2634</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02635"></a><span class="lineno"> 2635</span>&#160;</div>
<div class="line"><a name="l02636"></a><span class="lineno"> 2636</span>&#160;   // cudaFree all device pointers</div>
<div class="line"><a name="l02637"></a><span class="lineno"> 2637</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].voltage) );</div>
<div class="line"><a name="l02638"></a><span class="lineno"> 2638</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].recovery) );</div>
<div class="line"><a name="l02639"></a><span class="lineno"> 2639</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].current) );</div>
<div class="line"><a name="l02640"></a><span class="lineno"> 2640</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].extCurrent) );</div>
<div class="line"><a name="l02641"></a><span class="lineno"> 2641</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Npre) );</div>
<div class="line"><a name="l02642"></a><span class="lineno"> 2642</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Npre_plastic) );</div>
<div class="line"><a name="l02643"></a><span class="lineno"> 2643</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Npre_plasticInv) );</div>
<div class="line"><a name="l02644"></a><span class="lineno"> 2644</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Npost) );</div>
<div class="line"><a name="l02645"></a><span class="lineno"> 2645</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].cumulativePost) );</div>
<div class="line"><a name="l02646"></a><span class="lineno"> 2646</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].cumulativePre) );</div>
<div class="line"><a name="l02647"></a><span class="lineno"> 2647</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].synSpikeTime) );</div>
<div class="line"><a name="l02648"></a><span class="lineno"> 2648</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].wt) );</div>
<div class="line"><a name="l02649"></a><span class="lineno"> 2649</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].wtChange) );</div>
<div class="line"><a name="l02650"></a><span class="lineno"> 2650</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].maxSynWt) );</div>
<div class="line"><a name="l02651"></a><span class="lineno"> 2651</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].nSpikeCnt) );</div>
<div class="line"><a name="l02652"></a><span class="lineno"> 2652</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].avgFiring) );</div>
<div class="line"><a name="l02653"></a><span class="lineno"> 2653</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].baseFiring) );</div>
<div class="line"><a name="l02654"></a><span class="lineno"> 2654</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].baseFiringInv) );</div>
<div class="line"><a name="l02655"></a><span class="lineno"> 2655</span>&#160;</div>
<div class="line"><a name="l02656"></a><span class="lineno"> 2656</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grpDA) );</div>
<div class="line"><a name="l02657"></a><span class="lineno"> 2657</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grp5HT) );</div>
<div class="line"><a name="l02658"></a><span class="lineno"> 2658</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grpACh) );</div>
<div class="line"><a name="l02659"></a><span class="lineno"> 2659</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grpNE) );</div>
<div class="line"><a name="l02660"></a><span class="lineno"> 2660</span>&#160;</div>
<div class="line"><a name="l02661"></a><span class="lineno"> 2661</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grpDABuffer) );</div>
<div class="line"><a name="l02662"></a><span class="lineno"> 2662</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grp5HTBuffer) );</div>
<div class="line"><a name="l02663"></a><span class="lineno"> 2663</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grpAChBuffer) );</div>
<div class="line"><a name="l02664"></a><span class="lineno"> 2664</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grpNEBuffer) );</div>
<div class="line"><a name="l02665"></a><span class="lineno"> 2665</span>&#160;</div>
<div class="line"><a name="l02666"></a><span class="lineno"> 2666</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].grpIds) );</div>
<div class="line"><a name="l02667"></a><span class="lineno"> 2667</span>&#160;</div>
<div class="line"><a name="l02668"></a><span class="lineno"> 2668</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Izh_a) );</div>
<div class="line"><a name="l02669"></a><span class="lineno"> 2669</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Izh_b) );</div>
<div class="line"><a name="l02670"></a><span class="lineno"> 2670</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Izh_c) );</div>
<div class="line"><a name="l02671"></a><span class="lineno"> 2671</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].Izh_d) );</div>
<div class="line"><a name="l02672"></a><span class="lineno"> 2672</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gAMPA) );</div>
<div class="line"><a name="l02673"></a><span class="lineno"> 2673</span>&#160;   if (sim_with_NMDA_rise) {</div>
<div class="line"><a name="l02674"></a><span class="lineno"> 2674</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gNMDA_r) );</div>
<div class="line"><a name="l02675"></a><span class="lineno"> 2675</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gNMDA_d) );</div>
<div class="line"><a name="l02676"></a><span class="lineno"> 2676</span>&#160;   } else {</div>
<div class="line"><a name="l02677"></a><span class="lineno"> 2677</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gNMDA) );</div>
<div class="line"><a name="l02678"></a><span class="lineno"> 2678</span>&#160;   }</div>
<div class="line"><a name="l02679"></a><span class="lineno"> 2679</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gGABAa) );</div>
<div class="line"><a name="l02680"></a><span class="lineno"> 2680</span>&#160;   if (sim_with_GABAb_rise) {</div>
<div class="line"><a name="l02681"></a><span class="lineno"> 2681</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gGABAb_r) );</div>
<div class="line"><a name="l02682"></a><span class="lineno"> 2682</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gGABAb_d) );</div>
<div class="line"><a name="l02683"></a><span class="lineno"> 2683</span>&#160;   } else {</div>
<div class="line"><a name="l02684"></a><span class="lineno"> 2684</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].gGABAb) );</div>
<div class="line"><a name="l02685"></a><span class="lineno"> 2685</span>&#160;   }</div>
<div class="line"><a name="l02686"></a><span class="lineno"> 2686</span>&#160;</div>
<div class="line"><a name="l02687"></a><span class="lineno"> 2687</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].stpu) );</div>
<div class="line"><a name="l02688"></a><span class="lineno"> 2688</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].stpx) );</div>
<div class="line"><a name="l02689"></a><span class="lineno"> 2689</span>&#160;</div>
<div class="line"><a name="l02690"></a><span class="lineno"> 2690</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].connIdsPreIdx) );</div>
<div class="line"><a name="l02691"></a><span class="lineno"> 2691</span>&#160;</div>
<div class="line"><a name="l02692"></a><span class="lineno"> 2692</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].groupIdInfo) );</div>
<div class="line"><a name="l02693"></a><span class="lineno"> 2693</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].neuronAllocation) );</div>
<div class="line"><a name="l02694"></a><span class="lineno"> 2694</span>&#160;</div>
<div class="line"><a name="l02695"></a><span class="lineno"> 2695</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].postDelayInfo) );</div>
<div class="line"><a name="l02696"></a><span class="lineno"> 2696</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].postSynapticIds) );</div>
<div class="line"><a name="l02697"></a><span class="lineno"> 2697</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].preSynapticIds) );</div>
<div class="line"><a name="l02698"></a><span class="lineno"> 2698</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].I_set) );</div>
<div class="line"><a name="l02699"></a><span class="lineno"> 2699</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].poissonFireRate) );</div>
<div class="line"><a name="l02700"></a><span class="lineno"> 2700</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].lastSpikeTime) );</div>
<div class="line"><a name="l02701"></a><span class="lineno"> 2701</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].spikeGenBits) );</div>
<div class="line"><a name="l02702"></a><span class="lineno"> 2702</span>&#160;</div>
<div class="line"><a name="l02703"></a><span class="lineno"> 2703</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].firingTableD2) );</div>
<div class="line"><a name="l02704"></a><span class="lineno"> 2704</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].firingTableD1) );</div>
<div class="line"><a name="l02705"></a><span class="lineno"> 2705</span>&#160;</div>
<div class="line"><a name="l02706"></a><span class="lineno"> 2706</span>&#160;   int** tempPtrs;</div>
<div class="line"><a name="l02707"></a><span class="lineno"> 2707</span>&#160;   tempPtrs = new int*[networkConfigs[netId].numGroups];</div>
<div class="line"><a name="l02708"></a><span class="lineno"> 2708</span>&#160;</div>
<div class="line"><a name="l02709"></a><span class="lineno"> 2709</span>&#160;   // fetch device memory address stored in extFiringTableD2</div>
<div class="line"><a name="l02710"></a><span class="lineno"> 2710</span>&#160;   CUDA_CHECK_ERRORS( cudaMemcpy(tempPtrs, runtimeData[netId].extFiringTableD2, sizeof(int*) * networkConfigs[netId].numGroups, cudaMemcpyDeviceToHost) );</div>
<div class="line"><a name="l02711"></a><span class="lineno"> 2711</span>&#160;   for (int i = 0; i &lt; networkConfigs[netId].numGroups; i++)</div>
<div class="line"><a name="l02712"></a><span class="lineno"> 2712</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(tempPtrs[i]) );</div>
<div class="line"><a name="l02713"></a><span class="lineno"> 2713</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].extFiringTableD2) );</div>
<div class="line"><a name="l02714"></a><span class="lineno"> 2714</span>&#160;</div>
<div class="line"><a name="l02715"></a><span class="lineno"> 2715</span>&#160;   // fetch device memory address stored in extFiringTableD1</div>
<div class="line"><a name="l02716"></a><span class="lineno"> 2716</span>&#160;   CUDA_CHECK_ERRORS( cudaMemcpy(tempPtrs, runtimeData[netId].extFiringTableD1, sizeof(int*) * networkConfigs[netId].numGroups, cudaMemcpyDeviceToHost) );</div>
<div class="line"><a name="l02717"></a><span class="lineno"> 2717</span>&#160;   for (int i = 0; i &lt; networkConfigs[netId].numGroups; i++)</div>
<div class="line"><a name="l02718"></a><span class="lineno"> 2718</span>&#160;       CUDA_CHECK_ERRORS( cudaFree(tempPtrs[i]) );</div>
<div class="line"><a name="l02719"></a><span class="lineno"> 2719</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].extFiringTableD1) );</div>
<div class="line"><a name="l02720"></a><span class="lineno"> 2720</span>&#160;</div>
<div class="line"><a name="l02721"></a><span class="lineno"> 2721</span>&#160;   delete[] tempPtrs;</div>
<div class="line"><a name="l02722"></a><span class="lineno"> 2722</span>&#160;</div>
<div class="line"><a name="l02723"></a><span class="lineno"> 2723</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].extFiringTableEndIdxD2) );</div>
<div class="line"><a name="l02724"></a><span class="lineno"> 2724</span>&#160;   CUDA_CHECK_ERRORS( cudaFree(runtimeData[netId].extFiringTableEndIdxD1) );</div>
<div class="line"><a name="l02725"></a><span class="lineno"> 2725</span>&#160;</div>
<div class="line"><a name="l02726"></a><span class="lineno"> 2726</span>&#160;   // delete random numbr generator on GPU(s)</div>
<div class="line"><a name="l02727"></a><span class="lineno"> 2727</span>&#160;   // Note: RNG_rand48 objects allocate device memory</div>
<div class="line"><a name="l02728"></a><span class="lineno"> 2728</span>&#160;   if (runtimeData[netId].gpuRandGen != NULL) curandDestroyGenerator(runtimeData[netId].gpuRandGen);</div>
<div class="line"><a name="l02729"></a><span class="lineno"> 2729</span>&#160;   runtimeData[netId].gpuRandGen = NULL;</div>
<div class="line"><a name="l02730"></a><span class="lineno"> 2730</span>&#160;</div>
<div class="line"><a name="l02731"></a><span class="lineno"> 2731</span>&#160;   if (runtimeData[netId].randNum != NULL) CUDA_CHECK_ERRORS(cudaFree(runtimeData[netId].randNum));</div>
<div class="line"><a name="l02732"></a><span class="lineno"> 2732</span>&#160;   runtimeData[netId].randNum = NULL;</div>
<div class="line"><a name="l02733"></a><span class="lineno"> 2733</span>&#160;}</div>
<div class="line"><a name="l02734"></a><span class="lineno"> 2734</span>&#160;</div>
<div class="line"><a name="l02735"></a><span class="lineno"> 2735</span>&#160;void SNN::globalStateUpdate_C_GPU(int netId) {</div>
<div class="line"><a name="l02736"></a><span class="lineno"> 2736</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02737"></a><span class="lineno"> 2737</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02738"></a><span class="lineno"> 2738</span>&#160;</div>
<div class="line"><a name="l02739"></a><span class="lineno"> 2739</span>&#160;   kernel_conductanceUpdate &lt;&lt; &lt;NUM_BLOCKS, NUM_THREADS &gt;&gt; &gt; (simTimeMs, simTimeSec, simTime);</div>
<div class="line"><a name="l02740"></a><span class="lineno"> 2740</span>&#160;   CUDA_GET_LAST_ERROR(&quot;kernel_conductanceUpdate failed&quot;);</div>
<div class="line"><a name="l02741"></a><span class="lineno"> 2741</span>&#160;</div>
<div class="line"><a name="l02742"></a><span class="lineno"> 2742</span>&#160;   // use memset to reset I_set for debugging, resume it later</div>
<div class="line"><a name="l02743"></a><span class="lineno"> 2743</span>&#160;   //CUDA_CHECK_ERRORS(cudaMemset(runtimeData[netId].I_set, 0, networkConfigs[netId].I_setPitch * networkConfigs[netId].I_setLength));</div>
<div class="line"><a name="l02744"></a><span class="lineno"> 2744</span>&#160;}</div>
<div class="line"><a name="l02745"></a><span class="lineno"> 2745</span>&#160;</div>
<div class="line"><a name="l02746"></a><span class="lineno"> 2746</span>&#160;void SNN::globalStateUpdate_N_GPU(int netId) {</div>
<div class="line"><a name="l02747"></a><span class="lineno"> 2747</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02748"></a><span class="lineno"> 2748</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02749"></a><span class="lineno"> 2749</span>&#160;   </div>
<div class="line"><a name="l02750"></a><span class="lineno"> 2750</span>&#160;   // update all neuron state (i.e., voltage and recovery), including homeostasis</div>
<div class="line"><a name="l02751"></a><span class="lineno"> 2751</span>&#160;   kernel_neuronStateUpdate &lt;&lt; &lt;NUM_BLOCKS, NUM_THREADS &gt;&gt; &gt; ();</div>
<div class="line"><a name="l02752"></a><span class="lineno"> 2752</span>&#160;   CUDA_GET_LAST_ERROR(&quot;Kernel execution failed&quot;);</div>
<div class="line"><a name="l02753"></a><span class="lineno"> 2753</span>&#160;}</div>
<div class="line"><a name="l02754"></a><span class="lineno"> 2754</span>&#160;</div>
<div class="line"><a name="l02755"></a><span class="lineno"> 2755</span>&#160;void SNN::globalStateUpdate_G_GPU(int netId) {</div>
<div class="line"><a name="l02756"></a><span class="lineno"> 2756</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02757"></a><span class="lineno"> 2757</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02758"></a><span class="lineno"> 2758</span>&#160;</div>
<div class="line"><a name="l02759"></a><span class="lineno"> 2759</span>&#160;   // update all group state (i.e., concentration of neuronmodulators)</div>
<div class="line"><a name="l02760"></a><span class="lineno"> 2760</span>&#160;   // currently support 4 x 128 groups</div>
<div class="line"><a name="l02761"></a><span class="lineno"> 2761</span>&#160;   kernel_groupStateUpdate&lt;&lt;&lt;4, NUM_THREADS&gt;&gt;&gt;(simTimeMs);</div>
<div class="line"><a name="l02762"></a><span class="lineno"> 2762</span>&#160;   CUDA_GET_LAST_ERROR(&quot;Kernel execution failed&quot;);</div>
<div class="line"><a name="l02763"></a><span class="lineno"> 2763</span>&#160;}</div>
<div class="line"><a name="l02764"></a><span class="lineno"> 2764</span>&#160;</div>
<div class="line"><a name="l02765"></a><span class="lineno"> 2765</span>&#160;void SNN::assignPoissonFiringRate_GPU(int netId) {</div>
<div class="line"><a name="l02766"></a><span class="lineno"> 2766</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02767"></a><span class="lineno"> 2767</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02768"></a><span class="lineno"> 2768</span>&#160;</div>
<div class="line"><a name="l02769"></a><span class="lineno"> 2769</span>&#160;   for (int lGrpId = 0; lGrpId &lt; networkConfigs[netId].numGroups; lGrpId++) {</div>
<div class="line"><a name="l02770"></a><span class="lineno"> 2770</span>&#160;       // given group of neurons belong to the poisson group....</div>
<div class="line"><a name="l02771"></a><span class="lineno"> 2771</span>&#160;       if (groupConfigs[netId][lGrpId].isSpikeGenerator) {</div>
<div class="line"><a name="l02772"></a><span class="lineno"> 2772</span>&#160;           int lNId = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l02773"></a><span class="lineno"> 2773</span>&#160;           int gGrpId = groupConfigs[netId][lGrpId].gGrpId;</div>
<div class="line"><a name="l02774"></a><span class="lineno"> 2774</span>&#160;           PoissonRate* rate = groupConfigMDMap[gGrpId].ratePtr;</div>
<div class="line"><a name="l02775"></a><span class="lineno"> 2775</span>&#160;</div>
<div class="line"><a name="l02776"></a><span class="lineno"> 2776</span>&#160;           // if spikeGenFunc group does not have a Poisson pointer, skip</div>
<div class="line"><a name="l02777"></a><span class="lineno"> 2777</span>&#160;           if (groupConfigMap[gGrpId].spikeGenFunc || rate == NULL)</div>
<div class="line"><a name="l02778"></a><span class="lineno"> 2778</span>&#160;               continue;</div>
<div class="line"><a name="l02779"></a><span class="lineno"> 2779</span>&#160;</div>
<div class="line"><a name="l02780"></a><span class="lineno"> 2780</span>&#160;           assert(runtimeData[netId].poissonFireRate != NULL);</div>
<div class="line"><a name="l02781"></a><span class="lineno"> 2781</span>&#160;           if (rate-&gt;isOnGPU()) {</div>
<div class="line"><a name="l02782"></a><span class="lineno"> 2782</span>&#160;               // rates allocated on GPU</div>
<div class="line"><a name="l02783"></a><span class="lineno"> 2783</span>&#160;               CUDA_CHECK_ERRORS(cudaMemcpy(&amp;runtimeData[netId].poissonFireRate[lNId - networkConfigs[netId].numNReg], rate-&gt;getRatePtrGPU(),</div>
<div class="line"><a name="l02784"></a><span class="lineno"> 2784</span>&#160;                   sizeof(float) * rate-&gt;getNumNeurons(), cudaMemcpyDeviceToDevice) );</div>
<div class="line"><a name="l02785"></a><span class="lineno"> 2785</span>&#160;           } else {</div>
<div class="line"><a name="l02786"></a><span class="lineno"> 2786</span>&#160;               // rates allocated on CPU</div>
<div class="line"><a name="l02787"></a><span class="lineno"> 2787</span>&#160;               CUDA_CHECK_ERRORS(cudaMemcpy(&amp;runtimeData[netId].poissonFireRate[lNId - networkConfigs[netId].numNReg], rate-&gt;getRatePtrCPU(),</div>
<div class="line"><a name="l02788"></a><span class="lineno"> 2788</span>&#160;                   sizeof(float) * rate-&gt;getNumNeurons(), cudaMemcpyHostToDevice) );</div>
<div class="line"><a name="l02789"></a><span class="lineno"> 2789</span>&#160;           }</div>
<div class="line"><a name="l02790"></a><span class="lineno"> 2790</span>&#160;       }</div>
<div class="line"><a name="l02791"></a><span class="lineno"> 2791</span>&#160;   }</div>
<div class="line"><a name="l02792"></a><span class="lineno"> 2792</span>&#160;}</div>
<div class="line"><a name="l02793"></a><span class="lineno"> 2793</span>&#160;</div>
<div class="line"><a name="l02794"></a><span class="lineno"> 2794</span>&#160;// Note: for temporarily use, might be merged into exchangeExternalSpike</div>
<div class="line"><a name="l02795"></a><span class="lineno"> 2795</span>&#160;void SNN::clearExtFiringTable_GPU(int netId) {</div>
<div class="line"><a name="l02796"></a><span class="lineno"> 2796</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02797"></a><span class="lineno"> 2797</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02798"></a><span class="lineno"> 2798</span>&#160;</div>
<div class="line"><a name="l02799"></a><span class="lineno"> 2799</span>&#160;   CUDA_CHECK_ERRORS(cudaMemset(runtimeData[netId].extFiringTableEndIdxD1, 0, sizeof(int) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02800"></a><span class="lineno"> 2800</span>&#160;   CUDA_CHECK_ERRORS(cudaMemset(runtimeData[netId].extFiringTableEndIdxD2, 0, sizeof(int) * networkConfigs[netId].numGroups));</div>
<div class="line"><a name="l02801"></a><span class="lineno"> 2801</span>&#160;}</div>
<div class="line"><a name="l02802"></a><span class="lineno"> 2802</span>&#160;</div>
<div class="line"><a name="l02803"></a><span class="lineno"> 2803</span>&#160;//void SNN::routeSpikes_GPU() {</div>
<div class="line"><a name="l02804"></a><span class="lineno"> 2804</span>&#160;// int firingTableIdxD2, firingTableIdxD1;</div>
<div class="line"><a name="l02805"></a><span class="lineno"> 2805</span>&#160;// int GtoLOffset;</div>
<div class="line"><a name="l02806"></a><span class="lineno"> 2806</span>&#160;// // ToDo: route spikes using routing table. currently only exchange spikes between GPU0 and GPU1</div>
<div class="line"><a name="l02807"></a><span class="lineno"> 2807</span>&#160;// // GPU0 -&gt; GPU1</div>
<div class="line"><a name="l02808"></a><span class="lineno"> 2808</span>&#160;// if (!groupPartitionLists[0].empty() &amp;&amp; !groupPartitionLists[1].empty()) {</div>
<div class="line"><a name="l02809"></a><span class="lineno"> 2809</span>&#160;//     checkAndSetGPUDevice(0);</div>
<div class="line"><a name="l02810"></a><span class="lineno"> 2810</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD2, runtimeData[0].extFiringTableEndIdxD2, sizeof(int) * networkConfigs[0].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02811"></a><span class="lineno"> 2811</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD1, runtimeData[0].extFiringTableEndIdxD1, sizeof(int) * networkConfigs[0].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02812"></a><span class="lineno"> 2812</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableD2, runtimeData[0].extFiringTableD2, sizeof(int*) * networkConfigs[0].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02813"></a><span class="lineno"> 2813</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableD1, runtimeData[0].extFiringTableD1, sizeof(int*) * networkConfigs[0].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02814"></a><span class="lineno"> 2814</span>&#160;//     //KERNEL_DEBUG(&quot;GPU0 D1ex:%d/D2ex:%d&quot;, managerRuntimeData.extFiringTableEndIdxD1[0], managerRuntimeData.extFiringTableEndIdxD2[0]);</div>
<div class="line"><a name="l02815"></a><span class="lineno"> 2815</span>&#160;//</div>
<div class="line"><a name="l02816"></a><span class="lineno"> 2816</span>&#160;//     checkAndSetGPUDevice(1);</div>
<div class="line"><a name="l02817"></a><span class="lineno"> 2817</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD2, timeTableD2GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02818"></a><span class="lineno"> 2818</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD1, timeTableD1GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02819"></a><span class="lineno"> 2819</span>&#160;//     firingTableIdxD2 = managerRuntimeData.timeTableD2[simTimeMs + glbNetworkConfig.maxDelay + 1];</div>
<div class="line"><a name="l02820"></a><span class="lineno"> 2820</span>&#160;//     firingTableIdxD1 = managerRuntimeData.timeTableD1[simTimeMs + glbNetworkConfig.maxDelay + 1];</div>
<div class="line"><a name="l02821"></a><span class="lineno"> 2821</span>&#160;//     //KERNEL_DEBUG(&quot;GPU1 D1:%d/D2:%d&quot;, firingTableIdxD1, firingTableIdxD2);</div>
<div class="line"><a name="l02822"></a><span class="lineno"> 2822</span>&#160;//</div>
<div class="line"><a name="l02823"></a><span class="lineno"> 2823</span>&#160;//     for (int lGrpId = 0; lGrpId &lt; networkConfigs[0].numGroups; lGrpId++) {</div>
<div class="line"><a name="l02824"></a><span class="lineno"> 2824</span>&#160;//         if (groupConfigs[0][lGrpId].hasExternalConnect &amp;&amp; managerRuntimeData.extFiringTableEndIdxD2[lGrpId] &gt; 0) {</div>
<div class="line"><a name="l02825"></a><span class="lineno"> 2825</span>&#160;//             CUDA_CHECK_ERRORS( cudaMemcpyPeer(runtimeData[1].firingTableD2 + firingTableIdxD2, 1,</div>
<div class="line"><a name="l02826"></a><span class="lineno"> 2826</span>&#160;//                                               managerRuntimeData.extFiringTableD2[lGrpId], 0,</div>
<div class="line"><a name="l02827"></a><span class="lineno"> 2827</span>&#160;//                                               sizeof(int) * managerRuntimeData.extFiringTableEndIdxD2[lGrpId]));</div>
<div class="line"><a name="l02828"></a><span class="lineno"> 2828</span>&#160;//</div>
<div class="line"><a name="l02829"></a><span class="lineno"> 2829</span>&#160;//             for (std::list&lt;GroupConfigMD&gt;::iterator grpIt = groupPartitionLists[1].begin(); grpIt != groupPartitionLists[1].end(); grpIt++) {</div>
<div class="line"><a name="l02830"></a><span class="lineno"> 2830</span>&#160;//                 if (grpIt-&gt;gGrpId == groupConfigs[0][lGrpId].gGrpId)</div>
<div class="line"><a name="l02831"></a><span class="lineno"> 2831</span>&#160;//                     GtoLOffset = grpIt-&gt;GtoLOffset;</div>
<div class="line"><a name="l02832"></a><span class="lineno"> 2832</span>&#160;//             }</div>
<div class="line"><a name="l02833"></a><span class="lineno"> 2833</span>&#160;//</div>
<div class="line"><a name="l02834"></a><span class="lineno"> 2834</span>&#160;//             kernel_convertExtSpikesD2&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(firingTableIdxD2,</div>
<div class="line"><a name="l02835"></a><span class="lineno"> 2835</span>&#160;//                                                                    firingTableIdxD2 + managerRuntimeData.extFiringTableEndIdxD2[lGrpId],</div>
<div class="line"><a name="l02836"></a><span class="lineno"> 2836</span>&#160;//                                                                    GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l02837"></a><span class="lineno"> 2837</span>&#160;//             firingTableIdxD2 += managerRuntimeData.extFiringTableEndIdxD2[lGrpId];</div>
<div class="line"><a name="l02838"></a><span class="lineno"> 2838</span>&#160;//         }</div>
<div class="line"><a name="l02839"></a><span class="lineno"> 2839</span>&#160;//</div>
<div class="line"><a name="l02840"></a><span class="lineno"> 2840</span>&#160;//         if (groupConfigs[0][lGrpId].hasExternalConnect &amp;&amp; managerRuntimeData.extFiringTableEndIdxD1[lGrpId] &gt; 0) {</div>
<div class="line"><a name="l02841"></a><span class="lineno"> 2841</span>&#160;//             CUDA_CHECK_ERRORS( cudaMemcpyPeer(runtimeData[1].firingTableD1 + firingTableIdxD1, 1,</div>
<div class="line"><a name="l02842"></a><span class="lineno"> 2842</span>&#160;//                                               managerRuntimeData.extFiringTableD1[lGrpId], 0,</div>
<div class="line"><a name="l02843"></a><span class="lineno"> 2843</span>&#160;//                                               sizeof(int) * managerRuntimeData.extFiringTableEndIdxD1[lGrpId]));</div>
<div class="line"><a name="l02844"></a><span class="lineno"> 2844</span>&#160;//</div>
<div class="line"><a name="l02845"></a><span class="lineno"> 2845</span>&#160;//             for (std::list&lt;GroupConfigMD&gt;::iterator grpIt = groupPartitionLists[1].begin(); grpIt != groupPartitionLists[1].end(); grpIt++) {</div>
<div class="line"><a name="l02846"></a><span class="lineno"> 2846</span>&#160;//                 if (grpIt-&gt;gGrpId == groupConfigs[0][lGrpId].gGrpId)</div>
<div class="line"><a name="l02847"></a><span class="lineno"> 2847</span>&#160;//                     GtoLOffset = grpIt-&gt;GtoLOffset;</div>
<div class="line"><a name="l02848"></a><span class="lineno"> 2848</span>&#160;//             }</div>
<div class="line"><a name="l02849"></a><span class="lineno"> 2849</span>&#160;//</div>
<div class="line"><a name="l02850"></a><span class="lineno"> 2850</span>&#160;//             kernel_convertExtSpikesD1&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(firingTableIdxD1,</div>
<div class="line"><a name="l02851"></a><span class="lineno"> 2851</span>&#160;//                                                                    firingTableIdxD1 + managerRuntimeData.extFiringTableEndIdxD1[lGrpId],</div>
<div class="line"><a name="l02852"></a><span class="lineno"> 2852</span>&#160;//                                                                    GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l02853"></a><span class="lineno"> 2853</span>&#160;//             firingTableIdxD1 += managerRuntimeData.extFiringTableEndIdxD1[lGrpId];</div>
<div class="line"><a name="l02854"></a><span class="lineno"> 2854</span>&#160;//</div>
<div class="line"><a name="l02855"></a><span class="lineno"> 2855</span>&#160;//         }</div>
<div class="line"><a name="l02856"></a><span class="lineno"> 2856</span>&#160;//         //KERNEL_DEBUG(&quot;GPU1 New D1:%d/D2:%d&quot;, firingTableIdxD1, firingTableIdxD2);</div>
<div class="line"><a name="l02857"></a><span class="lineno"> 2857</span>&#160;//     }</div>
<div class="line"><a name="l02858"></a><span class="lineno"> 2858</span>&#160;//     managerRuntimeData.timeTableD2[simTimeMs + glbNetworkConfig.maxDelay + 1] = firingTableIdxD2;</div>
<div class="line"><a name="l02859"></a><span class="lineno"> 2859</span>&#160;//     managerRuntimeData.timeTableD1[simTimeMs + glbNetworkConfig.maxDelay + 1] = firingTableIdxD1;</div>
<div class="line"><a name="l02860"></a><span class="lineno"> 2860</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyToSymbol(timeTableD2GPU, managerRuntimeData.timeTableD2, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02861"></a><span class="lineno"> 2861</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyToSymbol(timeTableD1GPU, managerRuntimeData.timeTableD1, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02862"></a><span class="lineno"> 2862</span>&#160;// }</div>
<div class="line"><a name="l02863"></a><span class="lineno"> 2863</span>&#160;//</div>
<div class="line"><a name="l02864"></a><span class="lineno"> 2864</span>&#160;// // GPU1 -&gt; GPU0</div>
<div class="line"><a name="l02865"></a><span class="lineno"> 2865</span>&#160;// if (!groupPartitionLists[1].empty() &amp;&amp; !groupPartitionLists[0].empty()) {</div>
<div class="line"><a name="l02866"></a><span class="lineno"> 2866</span>&#160;//     checkAndSetGPUDevice(1);</div>
<div class="line"><a name="l02867"></a><span class="lineno"> 2867</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD2, runtimeData[1].extFiringTableEndIdxD2, sizeof(int) * networkConfigs[1].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02868"></a><span class="lineno"> 2868</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD1, runtimeData[1].extFiringTableEndIdxD1, sizeof(int) * networkConfigs[1].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02869"></a><span class="lineno"> 2869</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableD2, runtimeData[1].extFiringTableD2, sizeof(int*) * networkConfigs[1].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02870"></a><span class="lineno"> 2870</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableD1, runtimeData[1].extFiringTableD1, sizeof(int*) * networkConfigs[1].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02871"></a><span class="lineno"> 2871</span>&#160;//     //KERNEL_DEBUG(&quot;GPU1 D1ex:%d/D2ex:%d&quot;, managerRuntimeData.extFiringTableEndIdxD1[0], managerRuntimeData.extFiringTableEndIdxD2[0]);</div>
<div class="line"><a name="l02872"></a><span class="lineno"> 2872</span>&#160;//</div>
<div class="line"><a name="l02873"></a><span class="lineno"> 2873</span>&#160;//     checkAndSetGPUDevice(0);</div>
<div class="line"><a name="l02874"></a><span class="lineno"> 2874</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD2, timeTableD2GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02875"></a><span class="lineno"> 2875</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD1, timeTableD1GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02876"></a><span class="lineno"> 2876</span>&#160;//     firingTableIdxD2 = managerRuntimeData.timeTableD2[simTimeMs + glbNetworkConfig.maxDelay + 1];</div>
<div class="line"><a name="l02877"></a><span class="lineno"> 2877</span>&#160;//     firingTableIdxD1 = managerRuntimeData.timeTableD1[simTimeMs + glbNetworkConfig.maxDelay + 1];</div>
<div class="line"><a name="l02878"></a><span class="lineno"> 2878</span>&#160;//     //KERNEL_DEBUG(&quot;GPU0 D1:%d/D2:%d&quot;, firingTableIdxD1, firingTableIdxD2);</div>
<div class="line"><a name="l02879"></a><span class="lineno"> 2879</span>&#160;//</div>
<div class="line"><a name="l02880"></a><span class="lineno"> 2880</span>&#160;//     for (int lGrpId = 0; lGrpId &lt; networkConfigs[1].numGroups; lGrpId++) {</div>
<div class="line"><a name="l02881"></a><span class="lineno"> 2881</span>&#160;//         if (groupConfigs[1][lGrpId].hasExternalConnect &amp;&amp; managerRuntimeData.extFiringTableEndIdxD2[lGrpId] &gt; 0) {</div>
<div class="line"><a name="l02882"></a><span class="lineno"> 2882</span>&#160;//             CUDA_CHECK_ERRORS( cudaMemcpyPeer(runtimeData[0].firingTableD2 + firingTableIdxD2, 0,</div>
<div class="line"><a name="l02883"></a><span class="lineno"> 2883</span>&#160;//                                               managerRuntimeData.extFiringTableD2[lGrpId], 1,</div>
<div class="line"><a name="l02884"></a><span class="lineno"> 2884</span>&#160;//                                               sizeof(int) * managerRuntimeData.extFiringTableEndIdxD2[lGrpId]));</div>
<div class="line"><a name="l02885"></a><span class="lineno"> 2885</span>&#160;//</div>
<div class="line"><a name="l02886"></a><span class="lineno"> 2886</span>&#160;//             for (std::list&lt;GroupConfigMD&gt;::iterator grpIt = groupPartitionLists[0].begin(); grpIt != groupPartitionLists[0].end(); grpIt++) {</div>
<div class="line"><a name="l02887"></a><span class="lineno"> 2887</span>&#160;//                 if (grpIt-&gt;gGrpId == groupConfigs[1][lGrpId].gGrpId)</div>
<div class="line"><a name="l02888"></a><span class="lineno"> 2888</span>&#160;//                     GtoLOffset = grpIt-&gt;GtoLOffset;</div>
<div class="line"><a name="l02889"></a><span class="lineno"> 2889</span>&#160;//             }</div>
<div class="line"><a name="l02890"></a><span class="lineno"> 2890</span>&#160;//         </div>
<div class="line"><a name="l02891"></a><span class="lineno"> 2891</span>&#160;//             kernel_convertExtSpikesD2&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(firingTableIdxD2,</div>
<div class="line"><a name="l02892"></a><span class="lineno"> 2892</span>&#160;//                                                                    firingTableIdxD2 + managerRuntimeData.extFiringTableEndIdxD2[lGrpId],</div>
<div class="line"><a name="l02893"></a><span class="lineno"> 2893</span>&#160;//                                                                    GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l02894"></a><span class="lineno"> 2894</span>&#160;//             firingTableIdxD2 += managerRuntimeData.extFiringTableEndIdxD2[lGrpId];</div>
<div class="line"><a name="l02895"></a><span class="lineno"> 2895</span>&#160;//         }</div>
<div class="line"><a name="l02896"></a><span class="lineno"> 2896</span>&#160;//</div>
<div class="line"><a name="l02897"></a><span class="lineno"> 2897</span>&#160;//         if (groupConfigs[1][lGrpId].hasExternalConnect &amp;&amp; managerRuntimeData.extFiringTableEndIdxD1[lGrpId] &gt; 0) {</div>
<div class="line"><a name="l02898"></a><span class="lineno"> 2898</span>&#160;//             CUDA_CHECK_ERRORS( cudaMemcpyPeer(runtimeData[0].firingTableD1 + firingTableIdxD1, 0,</div>
<div class="line"><a name="l02899"></a><span class="lineno"> 2899</span>&#160;//                                               managerRuntimeData.extFiringTableD1[lGrpId], 1,</div>
<div class="line"><a name="l02900"></a><span class="lineno"> 2900</span>&#160;//                                               sizeof(int) * managerRuntimeData.extFiringTableEndIdxD1[lGrpId]));</div>
<div class="line"><a name="l02901"></a><span class="lineno"> 2901</span>&#160;//</div>
<div class="line"><a name="l02902"></a><span class="lineno"> 2902</span>&#160;//             for (std::list&lt;GroupConfigMD&gt;::iterator grpIt = groupPartitionLists[0].begin(); grpIt != groupPartitionLists[0].end(); grpIt++) {</div>
<div class="line"><a name="l02903"></a><span class="lineno"> 2903</span>&#160;//                 if (grpIt-&gt;gGrpId == groupConfigs[1][lGrpId].gGrpId)</div>
<div class="line"><a name="l02904"></a><span class="lineno"> 2904</span>&#160;//                     GtoLOffset = grpIt-&gt;GtoLOffset;</div>
<div class="line"><a name="l02905"></a><span class="lineno"> 2905</span>&#160;//             }</div>
<div class="line"><a name="l02906"></a><span class="lineno"> 2906</span>&#160;//         </div>
<div class="line"><a name="l02907"></a><span class="lineno"> 2907</span>&#160;//             kernel_convertExtSpikesD1&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(firingTableIdxD1,</div>
<div class="line"><a name="l02908"></a><span class="lineno"> 2908</span>&#160;//                                                                    firingTableIdxD1 + managerRuntimeData.extFiringTableEndIdxD1[lGrpId],</div>
<div class="line"><a name="l02909"></a><span class="lineno"> 2909</span>&#160;//                                                                    GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l02910"></a><span class="lineno"> 2910</span>&#160;//             firingTableIdxD1 += managerRuntimeData.extFiringTableEndIdxD1[lGrpId];</div>
<div class="line"><a name="l02911"></a><span class="lineno"> 2911</span>&#160;//         }</div>
<div class="line"><a name="l02912"></a><span class="lineno"> 2912</span>&#160;//         //KERNEL_DEBUG(&quot;GPU0 New D1:%d/D2:%d&quot;, firingTableIdxD1, firingTableIdxD2);</div>
<div class="line"><a name="l02913"></a><span class="lineno"> 2913</span>&#160;//     }</div>
<div class="line"><a name="l02914"></a><span class="lineno"> 2914</span>&#160;//     managerRuntimeData.timeTableD2[simTimeMs + glbNetworkConfig.maxDelay + 1] = firingTableIdxD2;</div>
<div class="line"><a name="l02915"></a><span class="lineno"> 2915</span>&#160;//     managerRuntimeData.timeTableD1[simTimeMs + glbNetworkConfig.maxDelay + 1] = firingTableIdxD1;</div>
<div class="line"><a name="l02916"></a><span class="lineno"> 2916</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyToSymbol(timeTableD2GPU, managerRuntimeData.timeTableD2, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02917"></a><span class="lineno"> 2917</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyToSymbol(timeTableD1GPU, managerRuntimeData.timeTableD1, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02918"></a><span class="lineno"> 2918</span>&#160;// }</div>
<div class="line"><a name="l02919"></a><span class="lineno"> 2919</span>&#160;// </div>
<div class="line"><a name="l02920"></a><span class="lineno"> 2920</span>&#160;//</div>
<div class="line"><a name="l02921"></a><span class="lineno"> 2921</span>&#160;// for (std::list&lt;RoutingTableEntry&gt;::iterator rteItr = spikeRoutingTable.begin(); rteItr != spikeRoutingTable.end(); rteItr++) {</div>
<div class="line"><a name="l02922"></a><span class="lineno"> 2922</span>&#160;//     int srcNetId = rteItr-&gt;srcNetId;</div>
<div class="line"><a name="l02923"></a><span class="lineno"> 2923</span>&#160;//     int destNetId = rteItr-&gt;destNetId;</div>
<div class="line"><a name="l02924"></a><span class="lineno"> 2924</span>&#160;//     assert(srcNetId &lt; CPU_RUNTIME_BASE);</div>
<div class="line"><a name="l02925"></a><span class="lineno"> 2925</span>&#160;//     assert(destNetId &lt; CPU_RUNTIME_BASE);</div>
<div class="line"><a name="l02926"></a><span class="lineno"> 2926</span>&#160;//     checkAndSetGPUDevice(srcNetId);</div>
<div class="line"><a name="l02927"></a><span class="lineno"> 2927</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD2, runtimeData[srcNetId].extFiringTableEndIdxD2, sizeof(int) * networkConfigs[srcNetId].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02928"></a><span class="lineno"> 2928</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD1, runtimeData[srcNetId].extFiringTableEndIdxD1, sizeof(int) * networkConfigs[srcNetId].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02929"></a><span class="lineno"> 2929</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableD2, runtimeData[srcNetId].extFiringTableD2, sizeof(int*) * networkConfigs[srcNetId].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02930"></a><span class="lineno"> 2930</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.extFiringTableD1, runtimeData[srcNetId].extFiringTableD1, sizeof(int*) * networkConfigs[srcNetId].numGroups, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02931"></a><span class="lineno"> 2931</span>&#160;//     //KERNEL_DEBUG(&quot;GPU0 D1ex:%d/D2ex:%d&quot;, managerRuntimeData.extFiringTableEndIdxD1[0], managerRuntimeData.extFiringTableEndIdxD2[0]);</div>
<div class="line"><a name="l02932"></a><span class="lineno"> 2932</span>&#160;//</div>
<div class="line"><a name="l02933"></a><span class="lineno"> 2933</span>&#160;//     checkAndSetGPUDevice(destNetId);</div>
<div class="line"><a name="l02934"></a><span class="lineno"> 2934</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD2, timeTableD2GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02935"></a><span class="lineno"> 2935</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD1, timeTableD1GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l02936"></a><span class="lineno"> 2936</span>&#160;//     firingTableIdxD2 = managerRuntimeData.timeTableD2[simTimeMs + glbNetworkConfig.maxDelay + 1];</div>
<div class="line"><a name="l02937"></a><span class="lineno"> 2937</span>&#160;//     firingTableIdxD1 = managerRuntimeData.timeTableD1[simTimeMs + glbNetworkConfig.maxDelay + 1];</div>
<div class="line"><a name="l02938"></a><span class="lineno"> 2938</span>&#160;//     //KERNEL_DEBUG(&quot;GPU1 D1:%d/D2:%d&quot;, firingTableIdxD1, firingTableIdxD2);</div>
<div class="line"><a name="l02939"></a><span class="lineno"> 2939</span>&#160;//</div>
<div class="line"><a name="l02940"></a><span class="lineno"> 2940</span>&#160;//     for (int lGrpId = 0; lGrpId &lt; networkConfigs[srcNetId].numGroups; lGrpId++) {</div>
<div class="line"><a name="l02941"></a><span class="lineno"> 2941</span>&#160;//         if (groupConfigs[srcNetId][lGrpId].hasExternalConnect &amp;&amp; managerRuntimeData.extFiringTableEndIdxD2[lGrpId] &gt; 0) {</div>
<div class="line"><a name="l02942"></a><span class="lineno"> 2942</span>&#160;//             CUDA_CHECK_ERRORS( cudaMemcpyPeer(runtimeData[destNetId].firingTableD2 + firingTableIdxD2, destNetId,</div>
<div class="line"><a name="l02943"></a><span class="lineno"> 2943</span>&#160;//                                               managerRuntimeData.extFiringTableD2[lGrpId], srcNetId,</div>
<div class="line"><a name="l02944"></a><span class="lineno"> 2944</span>&#160;//                                               sizeof(int) * managerRuntimeData.extFiringTableEndIdxD2[lGrpId]));</div>
<div class="line"><a name="l02945"></a><span class="lineno"> 2945</span>&#160;//</div>
<div class="line"><a name="l02946"></a><span class="lineno"> 2946</span>&#160;//             for (std::list&lt;GroupConfigMD&gt;::iterator grpIt = groupPartitionLists[destNetId].begin(); grpIt != groupPartitionLists[destNetId].end(); grpIt++) {</div>
<div class="line"><a name="l02947"></a><span class="lineno"> 2947</span>&#160;//                 if (grpIt-&gt;gGrpId == groupConfigs[srcNetId][lGrpId].gGrpId)</div>
<div class="line"><a name="l02948"></a><span class="lineno"> 2948</span>&#160;//                     GtoLOffset = grpIt-&gt;GtoLOffset;</div>
<div class="line"><a name="l02949"></a><span class="lineno"> 2949</span>&#160;//             }</div>
<div class="line"><a name="l02950"></a><span class="lineno"> 2950</span>&#160;//</div>
<div class="line"><a name="l02951"></a><span class="lineno"> 2951</span>&#160;//             kernel_convertExtSpikesD2&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(firingTableIdxD2,</div>
<div class="line"><a name="l02952"></a><span class="lineno"> 2952</span>&#160;//                                                                    firingTableIdxD2 + managerRuntimeData.extFiringTableEndIdxD2[lGrpId],</div>
<div class="line"><a name="l02953"></a><span class="lineno"> 2953</span>&#160;//                                                                    GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l02954"></a><span class="lineno"> 2954</span>&#160;//             firingTableIdxD2 += managerRuntimeData.extFiringTableEndIdxD2[lGrpId];</div>
<div class="line"><a name="l02955"></a><span class="lineno"> 2955</span>&#160;//         }</div>
<div class="line"><a name="l02956"></a><span class="lineno"> 2956</span>&#160;//</div>
<div class="line"><a name="l02957"></a><span class="lineno"> 2957</span>&#160;//         if (groupConfigs[srcNetId][lGrpId].hasExternalConnect &amp;&amp; managerRuntimeData.extFiringTableEndIdxD1[lGrpId] &gt; 0) {</div>
<div class="line"><a name="l02958"></a><span class="lineno"> 2958</span>&#160;//             CUDA_CHECK_ERRORS( cudaMemcpyPeer(runtimeData[destNetId].firingTableD1 + firingTableIdxD1, destNetId,</div>
<div class="line"><a name="l02959"></a><span class="lineno"> 2959</span>&#160;//                                               managerRuntimeData.extFiringTableD1[lGrpId], srcNetId,</div>
<div class="line"><a name="l02960"></a><span class="lineno"> 2960</span>&#160;//                                               sizeof(int) * managerRuntimeData.extFiringTableEndIdxD1[lGrpId]));</div>
<div class="line"><a name="l02961"></a><span class="lineno"> 2961</span>&#160;//</div>
<div class="line"><a name="l02962"></a><span class="lineno"> 2962</span>&#160;//             for (std::list&lt;GroupConfigMD&gt;::iterator grpIt = groupPartitionLists[destNetId].begin(); grpIt != groupPartitionLists[destNetId].end(); grpIt++) {</div>
<div class="line"><a name="l02963"></a><span class="lineno"> 2963</span>&#160;//                 if (grpIt-&gt;gGrpId == groupConfigs[srcNetId][lGrpId].gGrpId)</div>
<div class="line"><a name="l02964"></a><span class="lineno"> 2964</span>&#160;//                     GtoLOffset = grpIt-&gt;GtoLOffset;</div>
<div class="line"><a name="l02965"></a><span class="lineno"> 2965</span>&#160;//             }</div>
<div class="line"><a name="l02966"></a><span class="lineno"> 2966</span>&#160;//</div>
<div class="line"><a name="l02967"></a><span class="lineno"> 2967</span>&#160;//             kernel_convertExtSpikesD1&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;(firingTableIdxD1,</div>
<div class="line"><a name="l02968"></a><span class="lineno"> 2968</span>&#160;//                                                                    firingTableIdxD1 + managerRuntimeData.extFiringTableEndIdxD1[lGrpId],</div>
<div class="line"><a name="l02969"></a><span class="lineno"> 2969</span>&#160;//                                                                    GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l02970"></a><span class="lineno"> 2970</span>&#160;//             firingTableIdxD1 += managerRuntimeData.extFiringTableEndIdxD1[lGrpId];</div>
<div class="line"><a name="l02971"></a><span class="lineno"> 2971</span>&#160;//</div>
<div class="line"><a name="l02972"></a><span class="lineno"> 2972</span>&#160;//         }</div>
<div class="line"><a name="l02973"></a><span class="lineno"> 2973</span>&#160;//         //KERNEL_DEBUG(&quot;GPU1 New D1:%d/D2:%d&quot;, firingTableIdxD1, firingTableIdxD2);</div>
<div class="line"><a name="l02974"></a><span class="lineno"> 2974</span>&#160;//     }</div>
<div class="line"><a name="l02975"></a><span class="lineno"> 2975</span>&#160;//     managerRuntimeData.timeTableD2[simTimeMs + glbNetworkConfig.maxDelay + 1] = firingTableIdxD2;</div>
<div class="line"><a name="l02976"></a><span class="lineno"> 2976</span>&#160;//     managerRuntimeData.timeTableD1[simTimeMs + glbNetworkConfig.maxDelay + 1] = firingTableIdxD1;</div>
<div class="line"><a name="l02977"></a><span class="lineno"> 2977</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyToSymbol(timeTableD2GPU, managerRuntimeData.timeTableD2, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02978"></a><span class="lineno"> 2978</span>&#160;//     CUDA_CHECK_ERRORS( cudaMemcpyToSymbol(timeTableD1GPU, managerRuntimeData.timeTableD1, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l02979"></a><span class="lineno"> 2979</span>&#160;// }</div>
<div class="line"><a name="l02980"></a><span class="lineno"> 2980</span>&#160;//}</div>
<div class="line"><a name="l02981"></a><span class="lineno"> 2981</span>&#160;</div>
<div class="line"><a name="l02982"></a><span class="lineno"> 2982</span>&#160;/*!</div>
<div class="line"><a name="l02983"></a><span class="lineno"> 2983</span>&#160; * \brief This function is called every second by SNN::runNetwork(). It updates the firingTableD1(D2)GPU and</div>
<div class="line"><a name="l02984"></a><span class="lineno"> 2984</span>&#160; * timeTableD1(D2)GPU by removing older firing information.</div>
<div class="line"><a name="l02985"></a><span class="lineno"> 2985</span>&#160; */</div>
<div class="line"><a name="l02986"></a><span class="lineno"> 2986</span>&#160;void SNN::shiftSpikeTables_F_GPU(int netId) {</div>
<div class="line"><a name="l02987"></a><span class="lineno"> 2987</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02988"></a><span class="lineno"> 2988</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02989"></a><span class="lineno"> 2989</span>&#160;   kernel_shiftFiringTable&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;();</div>
<div class="line"><a name="l02990"></a><span class="lineno"> 2990</span>&#160;}</div>
<div class="line"><a name="l02991"></a><span class="lineno"> 2991</span>&#160;</div>
<div class="line"><a name="l02992"></a><span class="lineno"> 2992</span>&#160;void SNN::shiftSpikeTables_T_GPU(int netId) {</div>
<div class="line"><a name="l02993"></a><span class="lineno"> 2993</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l02994"></a><span class="lineno"> 2994</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l02995"></a><span class="lineno"> 2995</span>&#160;   kernel_shiftTimeTable&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;();</div>
<div class="line"><a name="l02996"></a><span class="lineno"> 2996</span>&#160;}</div>
<div class="line"><a name="l02997"></a><span class="lineno"> 2997</span>&#160;</div>
<div class="line"><a name="l02998"></a><span class="lineno"> 2998</span>&#160;/*</div>
<div class="line"><a name="l02999"></a><span class="lineno"> 2999</span>&#160; * \brief Update syanptic weights every 10ms, 100ms, or 1000ms</div>
<div class="line"><a name="l03000"></a><span class="lineno"> 3000</span>&#160; *</div>
<div class="line"><a name="l03001"></a><span class="lineno"> 3001</span>&#160; *</div>
<div class="line"><a name="l03002"></a><span class="lineno"> 3002</span>&#160; */</div>
<div class="line"><a name="l03003"></a><span class="lineno"> 3003</span>&#160;void SNN::updateWeights_GPU(int netId) {</div>
<div class="line"><a name="l03004"></a><span class="lineno"> 3004</span>&#160;   assert(sim_in_testing == false);</div>
<div class="line"><a name="l03005"></a><span class="lineno"> 3005</span>&#160;   assert(sim_with_fixedwts == false);</div>
<div class="line"><a name="l03006"></a><span class="lineno"> 3006</span>&#160;   assert(runtimeData[netId].memType == GPU_MEM);</div>
<div class="line"><a name="l03007"></a><span class="lineno"> 3007</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03008"></a><span class="lineno"> 3008</span>&#160;</div>
<div class="line"><a name="l03009"></a><span class="lineno"> 3009</span>&#160;   kernel_updateWeights&lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS&gt;&gt;&gt;();</div>
<div class="line"><a name="l03010"></a><span class="lineno"> 3010</span>&#160;}</div>
<div class="line"><a name="l03011"></a><span class="lineno"> 3011</span>&#160;</div>
<div class="line"><a name="l03012"></a><span class="lineno"> 3012</span>&#160;//__global__ void gpu_resetFiringInformation() {</div>
<div class="line"><a name="l03013"></a><span class="lineno"> 3013</span>&#160;// if(threadIdx.x==0 &amp;&amp; blockIdx.x==0) {</div>
<div class="line"><a name="l03014"></a><span class="lineno"> 3014</span>&#160;//     for(int i = 0; i &lt; ROUNDED_TIMING_COUNT; i++) {</div>
<div class="line"><a name="l03015"></a><span class="lineno"> 3015</span>&#160;//         timeTableD2GPU[i]   = 0;</div>
<div class="line"><a name="l03016"></a><span class="lineno"> 3016</span>&#160;//         timeTableD1GPU[i]   = 0;</div>
<div class="line"><a name="l03017"></a><span class="lineno"> 3017</span>&#160;//     }</div>
<div class="line"><a name="l03018"></a><span class="lineno"> 3018</span>&#160;//     spikeCountD2SecGPU=0;</div>
<div class="line"><a name="l03019"></a><span class="lineno"> 3019</span>&#160;//     spikeCountD1SecGPU=0;</div>
<div class="line"><a name="l03020"></a><span class="lineno"> 3020</span>&#160;//     secD2fireCntTest=0;</div>
<div class="line"><a name="l03021"></a><span class="lineno"> 3021</span>&#160;//     secD1fireCntTest=0;</div>
<div class="line"><a name="l03022"></a><span class="lineno"> 3022</span>&#160;//     spikeCountD2GPU=0;</div>
<div class="line"><a name="l03023"></a><span class="lineno"> 3023</span>&#160;//     spikeCountD1GPU=0;</div>
<div class="line"><a name="l03024"></a><span class="lineno"> 3024</span>&#160;//</div>
<div class="line"><a name="l03025"></a><span class="lineno"> 3025</span>&#160;//    //spikeCountAll1Sec=0;//assigned in fetchSpikeTables()</div>
<div class="line"><a name="l03026"></a><span class="lineno"> 3026</span>&#160;// }</div>
<div class="line"><a name="l03027"></a><span class="lineno"> 3027</span>&#160;//</div>
<div class="line"><a name="l03028"></a><span class="lineno"> 3028</span>&#160;//}</div>
<div class="line"><a name="l03029"></a><span class="lineno"> 3029</span>&#160;//</div>
<div class="line"><a name="l03030"></a><span class="lineno"> 3030</span>&#160;//void SNN::resetFiringInformation_GPU() {</div>
<div class="line"><a name="l03031"></a><span class="lineno"> 3031</span>&#160;// checkAndSetGPUDevice();</div>
<div class="line"><a name="l03032"></a><span class="lineno"> 3032</span>&#160;//</div>
<div class="line"><a name="l03033"></a><span class="lineno"> 3033</span>&#160;// gpu_resetFiringInformation&lt;&lt;&lt;NUM_BLOCKS,NUM_THREADS&gt;&gt;&gt;();</div>
<div class="line"><a name="l03034"></a><span class="lineno"> 3034</span>&#160;//}</div>
<div class="line"><a name="l03035"></a><span class="lineno"> 3035</span>&#160;</div>
<div class="line"><a name="l03036"></a><span class="lineno"> 3036</span>&#160;/*!</div>
<div class="line"><a name="l03037"></a><span class="lineno"> 3037</span>&#160; * \brief this function allocates device (GPU) memory sapce and copies external current to it</div>
<div class="line"><a name="l03038"></a><span class="lineno"> 3038</span>&#160; *</div>
<div class="line"><a name="l03039"></a><span class="lineno"> 3039</span>&#160; * This function:</div>
<div class="line"><a name="l03040"></a><span class="lineno"> 3040</span>&#160;</div>
<div class="line"><a name="l03041"></a><span class="lineno"> 3041</span>&#160; * (allocate and) copy extCurrent</div>
<div class="line"><a name="l03042"></a><span class="lineno"> 3042</span>&#160; *</div>
<div class="line"><a name="l03043"></a><span class="lineno"> 3043</span>&#160; * This funcion is called by copyNeuronState() and setExternalCurrent. Only host-to-divice copy is required</div>
<div class="line"><a name="l03044"></a><span class="lineno"> 3044</span>&#160; *</div>
<div class="line"><a name="l03045"></a><span class="lineno"> 3045</span>&#160; * \param[in] netId the id of a local network, which is the same as the device (GPU) id</div>
<div class="line"><a name="l03046"></a><span class="lineno"> 3046</span>&#160; * \param[in] lGrpId the local group id in a local network, which specifiy the group(s) to be copied</div>
<div class="line"><a name="l03047"></a><span class="lineno"> 3047</span>&#160; * \param[in] dest pointer to runtime data desitnation</div>
<div class="line"><a name="l03048"></a><span class="lineno"> 3048</span>&#160; * \param[in] allocateMem a flag indicates whether allocating memory space before copying</div>
<div class="line"><a name="l03049"></a><span class="lineno"> 3049</span>&#160; *</div>
<div class="line"><a name="l03050"></a><span class="lineno"> 3050</span>&#160; * \sa allocateSNN_GPU fetchSTPState</div>
<div class="line"><a name="l03051"></a><span class="lineno"> 3051</span>&#160; * \since v3.0</div>
<div class="line"><a name="l03052"></a><span class="lineno"> 3052</span>&#160; */</div>
<div class="line"><a name="l03053"></a><span class="lineno"> 3053</span>&#160;void SNN::copyExternalCurrent(int netId, int lGrpId, RuntimeData* dest, cudaMemcpyKind kind, bool allocateMem) {</div>
<div class="line"><a name="l03054"></a><span class="lineno"> 3054</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03055"></a><span class="lineno"> 3055</span>&#160;   checkDestSrcPtrs(dest, &amp;managerRuntimeData, cudaMemcpyHostToDevice, allocateMem, lGrpId, 0);// check that the destination pointer is properly allocated..</div>
<div class="line"><a name="l03056"></a><span class="lineno"> 3056</span>&#160;   assert(kind == cudaMemcpyHostToDevice);</div>
<div class="line"><a name="l03057"></a><span class="lineno"> 3057</span>&#160;   </div>
<div class="line"><a name="l03058"></a><span class="lineno"> 3058</span>&#160;   int posN, lengthN;</div>
<div class="line"><a name="l03059"></a><span class="lineno"> 3059</span>&#160;</div>
<div class="line"><a name="l03060"></a><span class="lineno"> 3060</span>&#160;   if(lGrpId == ALL) {</div>
<div class="line"><a name="l03061"></a><span class="lineno"> 3061</span>&#160;       posN  = 0;</div>
<div class="line"><a name="l03062"></a><span class="lineno"> 3062</span>&#160;       lengthN  = networkConfigs[netId].numNReg;</div>
<div class="line"><a name="l03063"></a><span class="lineno"> 3063</span>&#160;   } else {</div>
<div class="line"><a name="l03064"></a><span class="lineno"> 3064</span>&#160;       assert(lGrpId &gt;= 0);</div>
<div class="line"><a name="l03065"></a><span class="lineno"> 3065</span>&#160;       posN = groupConfigs[netId][lGrpId].lStartN;</div>
<div class="line"><a name="l03066"></a><span class="lineno"> 3066</span>&#160;       lengthN = groupConfigs[netId][lGrpId].numN;</div>
<div class="line"><a name="l03067"></a><span class="lineno"> 3067</span>&#160;   }</div>
<div class="line"><a name="l03068"></a><span class="lineno"> 3068</span>&#160;   assert(lengthN &gt;= 0 &amp;&amp; lengthN &lt;= networkConfigs[netId].numNReg); // assert NOT poisson neurons</div>
<div class="line"><a name="l03069"></a><span class="lineno"> 3069</span>&#160;</div>
<div class="line"><a name="l03070"></a><span class="lineno"> 3070</span>&#160;   //KERNEL_DEBUG(&quot;copyExternalCurrent: lGrpId=%d, ptrPos=%d, length=%d, allocate=%s&quot;, lGrpId, posN, lengthN, allocateMem?&quot;y&quot;:&quot;n&quot;);</div>
<div class="line"><a name="l03071"></a><span class="lineno"> 3071</span>&#160;</div>
<div class="line"><a name="l03072"></a><span class="lineno"> 3072</span>&#160;   if(allocateMem)</div>
<div class="line"><a name="l03073"></a><span class="lineno"> 3073</span>&#160;       CUDA_CHECK_ERRORS(cudaMalloc((void**)&amp;dest-&gt;extCurrent, sizeof(float) * lengthN));</div>
<div class="line"><a name="l03074"></a><span class="lineno"> 3074</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(&amp;(dest-&gt;extCurrent[posN]), &amp;(managerRuntimeData.extCurrent[posN]), sizeof(float) * lengthN, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03075"></a><span class="lineno"> 3075</span>&#160;}</div>
<div class="line"><a name="l03076"></a><span class="lineno"> 3076</span>&#160;</div>
<div class="line"><a name="l03077"></a><span class="lineno"> 3077</span>&#160;/*!</div>
<div class="line"><a name="l03078"></a><span class="lineno"> 3078</span>&#160; * \brief This function fetch the spike count in all local networks and sum the up</div>
<div class="line"><a name="l03079"></a><span class="lineno"> 3079</span>&#160; */</div>
<div class="line"><a name="l03080"></a><span class="lineno"> 3080</span>&#160;void SNN::copyNetworkSpikeCount(int netId, cudaMemcpyKind kind,</div>
<div class="line"><a name="l03081"></a><span class="lineno"> 3081</span>&#160;                               unsigned int* spikeCountD1, unsigned int* spikeCountD2,</div>
<div class="line"><a name="l03082"></a><span class="lineno"> 3082</span>&#160;                               unsigned int* spikeCountExtD1, unsigned int* spikeCountExtD2) {</div>
<div class="line"><a name="l03083"></a><span class="lineno"> 3083</span>&#160;</div>
<div class="line"><a name="l03084"></a><span class="lineno"> 3084</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03085"></a><span class="lineno"> 3085</span>&#160;   assert(kind == cudaMemcpyDeviceToHost);</div>
<div class="line"><a name="l03086"></a><span class="lineno"> 3086</span>&#160;</div>
<div class="line"><a name="l03087"></a><span class="lineno"> 3087</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(spikeCountExtD2, spikeCountExtRxD2GPU, sizeof(int), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03088"></a><span class="lineno"> 3088</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(spikeCountExtD1, spikeCountExtRxD1GPU, sizeof(int), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03089"></a><span class="lineno"> 3089</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(spikeCountD2, spikeCountD2GPU, sizeof(int), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03090"></a><span class="lineno"> 3090</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(spikeCountD1, spikeCountD1GPU, sizeof(int), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03091"></a><span class="lineno"> 3091</span>&#160;}</div>
<div class="line"><a name="l03092"></a><span class="lineno"> 3092</span>&#160;</div>
<div class="line"><a name="l03093"></a><span class="lineno"> 3093</span>&#160;/*!</div>
<div class="line"><a name="l03094"></a><span class="lineno"> 3094</span>&#160; * \brief This function fetch spikeTables in the local network specified by netId</div>
<div class="line"><a name="l03095"></a><span class="lineno"> 3095</span>&#160; *</div>
<div class="line"><a name="l03096"></a><span class="lineno"> 3096</span>&#160; * \param[in] netId the id of local network of which timeTableD1(D2) and firingTableD1(D2) are copied to manager runtime data</div>
<div class="line"><a name="l03097"></a><span class="lineno"> 3097</span>&#160; */</div>
<div class="line"><a name="l03098"></a><span class="lineno"> 3098</span>&#160;void SNN::copySpikeTables(int netId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l03099"></a><span class="lineno"> 3099</span>&#160;   unsigned int gpuSpikeCountD1Sec, gpuSpikeCountD2Sec, gpuSpikeCountLastSecLeftD2;</div>
<div class="line"><a name="l03100"></a><span class="lineno"> 3100</span>&#160;</div>
<div class="line"><a name="l03101"></a><span class="lineno"> 3101</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03102"></a><span class="lineno"> 3102</span>&#160;   assert(kind == cudaMemcpyDeviceToHost);</div>
<div class="line"><a name="l03103"></a><span class="lineno"> 3103</span>&#160;</div>
<div class="line"><a name="l03104"></a><span class="lineno"> 3104</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(&amp;gpuSpikeCountLastSecLeftD2, spikeCountLastSecLeftD2GPU, sizeof(int), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03105"></a><span class="lineno"> 3105</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(&amp;gpuSpikeCountD2Sec, spikeCountD2SecGPU, sizeof(int), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03106"></a><span class="lineno"> 3106</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(&amp;gpuSpikeCountD1Sec, spikeCountD1SecGPU, sizeof(int), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03107"></a><span class="lineno"> 3107</span>&#160;   CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.firingTableD2, runtimeData[netId].firingTableD2, sizeof(int)*(gpuSpikeCountD2Sec + gpuSpikeCountLastSecLeftD2), cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03108"></a><span class="lineno"> 3108</span>&#160;   CUDA_CHECK_ERRORS( cudaMemcpy(managerRuntimeData.firingTableD1, runtimeData[netId].firingTableD1, sizeof(int)*gpuSpikeCountD1Sec, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03109"></a><span class="lineno"> 3109</span>&#160;   CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD2, timeTableD2GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03110"></a><span class="lineno"> 3110</span>&#160;   CUDA_CHECK_ERRORS( cudaMemcpyFromSymbol(managerRuntimeData.timeTableD1, timeTableD1GPU, sizeof(int)*(1000+glbNetworkConfig.maxDelay+1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03111"></a><span class="lineno"> 3111</span>&#160;}</div>
<div class="line"><a name="l03112"></a><span class="lineno"> 3112</span>&#160;</div>
<div class="line"><a name="l03113"></a><span class="lineno"> 3113</span>&#160;void SNN::copyTimeTable(int netId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l03114"></a><span class="lineno"> 3114</span>&#160;   assert(netId &lt; CPU_RUNTIME_BASE);</div>
<div class="line"><a name="l03115"></a><span class="lineno"> 3115</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03116"></a><span class="lineno"> 3116</span>&#160;</div>
<div class="line"><a name="l03117"></a><span class="lineno"> 3117</span>&#160;   if (kind == cudaMemcpyDeviceToHost) {</div>
<div class="line"><a name="l03118"></a><span class="lineno"> 3118</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(managerRuntimeData.timeTableD2, timeTableD2GPU, sizeof(int)*(1000 + glbNetworkConfig.maxDelay + 1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03119"></a><span class="lineno"> 3119</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpyFromSymbol(managerRuntimeData.timeTableD1, timeTableD1GPU, sizeof(int)*(1000 + glbNetworkConfig.maxDelay + 1), 0, cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03120"></a><span class="lineno"> 3120</span>&#160;   } else { // kind == cudaMemcpyHostToDevice</div>
<div class="line"><a name="l03121"></a><span class="lineno"> 3121</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(timeTableD2GPU, managerRuntimeData.timeTableD2, sizeof(int)*(1000 + glbNetworkConfig.maxDelay + 1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03122"></a><span class="lineno"> 3122</span>&#160;       CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(timeTableD1GPU, managerRuntimeData.timeTableD1, sizeof(int)*(1000 + glbNetworkConfig.maxDelay + 1), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03123"></a><span class="lineno"> 3123</span>&#160;   }</div>
<div class="line"><a name="l03124"></a><span class="lineno"> 3124</span>&#160;}</div>
<div class="line"><a name="l03125"></a><span class="lineno"> 3125</span>&#160;</div>
<div class="line"><a name="l03126"></a><span class="lineno"> 3126</span>&#160;void SNN::copyExtFiringTable(int netId, cudaMemcpyKind kind) {</div>
<div class="line"><a name="l03127"></a><span class="lineno"> 3127</span>&#160;   assert(netId &lt; CPU_RUNTIME_BASE);</div>
<div class="line"><a name="l03128"></a><span class="lineno"> 3128</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03129"></a><span class="lineno"> 3129</span>&#160;</div>
<div class="line"><a name="l03130"></a><span class="lineno"> 3130</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD2, runtimeData[netId].extFiringTableEndIdxD2, sizeof(int) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l03131"></a><span class="lineno"> 3131</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(managerRuntimeData.extFiringTableEndIdxD1, runtimeData[netId].extFiringTableEndIdxD1, sizeof(int) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l03132"></a><span class="lineno"> 3132</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(managerRuntimeData.extFiringTableD2, runtimeData[netId].extFiringTableD2, sizeof(int*) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l03133"></a><span class="lineno"> 3133</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(managerRuntimeData.extFiringTableD1, runtimeData[netId].extFiringTableD1, sizeof(int*) * networkConfigs[netId].numGroups, kind));</div>
<div class="line"><a name="l03134"></a><span class="lineno"> 3134</span>&#160;   //KERNEL_DEBUG(&quot;GPU0 D1ex:%d/D2ex:%d&quot;, managerRuntimeData.extFiringTableEndIdxD1[0], managerRuntimeData.extFiringTableEndIdxD2[0]);</div>
<div class="line"><a name="l03135"></a><span class="lineno"> 3135</span>&#160;}</div>
<div class="line"><a name="l03136"></a><span class="lineno"> 3136</span>&#160;</div>
<div class="line"><a name="l03137"></a><span class="lineno"> 3137</span>&#160;int SNN::configGPUDevice() {</div>
<div class="line"><a name="l03138"></a><span class="lineno"> 3138</span>&#160;   int devCount, devMax;</div>
<div class="line"><a name="l03139"></a><span class="lineno"> 3139</span>&#160;   cudaDeviceProp deviceProp;</div>
<div class="line"><a name="l03140"></a><span class="lineno"> 3140</span>&#160;</div>
<div class="line"><a name="l03141"></a><span class="lineno"> 3141</span>&#160;   CUDA_CHECK_ERRORS(cudaGetDeviceCount(&amp;devCount));</div>
<div class="line"><a name="l03142"></a><span class="lineno"> 3142</span>&#160;   KERNEL_INFO(&quot;CUDA devices Configuration:&quot;);</div>
<div class="line"><a name="l03143"></a><span class="lineno"> 3143</span>&#160;   KERNEL_INFO(&quot;  - Number of CUDA devices          = %9d&quot;, devCount);</div>
<div class="line"><a name="l03144"></a><span class="lineno"> 3144</span>&#160;</div>
<div class="line"><a name="l03145"></a><span class="lineno"> 3145</span>&#160;   devMax = CUDA_GET_MAXGFLOP_DEVICE_ID();</div>
<div class="line"><a name="l03146"></a><span class="lineno"> 3146</span>&#160;   KERNEL_INFO(&quot;  - CUDA device ID with max GFLOPs  = %9d&quot;, devMax);</div>
<div class="line"><a name="l03147"></a><span class="lineno"> 3147</span>&#160;</div>
<div class="line"><a name="l03148"></a><span class="lineno"> 3148</span>&#160;   for (int ithGPU = 0; ithGPU &lt; devCount; ithGPU++) {</div>
<div class="line"><a name="l03149"></a><span class="lineno"> 3149</span>&#160;       CUDA_CHECK_ERRORS(cudaGetDeviceProperties(&amp;deviceProp, ithGPU));</div>
<div class="line"><a name="l03150"></a><span class="lineno"> 3150</span>&#160;       KERNEL_INFO(&quot;  + Use CUDA device[%1d]              = %9s&quot;, ithGPU, deviceProp.name);</div>
<div class="line"><a name="l03151"></a><span class="lineno"> 3151</span>&#160;       KERNEL_INFO(&quot;  + CUDA Compute Capability (CC)    =      %2d.%d&quot;, deviceProp.major, deviceProp.minor);</div>
<div class="line"><a name="l03152"></a><span class="lineno"> 3152</span>&#160;   }</div>
<div class="line"><a name="l03153"></a><span class="lineno"> 3153</span>&#160;   </div>
<div class="line"><a name="l03154"></a><span class="lineno"> 3154</span>&#160;   if (deviceProp.major &lt; 2) {</div>
<div class="line"><a name="l03155"></a><span class="lineno"> 3155</span>&#160;       // Unmark this when CC 1.3 is deprecated</div>
<div class="line"><a name="l03156"></a><span class="lineno"> 3156</span>&#160;       //KERNEL_ERROR(&quot;CARLsim does not support CUDA devices older than CC 2.0&quot;);</div>
<div class="line"><a name="l03157"></a><span class="lineno"> 3157</span>&#160;       //exitSimulation(1);</div>
<div class="line"><a name="l03158"></a><span class="lineno"> 3158</span>&#160;       KERNEL_WARN(&quot;CUDA device with CC 1.3 will be deprecated in a future release&quot;);</div>
<div class="line"><a name="l03159"></a><span class="lineno"> 3159</span>&#160;   }</div>
<div class="line"><a name="l03160"></a><span class="lineno"> 3160</span>&#160;</div>
<div class="line"><a name="l03161"></a><span class="lineno"> 3161</span>&#160;   for (int ithGPU = 0; ithGPU &lt; devCount; ithGPU++) {</div>
<div class="line"><a name="l03162"></a><span class="lineno"> 3162</span>&#160;       CUDA_CHECK_ERRORS(cudaSetDevice(ithGPU));</div>
<div class="line"><a name="l03163"></a><span class="lineno"> 3163</span>&#160;       CUDA_DEVICE_RESET();</div>
<div class="line"><a name="l03164"></a><span class="lineno"> 3164</span>&#160;   }</div>
<div class="line"><a name="l03165"></a><span class="lineno"> 3165</span>&#160;</div>
<div class="line"><a name="l03166"></a><span class="lineno"> 3166</span>&#160;   if (devCount &gt;= 2) { // try to setup P2P access if more than 2 GPUs are presented</div>
<div class="line"><a name="l03167"></a><span class="lineno"> 3167</span>&#160;       // FIXME: generalize the initialization for mulit-GPUs up to 4 or 8</div>
<div class="line"><a name="l03168"></a><span class="lineno"> 3168</span>&#160;       // enable P2P access</div>
<div class="line"><a name="l03169"></a><span class="lineno"> 3169</span>&#160;       int canAccessPeer_0_1, canAccessPeer_1_0;</div>
<div class="line"><a name="l03170"></a><span class="lineno"> 3170</span>&#160;       cudaDeviceCanAccessPeer(&amp;canAccessPeer_0_1, 0, 1);</div>
<div class="line"><a name="l03171"></a><span class="lineno"> 3171</span>&#160;       cudaDeviceCanAccessPeer(&amp;canAccessPeer_1_0, 1, 0);</div>
<div class="line"><a name="l03172"></a><span class="lineno"> 3172</span>&#160;       // enable peer access between GPU0 and GPU1</div>
<div class="line"><a name="l03173"></a><span class="lineno"> 3173</span>&#160;       if (canAccessPeer_0_1 &amp; canAccessPeer_1_0) {</div>
<div class="line"><a name="l03174"></a><span class="lineno"> 3174</span>&#160;           cudaSetDevice(0);</div>
<div class="line"><a name="l03175"></a><span class="lineno"> 3175</span>&#160;           cudaDeviceEnablePeerAccess(1, 0);</div>
<div class="line"><a name="l03176"></a><span class="lineno"> 3176</span>&#160;           cudaSetDevice(1);</div>
<div class="line"><a name="l03177"></a><span class="lineno"> 3177</span>&#160;           cudaDeviceEnablePeerAccess(0, 0);</div>
<div class="line"><a name="l03178"></a><span class="lineno"> 3178</span>&#160;           KERNEL_INFO(&quot;* Peer Access is enabled&quot;);</div>
<div class="line"><a name="l03179"></a><span class="lineno"> 3179</span>&#160;       } else {</div>
<div class="line"><a name="l03180"></a><span class="lineno"> 3180</span>&#160;           KERNEL_INFO(&quot;* Peer Access is not enabled&quot;);</div>
<div class="line"><a name="l03181"></a><span class="lineno"> 3181</span>&#160;       }</div>
<div class="line"><a name="l03182"></a><span class="lineno"> 3182</span>&#160;   }</div>
<div class="line"><a name="l03183"></a><span class="lineno"> 3183</span>&#160;</div>
<div class="line"><a name="l03184"></a><span class="lineno"> 3184</span>&#160;   return devCount;</div>
<div class="line"><a name="l03185"></a><span class="lineno"> 3185</span>&#160;}</div>
<div class="line"><a name="l03186"></a><span class="lineno"> 3186</span>&#160;</div>
<div class="line"><a name="l03187"></a><span class="lineno"> 3187</span>&#160;void SNN::convertExtSpikesD2_GPU(int netId, int startIdx, int endIdx, int GtoLOffset) {</div>
<div class="line"><a name="l03188"></a><span class="lineno"> 3188</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03189"></a><span class="lineno"> 3189</span>&#160;   kernel_convertExtSpikesD2 &lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS &gt;&gt;&gt;(startIdx, endIdx, GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l03190"></a><span class="lineno"> 3190</span>&#160;}</div>
<div class="line"><a name="l03191"></a><span class="lineno"> 3191</span>&#160;void SNN::convertExtSpikesD1_GPU(int netId, int startIdx, int endIdx, int GtoLOffset) {</div>
<div class="line"><a name="l03192"></a><span class="lineno"> 3192</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03193"></a><span class="lineno"> 3193</span>&#160;   kernel_convertExtSpikesD1 &lt;&lt;&lt;NUM_BLOCKS, NUM_THREADS &gt;&gt;&gt;(startIdx, endIdx, GtoLOffset); // [StartIdx, EndIdx)</div>
<div class="line"><a name="l03194"></a><span class="lineno"> 3194</span>&#160;}</div>
<div class="line"><a name="l03195"></a><span class="lineno"> 3195</span>&#160;</div>
<div class="line"><a name="l03196"></a><span class="lineno"> 3196</span>&#160;void SNN::checkAndSetGPUDevice(int netId) {</div>
<div class="line"><a name="l03197"></a><span class="lineno"> 3197</span>&#160;   int currentDevice;</div>
<div class="line"><a name="l03198"></a><span class="lineno"> 3198</span>&#160;   cudaGetDevice(&amp;currentDevice);</div>
<div class="line"><a name="l03199"></a><span class="lineno"> 3199</span>&#160;</div>
<div class="line"><a name="l03200"></a><span class="lineno"> 3200</span>&#160;   assert(netId &gt;= 0 &amp;&amp; netId &lt; numAvailableGPUs);</div>
<div class="line"><a name="l03201"></a><span class="lineno"> 3201</span>&#160;</div>
<div class="line"><a name="l03202"></a><span class="lineno"> 3202</span>&#160;   if (currentDevice != netId) {</div>
<div class="line"><a name="l03203"></a><span class="lineno"> 3203</span>&#160;       //KERNEL_DEBUG(&quot;Change GPU context from GPU %d to GPU %d&quot;, currentDevice, netId);</div>
<div class="line"><a name="l03204"></a><span class="lineno"> 3204</span>&#160;       CUDA_CHECK_ERRORS(cudaSetDevice(netId));</div>
<div class="line"><a name="l03205"></a><span class="lineno"> 3205</span>&#160;   }</div>
<div class="line"><a name="l03206"></a><span class="lineno"> 3206</span>&#160;}</div>
<div class="line"><a name="l03207"></a><span class="lineno"> 3207</span>&#160;</div>
<div class="line"><a name="l03208"></a><span class="lineno"> 3208</span>&#160;// deprecated</div>
<div class="line"><a name="l03209"></a><span class="lineno"> 3209</span>&#160;//void SNN::copyWeightsGPU(int nid, int src_grp) {</div>
<div class="line"><a name="l03210"></a><span class="lineno"> 3210</span>&#160;// checkAndSetGPUDevice(&quot;copyWeightsGPU&quot;);</div>
<div class="line"><a name="l03211"></a><span class="lineno"> 3211</span>&#160;//</div>
<div class="line"><a name="l03212"></a><span class="lineno"> 3212</span>&#160;// assert(nid &lt; numNReg);</div>
<div class="line"><a name="l03213"></a><span class="lineno"> 3213</span>&#160;// unsigned int    cumId   =  managerRuntimeData.cumulativePre[nid];</div>
<div class="line"><a name="l03214"></a><span class="lineno"> 3214</span>&#160;// float* synWts  = &amp;(managerRuntimeData.wt[cumId]);</div>
<div class="line"><a name="l03215"></a><span class="lineno"> 3215</span>&#160;// //TODO: NEEDED TO COMMENT THIS FOR CARLSIM 2.1-2.2 FILEMERGE -- KDC</div>
<div class="line"><a name="l03216"></a><span class="lineno"> 3216</span>&#160;// // assert(cumId &gt;= (nid-numNPois));</div>
<div class="line"><a name="l03217"></a><span class="lineno"> 3217</span>&#160;// //assert(cumId &lt; numPreSynapses*networkConfigs[0].numN);</div>
<div class="line"><a name="l03218"></a><span class="lineno"> 3218</span>&#160;//</div>
<div class="line"><a name="l03219"></a><span class="lineno"> 3219</span>&#160;// CUDA_CHECK_ERRORS( cudaMemcpy( synWts, &amp;runtimeData[0].wt[cumId], sizeof(float)*managerRuntimeData.Npre[nid], cudaMemcpyDeviceToHost));</div>
<div class="line"><a name="l03220"></a><span class="lineno"> 3220</span>&#160;//}</div>
<div class="line"><a name="l03221"></a><span class="lineno"> 3221</span>&#160;</div>
<div class="line"><a name="l03222"></a><span class="lineno"> 3222</span>&#160;// Allocates required memory and then initialize the GPU</div>
<div class="line"><a name="l03223"></a><span class="lineno"> 3223</span>&#160;void SNN::allocateSNN_GPU(int netId) {</div>
<div class="line"><a name="l03224"></a><span class="lineno"> 3224</span>&#160;   checkAndSetGPUDevice(netId);</div>
<div class="line"><a name="l03225"></a><span class="lineno"> 3225</span>&#160;</div>
<div class="line"><a name="l03226"></a><span class="lineno"> 3226</span>&#160;   // setup memory type of GPU runtime data</div>
<div class="line"><a name="l03227"></a><span class="lineno"> 3227</span>&#160;   runtimeData[netId].memType = GPU_MEM;</div>
<div class="line"><a name="l03228"></a><span class="lineno"> 3228</span>&#160;</div>
<div class="line"><a name="l03229"></a><span class="lineno"> 3229</span>&#160;   // display some memory management info</div>
<div class="line"><a name="l03230"></a><span class="lineno"> 3230</span>&#160;   size_t avail, total, previous;</div>
<div class="line"><a name="l03231"></a><span class="lineno"> 3231</span>&#160;   float toMB = std::pow(1024.0f, 2);</div>
<div class="line"><a name="l03232"></a><span class="lineno"> 3232</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03233"></a><span class="lineno"> 3233</span>&#160;   KERNEL_INFO(&quot;GPU Memory Management: (Total %2.3f MB)&quot;,(float)(total/toMB));</div>
<div class="line"><a name="l03234"></a><span class="lineno"> 3234</span>&#160;   KERNEL_INFO(&quot;Data\t\t\tSize\t\tTotal Used\tTotal Available&quot;);</div>
<div class="line"><a name="l03235"></a><span class="lineno"> 3235</span>&#160;   KERNEL_INFO(&quot;Init:\t\t\t%2.3f MB\t%2.3f MB\t%2.3f MB&quot;,(float)(total)/toMB,(float)((total-avail)/toMB),</div>
<div class="line"><a name="l03236"></a><span class="lineno"> 3236</span>&#160;       (float)(avail/toMB));</div>
<div class="line"><a name="l03237"></a><span class="lineno"> 3237</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03238"></a><span class="lineno"> 3238</span>&#160;</div>
<div class="line"><a name="l03239"></a><span class="lineno"> 3239</span>&#160;   // allocate random number generator on GPU(s)</div>
<div class="line"><a name="l03240"></a><span class="lineno"> 3240</span>&#160;   if(runtimeData[netId].gpuRandGen == NULL) {</div>
<div class="line"><a name="l03241"></a><span class="lineno"> 3241</span>&#160;       curandCreateGenerator(&amp;runtimeData[netId].gpuRandGen, CURAND_RNG_PSEUDO_DEFAULT);</div>
<div class="line"><a name="l03242"></a><span class="lineno"> 3242</span>&#160;       curandSetPseudoRandomGeneratorSeed(runtimeData[netId].gpuRandGen, randSeed_ + netId);</div>
<div class="line"><a name="l03243"></a><span class="lineno"> 3243</span>&#160;   }</div>
<div class="line"><a name="l03244"></a><span class="lineno"> 3244</span>&#160;</div>
<div class="line"><a name="l03245"></a><span class="lineno"> 3245</span>&#160;   // allocate SNN::runtimeData[0].randNum for random number generators</div>
<div class="line"><a name="l03246"></a><span class="lineno"> 3246</span>&#160;   CUDA_CHECK_ERRORS(cudaMalloc((void **)&amp;runtimeData[netId].randNum, networkConfigs[netId].numNPois * sizeof(float)));</div>
<div class="line"><a name="l03247"></a><span class="lineno"> 3247</span>&#160;</div>
<div class="line"><a name="l03248"></a><span class="lineno"> 3248</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03249"></a><span class="lineno"> 3249</span>&#160;   KERNEL_INFO(&quot;Random Gen:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB&quot;,(float)(previous-avail)/toMB, (float)((total-avail)/toMB),(float)(avail/toMB));</div>
<div class="line"><a name="l03250"></a><span class="lineno"> 3250</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03251"></a><span class="lineno"> 3251</span>&#160;</div>
<div class="line"><a name="l03252"></a><span class="lineno"> 3252</span>&#160;   // initialize runtimeData[0].neuronAllocation, __device__ loadBufferCount, loadBufferSize</div>
<div class="line"><a name="l03253"></a><span class="lineno"> 3253</span>&#160;   allocateStaticLoad(netId, NUM_THREADS);</div>
<div class="line"><a name="l03254"></a><span class="lineno"> 3254</span>&#160;</div>
<div class="line"><a name="l03255"></a><span class="lineno"> 3255</span>&#160;   allocateGroupId(netId);</div>
<div class="line"><a name="l03256"></a><span class="lineno"> 3256</span>&#160;</div>
<div class="line"><a name="l03257"></a><span class="lineno"> 3257</span>&#160;   // this table is useful for quick evaluation of the position of fired neuron</div>
<div class="line"><a name="l03258"></a><span class="lineno"> 3258</span>&#160;   // given a sequence of bits denoting the firing..</div>
<div class="line"><a name="l03259"></a><span class="lineno"> 3259</span>&#160;   // initialize __device__ quickSynIdTableGPU[256]</div>
<div class="line"><a name="l03260"></a><span class="lineno"> 3260</span>&#160;   initQuickSynIdTable(netId);</div>
<div class="line"><a name="l03261"></a><span class="lineno"> 3261</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03262"></a><span class="lineno"> 3262</span>&#160;   KERNEL_INFO(&quot;Static Load:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB&quot;,(float)(previous-avail)/toMB, (float)((total-avail)/toMB),(float)(avail/toMB));</div>
<div class="line"><a name="l03263"></a><span class="lineno"> 3263</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03264"></a><span class="lineno"> 3264</span>&#160;</div>
<div class="line"><a name="l03265"></a><span class="lineno"> 3265</span>&#160;   // initialize (copy from SNN) runtimeData[0].Npre, runtimeData[0].Npre_plastic, runtimeData[0].Npre_plasticInv, runtimeData[0].cumulativePre</div>
<div class="line"><a name="l03266"></a><span class="lineno"> 3266</span>&#160;   // initialize (copy from SNN) runtimeData[0].cumulativePost, runtimeData[0].Npost, runtimeData[0].postDelayInfo</div>
<div class="line"><a name="l03267"></a><span class="lineno"> 3267</span>&#160;   // initialize (copy from SNN) runtimeData[0].postSynapticIds, runtimeData[0].preSynapticIds</div>
<div class="line"><a name="l03268"></a><span class="lineno"> 3268</span>&#160;   copyPreConnectionInfo(netId, ALL, &amp;runtimeData[netId], &amp;managerRuntimeData, cudaMemcpyHostToDevice, true);</div>
<div class="line"><a name="l03269"></a><span class="lineno"> 3269</span>&#160;   copyPostConnectionInfo(netId, ALL, &amp;runtimeData[netId], &amp;managerRuntimeData, cudaMemcpyHostToDevice, true);</div>
<div class="line"><a name="l03270"></a><span class="lineno"> 3270</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03271"></a><span class="lineno"> 3271</span>&#160;   KERNEL_INFO(&quot;Conn Info:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB&quot;,(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));</div>
<div class="line"><a name="l03272"></a><span class="lineno"> 3272</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03273"></a><span class="lineno"> 3273</span>&#160;   </div>
<div class="line"><a name="l03274"></a><span class="lineno"> 3274</span>&#160;   // initialize (copy from SNN) runtimeData[0].wt, runtimeData[0].wtChange, runtimeData[0].maxSynWt</div>
<div class="line"><a name="l03275"></a><span class="lineno"> 3275</span>&#160;   copySynapseState(netId, &amp;runtimeData[netId], &amp;managerRuntimeData, cudaMemcpyHostToDevice, true);</div>
<div class="line"><a name="l03276"></a><span class="lineno"> 3276</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03277"></a><span class="lineno"> 3277</span>&#160;   KERNEL_INFO(&quot;Syn State:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB&quot;,(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));</div>
<div class="line"><a name="l03278"></a><span class="lineno"> 3278</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03279"></a><span class="lineno"> 3279</span>&#160;   </div>
<div class="line"><a name="l03280"></a><span class="lineno"> 3280</span>&#160;   // copy the neuron state information to the GPU..</div>
<div class="line"><a name="l03281"></a><span class="lineno"> 3281</span>&#160;   // initialize (copy from managerRuntimeData) runtimeData[0].recovery, runtimeData[0].voltage, runtimeData[0].current</div>
<div class="line"><a name="l03282"></a><span class="lineno"> 3282</span>&#160;   // initialize (copy from managerRuntimeData) runtimeData[0].gGABAa, runtimeData[0].gGABAb, runtimeData[0].gAMPA, runtimeData[0].gNMDA</div>
<div class="line"><a name="l03283"></a><span class="lineno"> 3283</span>&#160;   // initialize (copy from SNN) runtimeData[0].Izh_a, runtimeData[0].Izh_b, runtimeData[0].Izh_c, runtimeData[0].Izh_d</div>
<div class="line"><a name="l03284"></a><span class="lineno"> 3284</span>&#160;   // initialize (copy form SNN) runtimeData[0].baseFiring, runtimeData[0].baseFiringInv</div>
<div class="line"><a name="l03285"></a><span class="lineno"> 3285</span>&#160;   copyNeuronState(netId, ALL, &amp;runtimeData[netId], cudaMemcpyHostToDevice, true);</div>
<div class="line"><a name="l03286"></a><span class="lineno"> 3286</span>&#160;</div>
<div class="line"><a name="l03287"></a><span class="lineno"> 3287</span>&#160;   // copy STP state, considered as neuron state</div>
<div class="line"><a name="l03288"></a><span class="lineno"> 3288</span>&#160;   if (sim_with_stp) {</div>
<div class="line"><a name="l03289"></a><span class="lineno"> 3289</span>&#160;       // initialize (copy from SNN) stpu, stpx</div>
<div class="line"><a name="l03290"></a><span class="lineno"> 3290</span>&#160;       copySTPState(netId, ALL, &amp;runtimeData[netId], &amp;managerRuntimeData, cudaMemcpyHostToDevice, true);</div>
<div class="line"><a name="l03291"></a><span class="lineno"> 3291</span>&#160;   }</div>
<div class="line"><a name="l03292"></a><span class="lineno"> 3292</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03293"></a><span class="lineno"> 3293</span>&#160;   KERNEL_INFO(&quot;Neuron State:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB&quot;,(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));</div>
<div class="line"><a name="l03294"></a><span class="lineno"> 3294</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03295"></a><span class="lineno"> 3295</span>&#160;       </div>
<div class="line"><a name="l03296"></a><span class="lineno"> 3296</span>&#160;   // initialize (copy from SNN) runtimeData[0].grpDA(5HT,ACh,NE)</div>
<div class="line"><a name="l03297"></a><span class="lineno"> 3297</span>&#160;   // initialize (copy from SNN) runtimeData[0].grpDA(5HT,ACh,NE)Buffer[]</div>
<div class="line"><a name="l03298"></a><span class="lineno"> 3298</span>&#160;   copyGroupState(netId, ALL, &amp;runtimeData[netId], &amp;managerRuntimeData, cudaMemcpyHostToDevice, true);</div>
<div class="line"><a name="l03299"></a><span class="lineno"> 3299</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03300"></a><span class="lineno"> 3300</span>&#160;   KERNEL_INFO(&quot;Group State:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB&quot;,(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));</div>
<div class="line"><a name="l03301"></a><span class="lineno"> 3301</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03302"></a><span class="lineno"> 3302</span>&#160;</div>
<div class="line"><a name="l03303"></a><span class="lineno"> 3303</span>&#160;   // initialize (cudaMemset) runtimeData[0].I_set, runtimeData[0].poissonFireRate</div>
<div class="line"><a name="l03304"></a><span class="lineno"> 3304</span>&#160;   // initialize (copy from SNN) runtimeData[0].firingTableD1, runtimeData[0].firingTableD2</div>
<div class="line"><a name="l03305"></a><span class="lineno"> 3305</span>&#160;   // initialize (cudaMalloc) runtimeData[0].spikeGenBits</div>
<div class="line"><a name="l03306"></a><span class="lineno"> 3306</span>&#160;   // initialize (copy from managerRuntimeData) runtimeData[0].nSpikeCnt,</div>
<div class="line"><a name="l03307"></a><span class="lineno"> 3307</span>&#160;   // initialize (copy from SNN) runtimeData[0].synSpikeTime, runtimeData[0].lastSpikeTime</div>
<div class="line"><a name="l03308"></a><span class="lineno"> 3308</span>&#160;   copyAuxiliaryData(netId, ALL, &amp;runtimeData[netId], cudaMemcpyHostToDevice, true);</div>
<div class="line"><a name="l03309"></a><span class="lineno"> 3309</span>&#160;   cudaMemGetInfo(&amp;avail,&amp;total);</div>
<div class="line"><a name="l03310"></a><span class="lineno"> 3310</span>&#160;   KERNEL_INFO(&quot;Auxiliary Data:\t\t%2.3f MB\t%2.3f MB\t%2.3f MB\n\n&quot;,(float)(previous-avail)/toMB,(float)((total-avail)/toMB), (float)(avail/toMB));</div>
<div class="line"><a name="l03311"></a><span class="lineno"> 3311</span>&#160;   previous=avail;</div>
<div class="line"><a name="l03312"></a><span class="lineno"> 3312</span>&#160;</div>
<div class="line"><a name="l03313"></a><span class="lineno"> 3313</span>&#160;   // copy relevant pointers and network information to GPU</div>
<div class="line"><a name="l03314"></a><span class="lineno"> 3314</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(runtimeDataGPU, &amp;runtimeData[netId], sizeof(RuntimeData), 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03315"></a><span class="lineno"> 3315</span>&#160;</div>
<div class="line"><a name="l03316"></a><span class="lineno"> 3316</span>&#160;   // copy data to from SNN:: to NetworkConfigRT SNN::networkConfigs[0]</div>
<div class="line"><a name="l03317"></a><span class="lineno"> 3317</span>&#160;   copyNetworkConfig(netId, cudaMemcpyHostToDevice); // FIXME: we can change the group properties such as STDP as the network is running.  So, we need a way to updating the GPU when changes are made.</div>
<div class="line"><a name="l03318"></a><span class="lineno"> 3318</span>&#160;</div>
<div class="line"><a name="l03319"></a><span class="lineno"> 3319</span>&#160;   // TODO: move mulSynFast, mulSynSlow to ConnectConfig structure</div>
<div class="line"><a name="l03320"></a><span class="lineno"> 3320</span>&#160;   // copy connection configs</div>
<div class="line"><a name="l03321"></a><span class="lineno"> 3321</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(d_mulSynFast, mulSynFast, sizeof(float) * networkConfigs[netId].numConnections, 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03322"></a><span class="lineno"> 3322</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpyToSymbol(d_mulSynSlow, mulSynSlow, sizeof(float) * networkConfigs[netId].numConnections, 0, cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03323"></a><span class="lineno"> 3323</span>&#160;</div>
<div class="line"><a name="l03324"></a><span class="lineno"> 3324</span>&#160;   copyGroupConfigs(netId);</div>
<div class="line"><a name="l03325"></a><span class="lineno"> 3325</span>&#160;</div>
<div class="line"><a name="l03326"></a><span class="lineno"> 3326</span>&#160;   KERNEL_DEBUG(&quot;Transfering group settings to GPU:&quot;);</div>
<div class="line"><a name="l03327"></a><span class="lineno"> 3327</span>&#160;   for (int lGrpId = 0; lGrpId &lt; networkConfigs[netId].numGroupsAssigned; lGrpId++) {</div>
<div class="line"><a name="l03328"></a><span class="lineno"> 3328</span>&#160;       KERNEL_DEBUG(&quot;Settings for Group %s:&quot;, groupConfigMap[groupConfigs[netId][lGrpId].gGrpId].grpName.c_str());</div>
<div class="line"><a name="l03329"></a><span class="lineno"> 3329</span>&#160;       </div>
<div class="line"><a name="l03330"></a><span class="lineno"> 3330</span>&#160;       KERNEL_DEBUG(&quot;\tType: %d&quot;,(int)groupConfigs[netId][lGrpId].Type);</div>
<div class="line"><a name="l03331"></a><span class="lineno"> 3331</span>&#160;       KERNEL_DEBUG(&quot;\tNumN: %d&quot;,groupConfigs[netId][lGrpId].numN);</div>
<div class="line"><a name="l03332"></a><span class="lineno"> 3332</span>&#160;       KERNEL_DEBUG(&quot;\tM: %d&quot;,groupConfigs[netId][lGrpId].numPostSynapses);</div>
<div class="line"><a name="l03333"></a><span class="lineno"> 3333</span>&#160;       KERNEL_DEBUG(&quot;\tPreM: %d&quot;,groupConfigs[netId][lGrpId].numPreSynapses);</div>
<div class="line"><a name="l03334"></a><span class="lineno"> 3334</span>&#160;       KERNEL_DEBUG(&quot;\tspikeGenerator: %d&quot;,(int)groupConfigs[netId][lGrpId].isSpikeGenerator);</div>
<div class="line"><a name="l03335"></a><span class="lineno"> 3335</span>&#160;       KERNEL_DEBUG(&quot;\tFixedInputWts: %d&quot;,(int)groupConfigs[netId][lGrpId].FixedInputWts);</div>
<div class="line"><a name="l03336"></a><span class="lineno"> 3336</span>&#160;       KERNEL_DEBUG(&quot;\tMaxDelay: %d&quot;,(int)groupConfigs[netId][lGrpId].MaxDelay);</div>
<div class="line"><a name="l03337"></a><span class="lineno"> 3337</span>&#160;       KERNEL_DEBUG(&quot;\tWithSTDP: %d&quot;,(int)groupConfigs[netId][lGrpId].WithSTDP);</div>
<div class="line"><a name="l03338"></a><span class="lineno"> 3338</span>&#160;       if (groupConfigs[netId][lGrpId].WithSTDP) {</div>
<div class="line"><a name="l03339"></a><span class="lineno"> 3339</span>&#160;           KERNEL_DEBUG(&quot;\t\tE-STDP type: %s&quot;,stdpType_string[groupConfigs[netId][lGrpId].WithESTDPtype]);</div>
<div class="line"><a name="l03340"></a><span class="lineno"> 3340</span>&#160;           KERNEL_DEBUG(&quot;\t\tTAU_PLUS_INV_EXC: %f&quot;,groupConfigs[netId][lGrpId].TAU_PLUS_INV_EXC);</div>
<div class="line"><a name="l03341"></a><span class="lineno"> 3341</span>&#160;           KERNEL_DEBUG(&quot;\t\tTAU_MINUS_INV_EXC: %f&quot;,groupConfigs[netId][lGrpId].TAU_MINUS_INV_EXC);</div>
<div class="line"><a name="l03342"></a><span class="lineno"> 3342</span>&#160;           KERNEL_DEBUG(&quot;\t\tALPHA_PLUS_EXC: %f&quot;,groupConfigs[netId][lGrpId].ALPHA_PLUS_EXC);</div>
<div class="line"><a name="l03343"></a><span class="lineno"> 3343</span>&#160;           KERNEL_DEBUG(&quot;\t\tALPHA_MINUS_EXC: %f&quot;,groupConfigs[netId][lGrpId].ALPHA_MINUS_EXC);</div>
<div class="line"><a name="l03344"></a><span class="lineno"> 3344</span>&#160;           KERNEL_DEBUG(&quot;\t\tI-STDP type: %s&quot;,stdpType_string[groupConfigs[netId][lGrpId].WithISTDPtype]);</div>
<div class="line"><a name="l03345"></a><span class="lineno"> 3345</span>&#160;           KERNEL_DEBUG(&quot;\t\tTAU_PLUS_INV_INB: %f&quot;,groupConfigs[netId][lGrpId].TAU_PLUS_INV_INB);</div>
<div class="line"><a name="l03346"></a><span class="lineno"> 3346</span>&#160;           KERNEL_DEBUG(&quot;\t\tTAU_MINUS_INV_INB: %f&quot;,groupConfigs[netId][lGrpId].TAU_MINUS_INV_INB);</div>
<div class="line"><a name="l03347"></a><span class="lineno"> 3347</span>&#160;           KERNEL_DEBUG(&quot;\t\tALPHA_PLUS_INB: %f&quot;,groupConfigs[netId][lGrpId].ALPHA_PLUS_INB);</div>
<div class="line"><a name="l03348"></a><span class="lineno"> 3348</span>&#160;           KERNEL_DEBUG(&quot;\t\tALPHA_MINUS_INB: %f&quot;,groupConfigs[netId][lGrpId].ALPHA_MINUS_INB);</div>
<div class="line"><a name="l03349"></a><span class="lineno"> 3349</span>&#160;           KERNEL_DEBUG(&quot;\t\tLAMBDA: %f&quot;,groupConfigs[netId][lGrpId].LAMBDA);</div>
<div class="line"><a name="l03350"></a><span class="lineno"> 3350</span>&#160;           KERNEL_DEBUG(&quot;\t\tDELTA: %f&quot;,groupConfigs[netId][lGrpId].DELTA);</div>
<div class="line"><a name="l03351"></a><span class="lineno"> 3351</span>&#160;           KERNEL_DEBUG(&quot;\t\tBETA_LTP: %f&quot;,groupConfigs[netId][lGrpId].BETA_LTP);</div>
<div class="line"><a name="l03352"></a><span class="lineno"> 3352</span>&#160;           KERNEL_DEBUG(&quot;\t\tBETA_LTD: %f&quot;,groupConfigs[netId][lGrpId].BETA_LTD);</div>
<div class="line"><a name="l03353"></a><span class="lineno"> 3353</span>&#160;       }</div>
<div class="line"><a name="l03354"></a><span class="lineno"> 3354</span>&#160;       KERNEL_DEBUG(&quot;\tWithSTP: %d&quot;,(int)groupConfigs[netId][lGrpId].WithSTP);</div>
<div class="line"><a name="l03355"></a><span class="lineno"> 3355</span>&#160;       if (groupConfigs[netId][lGrpId].WithSTP) {</div>
<div class="line"><a name="l03356"></a><span class="lineno"> 3356</span>&#160;           KERNEL_DEBUG(&quot;\t\tSTP_U: %f&quot;,groupConfigs[netId][lGrpId].STP_U);</div>
<div class="line"><a name="l03357"></a><span class="lineno"> 3357</span>&#160;//             KERNEL_DEBUG(&quot;\t\tSTP_tD: %f&quot;,groupConfigs[netId][lGrpId].STP_tD);</div>
<div class="line"><a name="l03358"></a><span class="lineno"> 3358</span>&#160;//             KERNEL_DEBUG(&quot;\t\tSTP_tF: %f&quot;,groupConfigs[netId][lGrpId].STP_tF);</div>
<div class="line"><a name="l03359"></a><span class="lineno"> 3359</span>&#160;       }</div>
<div class="line"><a name="l03360"></a><span class="lineno"> 3360</span>&#160;       KERNEL_DEBUG(&quot;\tspikeGen: %s&quot;, groupConfigs[netId][lGrpId].isSpikeGenFunc? &quot;is Set&quot; : &quot;is not set &quot;);</div>
<div class="line"><a name="l03361"></a><span class="lineno"> 3361</span>&#160;   }</div>
<div class="line"><a name="l03362"></a><span class="lineno"> 3362</span>&#160;</div>
<div class="line"><a name="l03363"></a><span class="lineno"> 3363</span>&#160;   // allocation of gpu runtime data is done</div>
<div class="line"><a name="l03364"></a><span class="lineno"> 3364</span>&#160;   runtimeData[netId].allocated = true;</div>
<div class="line"><a name="l03365"></a><span class="lineno"> 3365</span>&#160;</div>
<div class="line"><a name="l03366"></a><span class="lineno"> 3366</span>&#160;   // map the timing table to texture.. saves a lot of headache in using shared memory</div>
<div class="line"><a name="l03367"></a><span class="lineno"> 3367</span>&#160;   void* devPtr;</div>
<div class="line"><a name="l03368"></a><span class="lineno"> 3368</span>&#160;   size_t offset;</div>
<div class="line"><a name="l03369"></a><span class="lineno"> 3369</span>&#160;   CUDA_CHECK_ERRORS(cudaGetSymbolAddress(&amp;devPtr, timeTableD2GPU));</div>
<div class="line"><a name="l03370"></a><span class="lineno"> 3370</span>&#160;   CUDA_CHECK_ERRORS(cudaBindTexture(&amp;offset, timeTableD2GPU_tex, devPtr, sizeof(int) * TIMING_COUNT));</div>
<div class="line"><a name="l03371"></a><span class="lineno"> 3371</span>&#160;   offset = offset / sizeof(int);</div>
<div class="line"><a name="l03372"></a><span class="lineno"> 3372</span>&#160;   CUDA_CHECK_ERRORS(cudaGetSymbolAddress(&amp;devPtr, timeTableD2GPU_tex_offset));</div>
<div class="line"><a name="l03373"></a><span class="lineno"> 3373</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(devPtr, &amp;offset, sizeof(int), cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03374"></a><span class="lineno"> 3374</span>&#160;       </div>
<div class="line"><a name="l03375"></a><span class="lineno"> 3375</span>&#160;   CUDA_CHECK_ERRORS(cudaGetSymbolAddress(&amp;devPtr, timeTableD1GPU));</div>
<div class="line"><a name="l03376"></a><span class="lineno"> 3376</span>&#160;   CUDA_CHECK_ERRORS(cudaBindTexture(&amp;offset, timeTableD1GPU_tex, devPtr, sizeof(int) * TIMING_COUNT));</div>
<div class="line"><a name="l03377"></a><span class="lineno"> 3377</span>&#160;   offset = offset / sizeof(int);</div>
<div class="line"><a name="l03378"></a><span class="lineno"> 3378</span>&#160;   CUDA_CHECK_ERRORS(cudaGetSymbolAddress(&amp;devPtr, timeTableD1GPU_tex_offset));</div>
<div class="line"><a name="l03379"></a><span class="lineno"> 3379</span>&#160;   CUDA_CHECK_ERRORS(cudaMemcpy(devPtr, &amp;offset, sizeof(int), cudaMemcpyHostToDevice));</div>
<div class="line"><a name="l03380"></a><span class="lineno"> 3380</span>&#160;</div>
<div class="line"><a name="l03381"></a><span class="lineno"> 3381</span>&#160;   initGPU(netId);</div>
<div class="line"><a name="l03382"></a><span class="lineno"> 3382</span>&#160;}</div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_149f82d01cbdfd1b8607983cc286b463.html">carlsim</a></li><li class="navelem"><a class="el" href="dir_06a4305d97c5fa523d824c152466b991.html">kernel</a></li><li class="navelem"><a class="el" href="dir_f8d2999d3083f30732c444a1c48d221e.html">src</a></li><li class="navelem"><a class="el" href="dir_ff20d50015323f5845664c4544a3a5c4.html">gpu_module</a></li><li class="navelem"><a class="el" href="snn__gpu__module_8cu.html">snn_gpu_module.cu</a></li>
    <li class="footer">Generated on Thu Mar 30 2017 12:38:49 for CARLsim by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.9.1 </li>
  </ul>
</div>
</body>
</html>
